{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0983f1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from IPython.display import Image, display\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Set environment variables for LangSmith tracking and LangChain project\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGRAPH_LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "# groq api\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "213cbb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reducers\n",
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "732c6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Langgraph Application ###\n",
    "\n",
    "#  Messages have the type 'list'. Annotated is used to add metadata i.e which is human message and which is AI message.\n",
    "# The 'add_messages function in the annotation defines how this state key should be updated (in this case, it appends messags to the list,rather than overwriting them).\n",
    "# messages variable is of type Annotated\n",
    "# State class inheriting TypedDict. For key value pair representation.\n",
    "\n",
    "class State(TypedDict): \n",
    "  messages:Annotated[list,add_messages] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92d24d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatbot takes parameter as state. Chatbot is inheriting State class because state management keeps on changing.\n",
    "# chatbot function is invoking previous messages given by user with state and creating new messages which is returned.\n",
    "def chatbot(state:State):\n",
    "  return {\"messages\":[llm.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d61d38c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()\n",
    "\n",
    "#starting graph building process\n",
    "graph = StateGraph(State)\n",
    "\n",
    "#node\n",
    "graph.add_node(\"chatbot\",chatbot)\n",
    "#edges\n",
    "graph.add_edge(START,\"chatbot\")\n",
    "graph.add_edge(\"chatbot\",END)\n",
    "\n",
    "graph_builder = graph.compile()\n",
    "graph_memory = graph.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8858eaf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "## visualize graph as mermaid diagram\n",
    "display(Image(graph_builder.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c95b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='what is nuts', additional_kwargs={}, response_metadata={}, id='5b6eb8f6-ee67-4bc7-b2e7-fd2ee3d5ccb4')]}\n",
      "{'messages': [HumanMessage(content='what is nuts', additional_kwargs={}, response_metadata={}, id='5b6eb8f6-ee67-4bc7-b2e7-fd2ee3d5ccb4'), AIMessage(content=\"Nuts are the edible seeds of plants, typically from trees or shrubs. They are a type of fruit, specifically a type of dry fruit that doesn't open to release its seeds when ripe. Nuts are a good source of protein, fiber, and healthy fats, making them a popular snack and ingredient in many cuisines.\\n\\nTypes of nuts include:\\n\\n1. Tree nuts:\\n\\t* Almonds\\n\\t* Walnuts\\n\\t* Pecans\\n\\t* Hazelnuts\\n\\t* Cashews\\n\\t* Pistachios\\n\\t* Brazil nuts\\n2. Seed nuts:\\n\\t* Peanuts (technically a legume, but often referred to as a nut)\\n\\t* Pumpkin seeds\\n\\t* Sunflower seeds\\n\\nNuts have several characteristics that make them distinct from other seeds:\\n\\n1. Hard shell: Nuts have a hard, woody shell that protects the seed inside.\\n2. Single seed: Nuts usually contain a single seed, although some, like pine nuts, can have multiple seeds.\\n3. Dry fruit: Nuts are a type of dry fruit, meaning they don't have a juicy pulp like many other fruits.\\n\\nSome common uses for nuts include:\\n\\n1. Snacking: Nuts are a popular snack on their own or as part of a mix.\\n2. Baking: Nuts are often used in baked goods like cakes, cookies, and muffins.\\n3. Cooking: Nuts can be used in savory dishes, like stir-fries and curries.\\n4. Health supplements: Nuts are a good source of healthy fats and protein, making them a popular ingredient in health supplements.\\n\\nIt's worth noting that while nuts are nutritious, they can also be high in calories and fat. Moderation is key when consuming nuts as part of a balanced diet.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 367, 'prompt_tokens': 38, 'total_tokens': 405, 'completion_time': 0.630450204, 'prompt_time': 0.001988405, 'queue_time': 0.044054355, 'total_time': 0.632438609}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_7b3cfae3af', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--56985af2-bb3b-4c5b-88fb-f28a0033f816-0', usage_metadata={'input_tokens': 38, 'output_tokens': 367, 'total_tokens': 405})]}\n",
      "Good Bye!\n"
     ]
    }
   ],
   "source": [
    "# streaming -> methods - stream - sync, astream - async \n",
    "# parameters - values, updates\n",
    "\n",
    "# streaming the responses # full conversation is updated and displayed with each response\n",
    "config = {\"configurable\":{\"thread_id\":\"1\"}}\n",
    "while True:\n",
    "   user_input = input(\"User: \")\n",
    "   if user_input.lower() in [\"quit\",\"q\"]:\n",
    "      print(\"Good Bye!\")\n",
    "      break\n",
    "   for event in graph_builder.stream({'messages':(\"user\",user_input)},stream_mode='values'):\n",
    "      print(event)\n",
    "      for value in event.values():\n",
    "         print(value['messages']) # it should user message\n",
    "         print(\"Assistant:\", value['messages'][-1].content) # it should have llm model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c423097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming the responses #updates recent message with each response and displays only the latest message\n",
    "config = {\"configurable\":{\"thread_id\":\"2\"}}\n",
    "while True:\n",
    "   user_input = input(\"User: \")\n",
    "   if user_input.lower() in [\"quit\",\"q\"]:\n",
    "      print(\"Good Bye!\")\n",
    "      break\n",
    "   for event in graph_memory.stream({'messages':(\"user\",user_input)},config,stream_mode='updates'):\n",
    "      print(event)\n",
    "      for value in event.values():\n",
    "         print(value['messages']) # it should print user message\n",
    "         print(\"Assistant:\", value['messages'][-1].content) # it should have llm model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43357e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chain_start', 'data': {'input': {'messages': ('user', 'hi chatbot')}}, 'name': 'LangGraph', 'tags': [], 'run_id': '6c2e7174-78f3-4464-bc2f-bbc9295ba4e4', 'metadata': {'thread_id': '3'}, 'parent_ids': []}\n",
      "{'event': 'on_chain_start', 'data': {'input': {'messages': [HumanMessage(content='hi chatbot', additional_kwargs={}, response_metadata={}, id='41b7f8e3-3b18-4191-823b-a42731f791aa'), AIMessage(content='How can I help you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 38, 'total_tokens': 46, 'completion_time': 0.013411805, 'prompt_time': 0.001660218, 'queue_time': 0.048211952, 'total_time': 0.015072023}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--bfdab10e-885a-404f-9d3a-88f0e8e7394f-0', usage_metadata={'input_tokens': 38, 'output_tokens': 8, 'total_tokens': 46}), HumanMessage(content='hi chatbot', additional_kwargs={}, response_metadata={}, id='a2272ebc-5f81-4cd4-b412-55d0d48f37f1')]}}, 'name': 'chatbot', 'tags': ['graph:step:4'], 'run_id': '1d391f33-4886-42af-b606-80cd1e2da0d0', 'metadata': {'thread_id': '3', 'langgraph_step': 4, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:2324da28-95ea-bdae-e0e1-b49a11b5ce3d'}, 'parent_ids': ['6c2e7174-78f3-4464-bc2f-bbc9295ba4e4']}\n",
      "{'event': 'on_chain_stream', 'run_id': '1d391f33-4886-42af-b606-80cd1e2da0d0', 'name': 'chatbot', 'tags': ['graph:step:4'], 'metadata': {'thread_id': '3', 'langgraph_step': 4, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:2324da28-95ea-bdae-e0e1-b49a11b5ce3d', 'revision_id': '3899522'}, 'data': {'chunk': {'messages': [AIMessage(content='It seems like you said hello twice. Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 58, 'total_tokens': 82, 'completion_time': 0.042371427, 'prompt_time': 0.002722783, 'queue_time': 0.052576507, 'total_time': 0.04509421}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--417abb47-1a45-43ff-bb7a-1655734a18a7-0', usage_metadata={'input_tokens': 58, 'output_tokens': 24, 'total_tokens': 82})]}}, 'parent_ids': ['6c2e7174-78f3-4464-bc2f-bbc9295ba4e4']}\n",
      "{'event': 'on_chain_end', 'data': {'output': {'messages': [AIMessage(content='It seems like you said hello twice. Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 58, 'total_tokens': 82, 'completion_time': 0.042371427, 'prompt_time': 0.002722783, 'queue_time': 0.052576507, 'total_time': 0.04509421}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--417abb47-1a45-43ff-bb7a-1655734a18a7-0', usage_metadata={'input_tokens': 58, 'output_tokens': 24, 'total_tokens': 82})]}, 'input': {'messages': [HumanMessage(content='hi chatbot', additional_kwargs={}, response_metadata={}, id='41b7f8e3-3b18-4191-823b-a42731f791aa'), AIMessage(content='How can I help you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 38, 'total_tokens': 46, 'completion_time': 0.013411805, 'prompt_time': 0.001660218, 'queue_time': 0.048211952, 'total_time': 0.015072023}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--bfdab10e-885a-404f-9d3a-88f0e8e7394f-0', usage_metadata={'input_tokens': 38, 'output_tokens': 8, 'total_tokens': 46}), HumanMessage(content='hi chatbot', additional_kwargs={}, response_metadata={}, id='a2272ebc-5f81-4cd4-b412-55d0d48f37f1')]}}, 'run_id': '1d391f33-4886-42af-b606-80cd1e2da0d0', 'name': 'chatbot', 'tags': ['graph:step:4'], 'metadata': {'thread_id': '3', 'langgraph_step': 4, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:2324da28-95ea-bdae-e0e1-b49a11b5ce3d', 'revision_id': '3899522'}, 'parent_ids': ['6c2e7174-78f3-4464-bc2f-bbc9295ba4e4']}\n",
      "{'event': 'on_chain_stream', 'run_id': '6c2e7174-78f3-4464-bc2f-bbc9295ba4e4', 'name': 'LangGraph', 'tags': [], 'metadata': {'thread_id': '3', 'revision_id': '3899522'}, 'data': {'chunk': {'chatbot': {'messages': [AIMessage(content='It seems like you said hello twice. Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 58, 'total_tokens': 82, 'completion_time': 0.042371427, 'prompt_time': 0.002722783, 'queue_time': 0.052576507, 'total_time': 0.04509421}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--417abb47-1a45-43ff-bb7a-1655734a18a7-0', usage_metadata={'input_tokens': 58, 'output_tokens': 24, 'total_tokens': 82})]}}}, 'parent_ids': []}\n",
      "{'event': 'on_chain_end', 'data': {'output': {'messages': [HumanMessage(content='hi chatbot', additional_kwargs={}, response_metadata={}, id='41b7f8e3-3b18-4191-823b-a42731f791aa'), AIMessage(content='How can I help you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 38, 'total_tokens': 46, 'completion_time': 0.013411805, 'prompt_time': 0.001660218, 'queue_time': 0.048211952, 'total_time': 0.015072023}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--bfdab10e-885a-404f-9d3a-88f0e8e7394f-0', usage_metadata={'input_tokens': 38, 'output_tokens': 8, 'total_tokens': 46}), HumanMessage(content='hi chatbot', additional_kwargs={}, response_metadata={}, id='a2272ebc-5f81-4cd4-b412-55d0d48f37f1'), AIMessage(content='It seems like you said hello twice. Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 58, 'total_tokens': 82, 'completion_time': 0.042371427, 'prompt_time': 0.002722783, 'queue_time': 0.052576507, 'total_time': 0.04509421}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--417abb47-1a45-43ff-bb7a-1655734a18a7-0', usage_metadata={'input_tokens': 58, 'output_tokens': 24, 'total_tokens': 82})]}}, 'run_id': '6c2e7174-78f3-4464-bc2f-bbc9295ba4e4', 'name': 'LangGraph', 'tags': [], 'metadata': {'thread_id': '3', 'revision_id': '3899522'}, 'parent_ids': []}\n",
      "{'event': 'on_chain_start', 'data': {'input': {'messages': ('user', 'what is async')}}, 'name': 'LangGraph', 'tags': [], 'run_id': '2d53f1da-e741-44dd-a64f-2d39464ad840', 'metadata': {'thread_id': '3'}, 'parent_ids': []}\n",
      "{'event': 'on_chain_start', 'data': {'input': {'messages': [HumanMessage(content='hi chatbot', additional_kwargs={}, response_metadata={}, id='41b7f8e3-3b18-4191-823b-a42731f791aa'), AIMessage(content='How can I help you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 38, 'total_tokens': 46, 'completion_time': 0.013411805, 'prompt_time': 0.001660218, 'queue_time': 0.048211952, 'total_time': 0.015072023}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--bfdab10e-885a-404f-9d3a-88f0e8e7394f-0', usage_metadata={'input_tokens': 38, 'output_tokens': 8, 'total_tokens': 46}), HumanMessage(content='hi chatbot', additional_kwargs={}, response_metadata={}, id='a2272ebc-5f81-4cd4-b412-55d0d48f37f1'), AIMessage(content='It seems like you said hello twice. Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 58, 'total_tokens': 82, 'completion_time': 0.042371427, 'prompt_time': 0.002722783, 'queue_time': 0.052576507, 'total_time': 0.04509421}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--417abb47-1a45-43ff-bb7a-1655734a18a7-0', usage_metadata={'input_tokens': 58, 'output_tokens': 24, 'total_tokens': 82}), HumanMessage(content='what is async', additional_kwargs={}, response_metadata={}, id='4851f01a-b8aa-4d74-8b04-49ca2a05cf13')]}}, 'name': 'chatbot', 'tags': ['graph:step:7'], 'run_id': '2415c34d-76dc-4638-9b02-58c9c51f5b25', 'metadata': {'thread_id': '3', 'langgraph_step': 7, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:ff478b07-263a-888b-3d17-7a80dad9216d'}, 'parent_ids': ['2d53f1da-e741-44dd-a64f-2d39464ad840']}\n",
      "{'event': 'on_chain_stream', 'run_id': '2415c34d-76dc-4638-9b02-58c9c51f5b25', 'name': 'chatbot', 'tags': ['graph:step:7'], 'metadata': {'thread_id': '3', 'langgraph_step': 7, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:ff478b07-263a-888b-3d17-7a80dad9216d', 'revision_id': '3899522'}, 'data': {'chunk': {'messages': [AIMessage(content='\"Async\" is short for \"asynchronous,\" which refers to a programming paradigm that allows your code to execute tasks in the background without blocking the main execution flow.\\n\\nIn other words, when you\\'re doing something async, your code doesn\\'t wait for the task to complete before moving on to the next line. Instead, it continues executing other tasks while the async task runs in the background.\\n\\nThis is useful for several reasons:\\n\\n1. **Improves responsiveness**: Your code doesn\\'t freeze or become unresponsive while waiting for a task to complete.\\n2. **Enhances scalability**: Async code can handle multiple tasks concurrently, making it ideal for resource-intensive applications.\\n3. **Reduces latency**: By running tasks in the background, you can reduce the time it takes for your application to respond to user input.\\n\\nAsync programming is often used in:\\n\\n1. **Networking**: Sending HTTP requests or handling socket connections.\\n2. **Database operations**: Retrieving or updating data in a database.\\n3. **File I/O**: Reading or writing files.\\n4. **API integrations**: Interacting with external APIs.\\n\\nIn programming languages, async is often achieved through:\\n\\n1. **Coroutines**: Light-weight threads that can suspend and resume execution.\\n2. **Callbacks**: Functions that are called when an async task completes.\\n3. **Promises**: Objects that represent the eventual completion of an async task.\\n4. **Async/await**: A syntax sugar for writing async code that looks synchronous.\\n\\nHere\\'s a simple example in JavaScript:\\n```javascript\\nasync function fetchUser() {\\n  const response = await fetch(\\'https://api.example.com/user\\');\\n  const user = await response.json();\\n  console.log(user);\\n}\\n\\nfetchUser();\\n```\\nIn this example, the `fetchUser` function is async, meaning it will execute in the background. The `await` keyword is used to pause the execution of the function until the `fetch` and `json()` methods complete.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 396, 'prompt_tokens': 94, 'total_tokens': 490, 'completion_time': 0.547859797, 'prompt_time': 0.004999473, 'queue_time': 0.049100137, 'total_time': 0.55285927}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_33e8adf159', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--31ab2d9d-2b86-4343-a341-225ddb349f27-0', usage_metadata={'input_tokens': 94, 'output_tokens': 396, 'total_tokens': 490})]}}, 'parent_ids': ['2d53f1da-e741-44dd-a64f-2d39464ad840']}\n",
      "{'event': 'on_chain_end', 'data': {'output': {'messages': [AIMessage(content='\"Async\" is short for \"asynchronous,\" which refers to a programming paradigm that allows your code to execute tasks in the background without blocking the main execution flow.\\n\\nIn other words, when you\\'re doing something async, your code doesn\\'t wait for the task to complete before moving on to the next line. Instead, it continues executing other tasks while the async task runs in the background.\\n\\nThis is useful for several reasons:\\n\\n1. **Improves responsiveness**: Your code doesn\\'t freeze or become unresponsive while waiting for a task to complete.\\n2. **Enhances scalability**: Async code can handle multiple tasks concurrently, making it ideal for resource-intensive applications.\\n3. **Reduces latency**: By running tasks in the background, you can reduce the time it takes for your application to respond to user input.\\n\\nAsync programming is often used in:\\n\\n1. **Networking**: Sending HTTP requests or handling socket connections.\\n2. **Database operations**: Retrieving or updating data in a database.\\n3. **File I/O**: Reading or writing files.\\n4. **API integrations**: Interacting with external APIs.\\n\\nIn programming languages, async is often achieved through:\\n\\n1. **Coroutines**: Light-weight threads that can suspend and resume execution.\\n2. **Callbacks**: Functions that are called when an async task completes.\\n3. **Promises**: Objects that represent the eventual completion of an async task.\\n4. **Async/await**: A syntax sugar for writing async code that looks synchronous.\\n\\nHere\\'s a simple example in JavaScript:\\n```javascript\\nasync function fetchUser() {\\n  const response = await fetch(\\'https://api.example.com/user\\');\\n  const user = await response.json();\\n  console.log(user);\\n}\\n\\nfetchUser();\\n```\\nIn this example, the `fetchUser` function is async, meaning it will execute in the background. The `await` keyword is used to pause the execution of the function until the `fetch` and `json()` methods complete.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 396, 'prompt_tokens': 94, 'total_tokens': 490, 'completion_time': 0.547859797, 'prompt_time': 0.004999473, 'queue_time': 0.049100137, 'total_time': 0.55285927}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_33e8adf159', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--31ab2d9d-2b86-4343-a341-225ddb349f27-0', usage_metadata={'input_tokens': 94, 'output_tokens': 396, 'total_tokens': 490})]}, 'input': {'messages': [HumanMessage(content='hi chatbot', additional_kwargs={}, response_metadata={}, id='41b7f8e3-3b18-4191-823b-a42731f791aa'), AIMessage(content='How can I help you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 38, 'total_tokens': 46, 'completion_time': 0.013411805, 'prompt_time': 0.001660218, 'queue_time': 0.048211952, 'total_time': 0.015072023}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--bfdab10e-885a-404f-9d3a-88f0e8e7394f-0', usage_metadata={'input_tokens': 38, 'output_tokens': 8, 'total_tokens': 46}), HumanMessage(content='hi chatbot', additional_kwargs={}, response_metadata={}, id='a2272ebc-5f81-4cd4-b412-55d0d48f37f1'), AIMessage(content='It seems like you said hello twice. Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 58, 'total_tokens': 82, 'completion_time': 0.042371427, 'prompt_time': 0.002722783, 'queue_time': 0.052576507, 'total_time': 0.04509421}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--417abb47-1a45-43ff-bb7a-1655734a18a7-0', usage_metadata={'input_tokens': 58, 'output_tokens': 24, 'total_tokens': 82}), HumanMessage(content='what is async', additional_kwargs={}, response_metadata={}, id='4851f01a-b8aa-4d74-8b04-49ca2a05cf13')]}}, 'run_id': '2415c34d-76dc-4638-9b02-58c9c51f5b25', 'name': 'chatbot', 'tags': ['graph:step:7'], 'metadata': {'thread_id': '3', 'langgraph_step': 7, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:ff478b07-263a-888b-3d17-7a80dad9216d', 'revision_id': '3899522'}, 'parent_ids': ['2d53f1da-e741-44dd-a64f-2d39464ad840']}\n",
      "{'event': 'on_chain_stream', 'run_id': '2d53f1da-e741-44dd-a64f-2d39464ad840', 'name': 'LangGraph', 'tags': [], 'metadata': {'thread_id': '3', 'revision_id': '3899522'}, 'data': {'chunk': {'chatbot': {'messages': [AIMessage(content='\"Async\" is short for \"asynchronous,\" which refers to a programming paradigm that allows your code to execute tasks in the background without blocking the main execution flow.\\n\\nIn other words, when you\\'re doing something async, your code doesn\\'t wait for the task to complete before moving on to the next line. Instead, it continues executing other tasks while the async task runs in the background.\\n\\nThis is useful for several reasons:\\n\\n1. **Improves responsiveness**: Your code doesn\\'t freeze or become unresponsive while waiting for a task to complete.\\n2. **Enhances scalability**: Async code can handle multiple tasks concurrently, making it ideal for resource-intensive applications.\\n3. **Reduces latency**: By running tasks in the background, you can reduce the time it takes for your application to respond to user input.\\n\\nAsync programming is often used in:\\n\\n1. **Networking**: Sending HTTP requests or handling socket connections.\\n2. **Database operations**: Retrieving or updating data in a database.\\n3. **File I/O**: Reading or writing files.\\n4. **API integrations**: Interacting with external APIs.\\n\\nIn programming languages, async is often achieved through:\\n\\n1. **Coroutines**: Light-weight threads that can suspend and resume execution.\\n2. **Callbacks**: Functions that are called when an async task completes.\\n3. **Promises**: Objects that represent the eventual completion of an async task.\\n4. **Async/await**: A syntax sugar for writing async code that looks synchronous.\\n\\nHere\\'s a simple example in JavaScript:\\n```javascript\\nasync function fetchUser() {\\n  const response = await fetch(\\'https://api.example.com/user\\');\\n  const user = await response.json();\\n  console.log(user);\\n}\\n\\nfetchUser();\\n```\\nIn this example, the `fetchUser` function is async, meaning it will execute in the background. The `await` keyword is used to pause the execution of the function until the `fetch` and `json()` methods complete.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 396, 'prompt_tokens': 94, 'total_tokens': 490, 'completion_time': 0.547859797, 'prompt_time': 0.004999473, 'queue_time': 0.049100137, 'total_time': 0.55285927}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_33e8adf159', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--31ab2d9d-2b86-4343-a341-225ddb349f27-0', usage_metadata={'input_tokens': 94, 'output_tokens': 396, 'total_tokens': 490})]}}}, 'parent_ids': []}\n",
      "{'event': 'on_chain_end', 'data': {'output': {'messages': [HumanMessage(content='hi chatbot', additional_kwargs={}, response_metadata={}, id='41b7f8e3-3b18-4191-823b-a42731f791aa'), AIMessage(content='How can I help you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 38, 'total_tokens': 46, 'completion_time': 0.013411805, 'prompt_time': 0.001660218, 'queue_time': 0.048211952, 'total_time': 0.015072023}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--bfdab10e-885a-404f-9d3a-88f0e8e7394f-0', usage_metadata={'input_tokens': 38, 'output_tokens': 8, 'total_tokens': 46}), HumanMessage(content='hi chatbot', additional_kwargs={}, response_metadata={}, id='a2272ebc-5f81-4cd4-b412-55d0d48f37f1'), AIMessage(content='It seems like you said hello twice. Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 58, 'total_tokens': 82, 'completion_time': 0.042371427, 'prompt_time': 0.002722783, 'queue_time': 0.052576507, 'total_time': 0.04509421}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--417abb47-1a45-43ff-bb7a-1655734a18a7-0', usage_metadata={'input_tokens': 58, 'output_tokens': 24, 'total_tokens': 82}), HumanMessage(content='what is async', additional_kwargs={}, response_metadata={}, id='4851f01a-b8aa-4d74-8b04-49ca2a05cf13'), AIMessage(content='\"Async\" is short for \"asynchronous,\" which refers to a programming paradigm that allows your code to execute tasks in the background without blocking the main execution flow.\\n\\nIn other words, when you\\'re doing something async, your code doesn\\'t wait for the task to complete before moving on to the next line. Instead, it continues executing other tasks while the async task runs in the background.\\n\\nThis is useful for several reasons:\\n\\n1. **Improves responsiveness**: Your code doesn\\'t freeze or become unresponsive while waiting for a task to complete.\\n2. **Enhances scalability**: Async code can handle multiple tasks concurrently, making it ideal for resource-intensive applications.\\n3. **Reduces latency**: By running tasks in the background, you can reduce the time it takes for your application to respond to user input.\\n\\nAsync programming is often used in:\\n\\n1. **Networking**: Sending HTTP requests or handling socket connections.\\n2. **Database operations**: Retrieving or updating data in a database.\\n3. **File I/O**: Reading or writing files.\\n4. **API integrations**: Interacting with external APIs.\\n\\nIn programming languages, async is often achieved through:\\n\\n1. **Coroutines**: Light-weight threads that can suspend and resume execution.\\n2. **Callbacks**: Functions that are called when an async task completes.\\n3. **Promises**: Objects that represent the eventual completion of an async task.\\n4. **Async/await**: A syntax sugar for writing async code that looks synchronous.\\n\\nHere\\'s a simple example in JavaScript:\\n```javascript\\nasync function fetchUser() {\\n  const response = await fetch(\\'https://api.example.com/user\\');\\n  const user = await response.json();\\n  console.log(user);\\n}\\n\\nfetchUser();\\n```\\nIn this example, the `fetchUser` function is async, meaning it will execute in the background. The `await` keyword is used to pause the execution of the function until the `fetch` and `json()` methods complete.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 396, 'prompt_tokens': 94, 'total_tokens': 490, 'completion_time': 0.547859797, 'prompt_time': 0.004999473, 'queue_time': 0.049100137, 'total_time': 0.55285927}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_33e8adf159', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--31ab2d9d-2b86-4343-a341-225ddb349f27-0', usage_metadata={'input_tokens': 94, 'output_tokens': 396, 'total_tokens': 490})]}}, 'run_id': '2d53f1da-e741-44dd-a64f-2d39464ad840', 'name': 'LangGraph', 'tags': [], 'metadata': {'thread_id': '3', 'revision_id': '3899522'}, 'parent_ids': []}\n",
      "{'event': 'on_chain_start', 'data': {'input': {'messages': ('user', 'how can we achienve it')}}, 'name': 'LangGraph', 'tags': [], 'run_id': '400e563f-fd1b-4e44-b33c-48d304879c1e', 'metadata': {'thread_id': '3'}, 'parent_ids': []}\n",
      "{'event': 'on_chain_start', 'data': {'input': {'messages': [HumanMessage(content='hi chatbot', additional_kwargs={}, response_metadata={}, id='41b7f8e3-3b18-4191-823b-a42731f791aa'), AIMessage(content='How can I help you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 38, 'total_tokens': 46, 'completion_time': 0.013411805, 'prompt_time': 0.001660218, 'queue_time': 0.048211952, 'total_time': 0.015072023}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--bfdab10e-885a-404f-9d3a-88f0e8e7394f-0', usage_metadata={'input_tokens': 38, 'output_tokens': 8, 'total_tokens': 46}), HumanMessage(content='hi chatbot', additional_kwargs={}, response_metadata={}, id='a2272ebc-5f81-4cd4-b412-55d0d48f37f1'), AIMessage(content='It seems like you said hello twice. Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 58, 'total_tokens': 82, 'completion_time': 0.042371427, 'prompt_time': 0.002722783, 'queue_time': 0.052576507, 'total_time': 0.04509421}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--417abb47-1a45-43ff-bb7a-1655734a18a7-0', usage_metadata={'input_tokens': 58, 'output_tokens': 24, 'total_tokens': 82}), HumanMessage(content='what is async', additional_kwargs={}, response_metadata={}, id='4851f01a-b8aa-4d74-8b04-49ca2a05cf13'), AIMessage(content='\"Async\" is short for \"asynchronous,\" which refers to a programming paradigm that allows your code to execute tasks in the background without blocking the main execution flow.\\n\\nIn other words, when you\\'re doing something async, your code doesn\\'t wait for the task to complete before moving on to the next line. Instead, it continues executing other tasks while the async task runs in the background.\\n\\nThis is useful for several reasons:\\n\\n1. **Improves responsiveness**: Your code doesn\\'t freeze or become unresponsive while waiting for a task to complete.\\n2. **Enhances scalability**: Async code can handle multiple tasks concurrently, making it ideal for resource-intensive applications.\\n3. **Reduces latency**: By running tasks in the background, you can reduce the time it takes for your application to respond to user input.\\n\\nAsync programming is often used in:\\n\\n1. **Networking**: Sending HTTP requests or handling socket connections.\\n2. **Database operations**: Retrieving or updating data in a database.\\n3. **File I/O**: Reading or writing files.\\n4. **API integrations**: Interacting with external APIs.\\n\\nIn programming languages, async is often achieved through:\\n\\n1. **Coroutines**: Light-weight threads that can suspend and resume execution.\\n2. **Callbacks**: Functions that are called when an async task completes.\\n3. **Promises**: Objects that represent the eventual completion of an async task.\\n4. **Async/await**: A syntax sugar for writing async code that looks synchronous.\\n\\nHere\\'s a simple example in JavaScript:\\n```javascript\\nasync function fetchUser() {\\n  const response = await fetch(\\'https://api.example.com/user\\');\\n  const user = await response.json();\\n  console.log(user);\\n}\\n\\nfetchUser();\\n```\\nIn this example, the `fetchUser` function is async, meaning it will execute in the background. The `await` keyword is used to pause the execution of the function until the `fetch` and `json()` methods complete.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 396, 'prompt_tokens': 94, 'total_tokens': 490, 'completion_time': 0.547859797, 'prompt_time': 0.004999473, 'queue_time': 0.049100137, 'total_time': 0.55285927}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_33e8adf159', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--31ab2d9d-2b86-4343-a341-225ddb349f27-0', usage_metadata={'input_tokens': 94, 'output_tokens': 396, 'total_tokens': 490}), HumanMessage(content='how can we achienve it', additional_kwargs={}, response_metadata={}, id='797bcb4b-2f5e-4cd6-99da-8bbdf0fb33af')]}}, 'name': 'chatbot', 'tags': ['graph:step:10'], 'run_id': '0b8a5f19-4896-4fe5-a172-d5345aba7ac3', 'metadata': {'thread_id': '3', 'langgraph_step': 10, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:3127ee5f-9e0e-201c-3d60-98b8e4394167'}, 'parent_ids': ['400e563f-fd1b-4e44-b33c-48d304879c1e']}\n",
      "{'event': 'on_chain_stream', 'run_id': '0b8a5f19-4896-4fe5-a172-d5345aba7ac3', 'name': 'chatbot', 'tags': ['graph:step:10'], 'metadata': {'thread_id': '3', 'langgraph_step': 10, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:3127ee5f-9e0e-201c-3d60-98b8e4394167', 'revision_id': '3899522'}, 'data': {'chunk': {'messages': [AIMessage(content='Achieving asynchronous programming in various programming languages and frameworks involves several techniques and tools. Here are some ways to achieve async programming in different languages:\\n\\n**JavaScript**\\n\\n1. **Async/Await**: Use the `async` and `await` keywords to write async code that looks synchronous.\\n2. **Promises**: Use the `Promise` constructor to create a promise that resolves or rejects when an async operation completes.\\n3. **Callbacks**: Pass a callback function as an argument to an async function, which will be called when the async operation completes.\\n4. **Generators**: Use generators to create an iterator that can be paused and resumed when an async operation completes.\\n\\nExample:\\n```javascript\\nasync function fetchUser() {\\n  const response = await fetch(\\'https://api.example.com/user\\');\\n  const user = await response.json();\\n  console.log(user);\\n}\\n\\nfetchUser();\\n```\\n\\n**Python**\\n\\n1. **Async/Await**: Use the `async` and `await` keywords to write async code that looks synchronous.\\n2. **Coroutines**: Use the `asyncio` library to create coroutines that can be executed concurrently.\\n3. **Tasks**: Use the `asyncio` library to create tasks that can be executed concurrently.\\n\\nExample:\\n```python\\nimport asyncio\\n\\nasync def fetch_user():\\n    response = await fetch(\\'https://api.example.com/user\\')\\n    user = await response.json()\\n    print(user)\\n\\nasyncio.run(fetch_user())\\n```\\n\\n**Java**\\n\\n1. **CompletableFuture**: Use the `CompletableFuture` class to create a future that can be completed with a result or an exception.\\n2. **Async/Await**: Use the `CompletableFuture` class to write async code that looks synchronous.\\n3. **Java 8 Parallel Streams**: Use parallel streams to execute a task concurrently.\\n\\nExample:\\n```java\\nimport java.util.concurrent.CompletableFuture;\\n\\nCompletableFuture<String> future = CompletableFuture.supplyAsync(() -> {\\n    // perform some I/O-bound task\\n    return \"Hello, World!\";\\n});\\n\\nfuture.thenAccept(System.out::println);\\n```\\n\\n**C#**\\n\\n1. **Async/Await**: Use the `async` and `await` keywords to write async code that looks synchronous.\\n2. **Tasks**: Use the `Task` class to create a task that can be executed concurrently.\\n3. **PLINQ**: Use parallel LINQ to execute a query concurrently.\\n\\nExample:\\n```csharp\\nusing System.Threading.Tasks;\\n\\nasync Task FetchUserAsync()\\n{\\n    string response = await fetch(\\'https://api.example.com/user\\');\\n    string user = await response.json();\\n    Console.WriteLine(user);\\n}\\n\\nFetchUserAsync();\\n```\\n\\n**Go**\\n\\n1. **Goroutines**: Use goroutines to create concurrent execution of tasks.\\n2. **Channels**: Use channels to communicate between goroutines.\\n\\nExample:\\n```go\\nfunc fetchUser() {\\n    response := fetch(\\'https://api.example.com/user\\')\\n    user := response.json()\\n    fmt.Println(user)\\n}\\n\\ngo fetchUser()\\n```\\n\\n**Node.js**\\n\\n1. **Async/Await**: Use the `async` and `await` keywords to write async code that looks synchronous.\\n2. **Promises**: Use the `Promise` constructor to create a promise that resolves or rejects when an async operation completes.\\n3. **Callbacks**: Pass a callback function as an argument to an async function, which will be called when the async operation completes.\\n\\nExample:\\n```javascript\\nconst fetch = require(\\'node-fetch\\');\\n\\nasync function fetchUser() {\\n    const response = await fetch(\\'https://api.example.com/user\\');\\n    const user = await response.json();\\n    console.log(user);\\n}\\n\\nfetchUser();\\n```\\n\\nThese are just a few examples of how to achieve async programming in different languages and frameworks. The specific techniques and tools used will depend on the language and the requirements of the project.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 781, 'prompt_tokens': 506, 'total_tokens': 1287, 'completion_time': 1.155020839, 'prompt_time': 0.02909305, 'queue_time': 0.04851199, 'total_time': 1.184113889}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--50fbe790-39f3-4904-bcbb-ca8acf3add31-0', usage_metadata={'input_tokens': 506, 'output_tokens': 781, 'total_tokens': 1287})]}}, 'parent_ids': ['400e563f-fd1b-4e44-b33c-48d304879c1e']}\n",
      "{'event': 'on_chain_end', 'data': {'output': {'messages': [AIMessage(content='Achieving asynchronous programming in various programming languages and frameworks involves several techniques and tools. Here are some ways to achieve async programming in different languages:\\n\\n**JavaScript**\\n\\n1. **Async/Await**: Use the `async` and `await` keywords to write async code that looks synchronous.\\n2. **Promises**: Use the `Promise` constructor to create a promise that resolves or rejects when an async operation completes.\\n3. **Callbacks**: Pass a callback function as an argument to an async function, which will be called when the async operation completes.\\n4. **Generators**: Use generators to create an iterator that can be paused and resumed when an async operation completes.\\n\\nExample:\\n```javascript\\nasync function fetchUser() {\\n  const response = await fetch(\\'https://api.example.com/user\\');\\n  const user = await response.json();\\n  console.log(user);\\n}\\n\\nfetchUser();\\n```\\n\\n**Python**\\n\\n1. **Async/Await**: Use the `async` and `await` keywords to write async code that looks synchronous.\\n2. **Coroutines**: Use the `asyncio` library to create coroutines that can be executed concurrently.\\n3. **Tasks**: Use the `asyncio` library to create tasks that can be executed concurrently.\\n\\nExample:\\n```python\\nimport asyncio\\n\\nasync def fetch_user():\\n    response = await fetch(\\'https://api.example.com/user\\')\\n    user = await response.json()\\n    print(user)\\n\\nasyncio.run(fetch_user())\\n```\\n\\n**Java**\\n\\n1. **CompletableFuture**: Use the `CompletableFuture` class to create a future that can be completed with a result or an exception.\\n2. **Async/Await**: Use the `CompletableFuture` class to write async code that looks synchronous.\\n3. **Java 8 Parallel Streams**: Use parallel streams to execute a task concurrently.\\n\\nExample:\\n```java\\nimport java.util.concurrent.CompletableFuture;\\n\\nCompletableFuture<String> future = CompletableFuture.supplyAsync(() -> {\\n    // perform some I/O-bound task\\n    return \"Hello, World!\";\\n});\\n\\nfuture.thenAccept(System.out::println);\\n```\\n\\n**C#**\\n\\n1. **Async/Await**: Use the `async` and `await` keywords to write async code that looks synchronous.\\n2. **Tasks**: Use the `Task` class to create a task that can be executed concurrently.\\n3. **PLINQ**: Use parallel LINQ to execute a query concurrently.\\n\\nExample:\\n```csharp\\nusing System.Threading.Tasks;\\n\\nasync Task FetchUserAsync()\\n{\\n    string response = await fetch(\\'https://api.example.com/user\\');\\n    string user = await response.json();\\n    Console.WriteLine(user);\\n}\\n\\nFetchUserAsync();\\n```\\n\\n**Go**\\n\\n1. **Goroutines**: Use goroutines to create concurrent execution of tasks.\\n2. **Channels**: Use channels to communicate between goroutines.\\n\\nExample:\\n```go\\nfunc fetchUser() {\\n    response := fetch(\\'https://api.example.com/user\\')\\n    user := response.json()\\n    fmt.Println(user)\\n}\\n\\ngo fetchUser()\\n```\\n\\n**Node.js**\\n\\n1. **Async/Await**: Use the `async` and `await` keywords to write async code that looks synchronous.\\n2. **Promises**: Use the `Promise` constructor to create a promise that resolves or rejects when an async operation completes.\\n3. **Callbacks**: Pass a callback function as an argument to an async function, which will be called when the async operation completes.\\n\\nExample:\\n```javascript\\nconst fetch = require(\\'node-fetch\\');\\n\\nasync function fetchUser() {\\n    const response = await fetch(\\'https://api.example.com/user\\');\\n    const user = await response.json();\\n    console.log(user);\\n}\\n\\nfetchUser();\\n```\\n\\nThese are just a few examples of how to achieve async programming in different languages and frameworks. The specific techniques and tools used will depend on the language and the requirements of the project.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 781, 'prompt_tokens': 506, 'total_tokens': 1287, 'completion_time': 1.155020839, 'prompt_time': 0.02909305, 'queue_time': 0.04851199, 'total_time': 1.184113889}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--50fbe790-39f3-4904-bcbb-ca8acf3add31-0', usage_metadata={'input_tokens': 506, 'output_tokens': 781, 'total_tokens': 1287})]}, 'input': {'messages': [HumanMessage(content='hi chatbot', additional_kwargs={}, response_metadata={}, id='41b7f8e3-3b18-4191-823b-a42731f791aa'), AIMessage(content='How can I help you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 38, 'total_tokens': 46, 'completion_time': 0.013411805, 'prompt_time': 0.001660218, 'queue_time': 0.048211952, 'total_time': 0.015072023}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--bfdab10e-885a-404f-9d3a-88f0e8e7394f-0', usage_metadata={'input_tokens': 38, 'output_tokens': 8, 'total_tokens': 46}), HumanMessage(content='hi chatbot', additional_kwargs={}, response_metadata={}, id='a2272ebc-5f81-4cd4-b412-55d0d48f37f1'), AIMessage(content='It seems like you said hello twice. Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 58, 'total_tokens': 82, 'completion_time': 0.042371427, 'prompt_time': 0.002722783, 'queue_time': 0.052576507, 'total_time': 0.04509421}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--417abb47-1a45-43ff-bb7a-1655734a18a7-0', usage_metadata={'input_tokens': 58, 'output_tokens': 24, 'total_tokens': 82}), HumanMessage(content='what is async', additional_kwargs={}, response_metadata={}, id='4851f01a-b8aa-4d74-8b04-49ca2a05cf13'), AIMessage(content='\"Async\" is short for \"asynchronous,\" which refers to a programming paradigm that allows your code to execute tasks in the background without blocking the main execution flow.\\n\\nIn other words, when you\\'re doing something async, your code doesn\\'t wait for the task to complete before moving on to the next line. Instead, it continues executing other tasks while the async task runs in the background.\\n\\nThis is useful for several reasons:\\n\\n1. **Improves responsiveness**: Your code doesn\\'t freeze or become unresponsive while waiting for a task to complete.\\n2. **Enhances scalability**: Async code can handle multiple tasks concurrently, making it ideal for resource-intensive applications.\\n3. **Reduces latency**: By running tasks in the background, you can reduce the time it takes for your application to respond to user input.\\n\\nAsync programming is often used in:\\n\\n1. **Networking**: Sending HTTP requests or handling socket connections.\\n2. **Database operations**: Retrieving or updating data in a database.\\n3. **File I/O**: Reading or writing files.\\n4. **API integrations**: Interacting with external APIs.\\n\\nIn programming languages, async is often achieved through:\\n\\n1. **Coroutines**: Light-weight threads that can suspend and resume execution.\\n2. **Callbacks**: Functions that are called when an async task completes.\\n3. **Promises**: Objects that represent the eventual completion of an async task.\\n4. **Async/await**: A syntax sugar for writing async code that looks synchronous.\\n\\nHere\\'s a simple example in JavaScript:\\n```javascript\\nasync function fetchUser() {\\n  const response = await fetch(\\'https://api.example.com/user\\');\\n  const user = await response.json();\\n  console.log(user);\\n}\\n\\nfetchUser();\\n```\\nIn this example, the `fetchUser` function is async, meaning it will execute in the background. The `await` keyword is used to pause the execution of the function until the `fetch` and `json()` methods complete.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 396, 'prompt_tokens': 94, 'total_tokens': 490, 'completion_time': 0.547859797, 'prompt_time': 0.004999473, 'queue_time': 0.049100137, 'total_time': 0.55285927}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_33e8adf159', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--31ab2d9d-2b86-4343-a341-225ddb349f27-0', usage_metadata={'input_tokens': 94, 'output_tokens': 396, 'total_tokens': 490}), HumanMessage(content='how can we achienve it', additional_kwargs={}, response_metadata={}, id='797bcb4b-2f5e-4cd6-99da-8bbdf0fb33af')]}}, 'run_id': '0b8a5f19-4896-4fe5-a172-d5345aba7ac3', 'name': 'chatbot', 'tags': ['graph:step:10'], 'metadata': {'thread_id': '3', 'langgraph_step': 10, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:3127ee5f-9e0e-201c-3d60-98b8e4394167', 'revision_id': '3899522'}, 'parent_ids': ['400e563f-fd1b-4e44-b33c-48d304879c1e']}\n",
      "{'event': 'on_chain_stream', 'run_id': '400e563f-fd1b-4e44-b33c-48d304879c1e', 'name': 'LangGraph', 'tags': [], 'metadata': {'thread_id': '3', 'revision_id': '3899522'}, 'data': {'chunk': {'chatbot': {'messages': [AIMessage(content='Achieving asynchronous programming in various programming languages and frameworks involves several techniques and tools. Here are some ways to achieve async programming in different languages:\\n\\n**JavaScript**\\n\\n1. **Async/Await**: Use the `async` and `await` keywords to write async code that looks synchronous.\\n2. **Promises**: Use the `Promise` constructor to create a promise that resolves or rejects when an async operation completes.\\n3. **Callbacks**: Pass a callback function as an argument to an async function, which will be called when the async operation completes.\\n4. **Generators**: Use generators to create an iterator that can be paused and resumed when an async operation completes.\\n\\nExample:\\n```javascript\\nasync function fetchUser() {\\n  const response = await fetch(\\'https://api.example.com/user\\');\\n  const user = await response.json();\\n  console.log(user);\\n}\\n\\nfetchUser();\\n```\\n\\n**Python**\\n\\n1. **Async/Await**: Use the `async` and `await` keywords to write async code that looks synchronous.\\n2. **Coroutines**: Use the `asyncio` library to create coroutines that can be executed concurrently.\\n3. **Tasks**: Use the `asyncio` library to create tasks that can be executed concurrently.\\n\\nExample:\\n```python\\nimport asyncio\\n\\nasync def fetch_user():\\n    response = await fetch(\\'https://api.example.com/user\\')\\n    user = await response.json()\\n    print(user)\\n\\nasyncio.run(fetch_user())\\n```\\n\\n**Java**\\n\\n1. **CompletableFuture**: Use the `CompletableFuture` class to create a future that can be completed with a result or an exception.\\n2. **Async/Await**: Use the `CompletableFuture` class to write async code that looks synchronous.\\n3. **Java 8 Parallel Streams**: Use parallel streams to execute a task concurrently.\\n\\nExample:\\n```java\\nimport java.util.concurrent.CompletableFuture;\\n\\nCompletableFuture<String> future = CompletableFuture.supplyAsync(() -> {\\n    // perform some I/O-bound task\\n    return \"Hello, World!\";\\n});\\n\\nfuture.thenAccept(System.out::println);\\n```\\n\\n**C#**\\n\\n1. **Async/Await**: Use the `async` and `await` keywords to write async code that looks synchronous.\\n2. **Tasks**: Use the `Task` class to create a task that can be executed concurrently.\\n3. **PLINQ**: Use parallel LINQ to execute a query concurrently.\\n\\nExample:\\n```csharp\\nusing System.Threading.Tasks;\\n\\nasync Task FetchUserAsync()\\n{\\n    string response = await fetch(\\'https://api.example.com/user\\');\\n    string user = await response.json();\\n    Console.WriteLine(user);\\n}\\n\\nFetchUserAsync();\\n```\\n\\n**Go**\\n\\n1. **Goroutines**: Use goroutines to create concurrent execution of tasks.\\n2. **Channels**: Use channels to communicate between goroutines.\\n\\nExample:\\n```go\\nfunc fetchUser() {\\n    response := fetch(\\'https://api.example.com/user\\')\\n    user := response.json()\\n    fmt.Println(user)\\n}\\n\\ngo fetchUser()\\n```\\n\\n**Node.js**\\n\\n1. **Async/Await**: Use the `async` and `await` keywords to write async code that looks synchronous.\\n2. **Promises**: Use the `Promise` constructor to create a promise that resolves or rejects when an async operation completes.\\n3. **Callbacks**: Pass a callback function as an argument to an async function, which will be called when the async operation completes.\\n\\nExample:\\n```javascript\\nconst fetch = require(\\'node-fetch\\');\\n\\nasync function fetchUser() {\\n    const response = await fetch(\\'https://api.example.com/user\\');\\n    const user = await response.json();\\n    console.log(user);\\n}\\n\\nfetchUser();\\n```\\n\\nThese are just a few examples of how to achieve async programming in different languages and frameworks. The specific techniques and tools used will depend on the language and the requirements of the project.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 781, 'prompt_tokens': 506, 'total_tokens': 1287, 'completion_time': 1.155020839, 'prompt_time': 0.02909305, 'queue_time': 0.04851199, 'total_time': 1.184113889}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--50fbe790-39f3-4904-bcbb-ca8acf3add31-0', usage_metadata={'input_tokens': 506, 'output_tokens': 781, 'total_tokens': 1287})]}}}, 'parent_ids': []}\n",
      "{'event': 'on_chain_end', 'data': {'output': {'messages': [HumanMessage(content='hi chatbot', additional_kwargs={}, response_metadata={}, id='41b7f8e3-3b18-4191-823b-a42731f791aa'), AIMessage(content='How can I help you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 38, 'total_tokens': 46, 'completion_time': 0.013411805, 'prompt_time': 0.001660218, 'queue_time': 0.048211952, 'total_time': 0.015072023}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--bfdab10e-885a-404f-9d3a-88f0e8e7394f-0', usage_metadata={'input_tokens': 38, 'output_tokens': 8, 'total_tokens': 46}), HumanMessage(content='hi chatbot', additional_kwargs={}, response_metadata={}, id='a2272ebc-5f81-4cd4-b412-55d0d48f37f1'), AIMessage(content='It seems like you said hello twice. Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 58, 'total_tokens': 82, 'completion_time': 0.042371427, 'prompt_time': 0.002722783, 'queue_time': 0.052576507, 'total_time': 0.04509421}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--417abb47-1a45-43ff-bb7a-1655734a18a7-0', usage_metadata={'input_tokens': 58, 'output_tokens': 24, 'total_tokens': 82}), HumanMessage(content='what is async', additional_kwargs={}, response_metadata={}, id='4851f01a-b8aa-4d74-8b04-49ca2a05cf13'), AIMessage(content='\"Async\" is short for \"asynchronous,\" which refers to a programming paradigm that allows your code to execute tasks in the background without blocking the main execution flow.\\n\\nIn other words, when you\\'re doing something async, your code doesn\\'t wait for the task to complete before moving on to the next line. Instead, it continues executing other tasks while the async task runs in the background.\\n\\nThis is useful for several reasons:\\n\\n1. **Improves responsiveness**: Your code doesn\\'t freeze or become unresponsive while waiting for a task to complete.\\n2. **Enhances scalability**: Async code can handle multiple tasks concurrently, making it ideal for resource-intensive applications.\\n3. **Reduces latency**: By running tasks in the background, you can reduce the time it takes for your application to respond to user input.\\n\\nAsync programming is often used in:\\n\\n1. **Networking**: Sending HTTP requests or handling socket connections.\\n2. **Database operations**: Retrieving or updating data in a database.\\n3. **File I/O**: Reading or writing files.\\n4. **API integrations**: Interacting with external APIs.\\n\\nIn programming languages, async is often achieved through:\\n\\n1. **Coroutines**: Light-weight threads that can suspend and resume execution.\\n2. **Callbacks**: Functions that are called when an async task completes.\\n3. **Promises**: Objects that represent the eventual completion of an async task.\\n4. **Async/await**: A syntax sugar for writing async code that looks synchronous.\\n\\nHere\\'s a simple example in JavaScript:\\n```javascript\\nasync function fetchUser() {\\n  const response = await fetch(\\'https://api.example.com/user\\');\\n  const user = await response.json();\\n  console.log(user);\\n}\\n\\nfetchUser();\\n```\\nIn this example, the `fetchUser` function is async, meaning it will execute in the background. The `await` keyword is used to pause the execution of the function until the `fetch` and `json()` methods complete.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 396, 'prompt_tokens': 94, 'total_tokens': 490, 'completion_time': 0.547859797, 'prompt_time': 0.004999473, 'queue_time': 0.049100137, 'total_time': 0.55285927}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_33e8adf159', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--31ab2d9d-2b86-4343-a341-225ddb349f27-0', usage_metadata={'input_tokens': 94, 'output_tokens': 396, 'total_tokens': 490}), HumanMessage(content='how can we achienve it', additional_kwargs={}, response_metadata={}, id='797bcb4b-2f5e-4cd6-99da-8bbdf0fb33af'), AIMessage(content='Achieving asynchronous programming in various programming languages and frameworks involves several techniques and tools. Here are some ways to achieve async programming in different languages:\\n\\n**JavaScript**\\n\\n1. **Async/Await**: Use the `async` and `await` keywords to write async code that looks synchronous.\\n2. **Promises**: Use the `Promise` constructor to create a promise that resolves or rejects when an async operation completes.\\n3. **Callbacks**: Pass a callback function as an argument to an async function, which will be called when the async operation completes.\\n4. **Generators**: Use generators to create an iterator that can be paused and resumed when an async operation completes.\\n\\nExample:\\n```javascript\\nasync function fetchUser() {\\n  const response = await fetch(\\'https://api.example.com/user\\');\\n  const user = await response.json();\\n  console.log(user);\\n}\\n\\nfetchUser();\\n```\\n\\n**Python**\\n\\n1. **Async/Await**: Use the `async` and `await` keywords to write async code that looks synchronous.\\n2. **Coroutines**: Use the `asyncio` library to create coroutines that can be executed concurrently.\\n3. **Tasks**: Use the `asyncio` library to create tasks that can be executed concurrently.\\n\\nExample:\\n```python\\nimport asyncio\\n\\nasync def fetch_user():\\n    response = await fetch(\\'https://api.example.com/user\\')\\n    user = await response.json()\\n    print(user)\\n\\nasyncio.run(fetch_user())\\n```\\n\\n**Java**\\n\\n1. **CompletableFuture**: Use the `CompletableFuture` class to create a future that can be completed with a result or an exception.\\n2. **Async/Await**: Use the `CompletableFuture` class to write async code that looks synchronous.\\n3. **Java 8 Parallel Streams**: Use parallel streams to execute a task concurrently.\\n\\nExample:\\n```java\\nimport java.util.concurrent.CompletableFuture;\\n\\nCompletableFuture<String> future = CompletableFuture.supplyAsync(() -> {\\n    // perform some I/O-bound task\\n    return \"Hello, World!\";\\n});\\n\\nfuture.thenAccept(System.out::println);\\n```\\n\\n**C#**\\n\\n1. **Async/Await**: Use the `async` and `await` keywords to write async code that looks synchronous.\\n2. **Tasks**: Use the `Task` class to create a task that can be executed concurrently.\\n3. **PLINQ**: Use parallel LINQ to execute a query concurrently.\\n\\nExample:\\n```csharp\\nusing System.Threading.Tasks;\\n\\nasync Task FetchUserAsync()\\n{\\n    string response = await fetch(\\'https://api.example.com/user\\');\\n    string user = await response.json();\\n    Console.WriteLine(user);\\n}\\n\\nFetchUserAsync();\\n```\\n\\n**Go**\\n\\n1. **Goroutines**: Use goroutines to create concurrent execution of tasks.\\n2. **Channels**: Use channels to communicate between goroutines.\\n\\nExample:\\n```go\\nfunc fetchUser() {\\n    response := fetch(\\'https://api.example.com/user\\')\\n    user := response.json()\\n    fmt.Println(user)\\n}\\n\\ngo fetchUser()\\n```\\n\\n**Node.js**\\n\\n1. **Async/Await**: Use the `async` and `await` keywords to write async code that looks synchronous.\\n2. **Promises**: Use the `Promise` constructor to create a promise that resolves or rejects when an async operation completes.\\n3. **Callbacks**: Pass a callback function as an argument to an async function, which will be called when the async operation completes.\\n\\nExample:\\n```javascript\\nconst fetch = require(\\'node-fetch\\');\\n\\nasync function fetchUser() {\\n    const response = await fetch(\\'https://api.example.com/user\\');\\n    const user = await response.json();\\n    console.log(user);\\n}\\n\\nfetchUser();\\n```\\n\\nThese are just a few examples of how to achieve async programming in different languages and frameworks. The specific techniques and tools used will depend on the language and the requirements of the project.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 781, 'prompt_tokens': 506, 'total_tokens': 1287, 'completion_time': 1.155020839, 'prompt_time': 0.02909305, 'queue_time': 0.04851199, 'total_time': 1.184113889}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--50fbe790-39f3-4904-bcbb-ca8acf3add31-0', usage_metadata={'input_tokens': 506, 'output_tokens': 781, 'total_tokens': 1287})]}}, 'run_id': '400e563f-fd1b-4e44-b33c-48d304879c1e', 'name': 'LangGraph', 'tags': [], 'metadata': {'thread_id': '3', 'revision_id': '3899522'}, 'parent_ids': []}\n",
      "Good Bye!\n"
     ]
    }
   ],
   "source": [
    "# we often want to stream more than grAPH STATE. in particular with chat model calls it is common to stream the tokens as they are generated. \n",
    "# we can do this using astream_events method\n",
    "config = {\"configurable\":{\"thread_id\":\"3\"}}\n",
    "while True:\n",
    "   user_input = input(\"User: \")\n",
    "   if user_input.lower() in [\"quit\",\"q\"]:\n",
    "      print(\"Good Bye!\")\n",
    "      break\n",
    "   async for event in graph_memory.astream_events({'messages':(\"user\",user_input)},config,version='v2'):\n",
    "      print(event)\n",
    "      #for value in event.values():\n",
    "      #   print(value['messages']) # it should user message\n",
    "      #   print(\"Assistant:\", value['messages'][-1].content) # it should have llm model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da888575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c07c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f29dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
