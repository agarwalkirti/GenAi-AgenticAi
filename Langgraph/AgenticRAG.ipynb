{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4aec7a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a277cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001C482BF94E0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001C482BF9DE0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "llm=ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e589b931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hugging face embeddings technique\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "os.environ[\"HUGGING_FACE_API_KEY\"] = os.getenv(\"HUGGING_FACE_API_KEY\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c9c01197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e000da29",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='docs.langchain.com', port=443): Max retries exceeded with url: /oss/python/langgraph/overview/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001C482EA2680>: Failed to resolve 'docs.langchain.com' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\urllib3\\connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\socket.py:967\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    966\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 967\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    968\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\urllib3\\connection.py:753\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 753\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    754\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\urllib3\\connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x000001C482EA2680>: Failed to resolve 'docs.langchain.com' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\requests\\adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='docs.langchain.com', port=443): Max retries exceeded with url: /oss/python/langgraph/overview/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001C482EA2680>: Failed to resolve 'docs.langchain.com' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#langgraph blogs vector tools\u001b[39;00m\n\u001b[0;32m      2\u001b[0m urls\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.langchain.com/oss/python/langgraph/overview/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.langchain.com/oss/python/langgraph/workflows-agents/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.langchain.com/oss/python/langgraph/graph-api/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.langchain.com/oss/python/langgraph/functional-api/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m ]\n\u001b[1;32m----> 8\u001b[0m docs \u001b[38;5;241m=\u001b[39m [WebBaseLoader(urls)\u001b[38;5;241m.\u001b[39mload() \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls]\n\u001b[0;32m      9\u001b[0m docs\n",
      "Cell \u001b[1;32mIn[62], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#langgraph blogs vector tools\u001b[39;00m\n\u001b[0;32m      2\u001b[0m urls\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.langchain.com/oss/python/langgraph/overview/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.langchain.com/oss/python/langgraph/workflows-agents/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.langchain.com/oss/python/langgraph/graph-api/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.langchain.com/oss/python/langgraph/functional-api/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m ]\n\u001b[1;32m----> 8\u001b[0m docs \u001b[38;5;241m=\u001b[39m [\u001b[43mWebBaseLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43murls\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls]\n\u001b[0;32m      9\u001b[0m docs\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\langchain_core\\document_loaders\\base.py:43\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m        the documents.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\langchain_community\\document_loaders\\web_base.py:375\u001b[0m, in \u001b[0;36mWebBaseLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Lazy load text from the url(s) in web_path.\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweb_paths:\n\u001b[1;32m--> 375\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbs_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    376\u001b[0m     text \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mget_text(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbs_get_text_kwargs)\n\u001b[0;32m    377\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m _build_metadata(soup, path)\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\langchain_community\\document_loaders\\web_base.py:357\u001b[0m, in \u001b[0;36mWebBaseLoader._scrape\u001b[1;34m(self, url, parser, bs_kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m         parser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_parser\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_parser(parser)\n\u001b[1;32m--> 357\u001b[0m html_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mget(url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequests_kwargs)\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraise_for_status:\n\u001b[0;32m    359\u001b[0m     html_doc\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\requests\\adapters.py:677\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    674\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    675\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='docs.langchain.com', port=443): Max retries exceeded with url: /oss/python/langgraph/overview/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001C482EA2680>: Failed to resolve 'docs.langchain.com' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "#langgraph blogs vector tools\n",
    "urls=[\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/overview/\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/workflows-agents/\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/graph-api/\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/functional-api/\"\n",
    "]\n",
    "docs = [WebBaseLoader(urls).load() for url in urls]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941d8083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content=\"LangGraph overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeEnglishcloseOn this page InstallCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageCopy pageLangGraph v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='Trusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b Install\\npipuvCopyAsk AIpip install -U langgraph'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='Then, create a simple hello world example:\\nCopyAsk AIfrom langgraph.graph import StateGraph, MessagesState, START, END\\n\\ndef mock_llm(state: MessagesState):\\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\\n\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(mock_llm)\\ngraph.add_edge(START, \"mock_llm\")\\ngraph.add_edge(\"mock_llm\", END)\\ngraph = graph.compile()\\n\\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\\n\\n\\u200bCore benefits\\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='Durable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\nHuman-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.\\nComprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\nDebugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\nProduction-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\\n\\n\\u200bLangGraph ecosystem\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='LangSmith — Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\\nLangSmith — Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in Studio.\\nLangChain - Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\\n\\n\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content=\"Edit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoWhat's new in v1Next⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content=\"Workflows and agents - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedWorkflows and agentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeEnglishcloseOn this pageSetupLLMs and augmentationsPrompt chainingParallelizationRoutingOrchestrator-workerCreating workers in LangGraphEvaluator-optimizerAgentsGet startedWorkflows and agentsCopy pageCopy pageThis guide reviews common workflow and agent patterns.\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Workflows have predetermined code paths and are designed to operate in a certain order.\\nAgents are dynamic and define their own processes and tool usage.\\n\\n\\nLangGraph offers several benefits when building agents and workflows, including persistence, streaming, and support for debugging as well as deployment.\\n\\u200bSetup\\nTo build a workflow or agent, you can use any chat model that supports structured outputs and tool calling. The following example uses Anthropic:\\n\\nInstall dependencies:\\n\\nCopyAsk AIpip install langchain_core langchain-anthropic langgraph\\n\\n\\nInitialize the LLM:\\n\\nCopyAsk AIimport os\\nimport getpass\\n\\nfrom langchain_anthropic import ChatAnthropic\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\\n\\n\\u200bLLMs and augmentations\\nWorkflows and agentic systems are based on LLMs and the various augmentations you add to them. Tool calling, structured outputs, and short term memory are a few options for tailoring LLMs to your needs.\\n\\nCopyAsk AI# Schema for structured output\\nfrom pydantic import BaseModel, Field\\n\\n\\nclass SearchQuery(BaseModel):\\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\\n    justification: str = Field(\\n        None, description=\"Why this query is relevant to the user\\'s request.\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nstructured_llm = llm.with_structured_output(SearchQuery)\\n\\n# Invoke the augmented LLM\\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\\n\\n# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b\\n\\n# Augment the LLM with tools\\nllm_with_tools = llm.bind_tools([multiply])\\n\\n# Invoke the LLM with input that triggers the tool call\\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\\n\\n# Get the tool call\\nmsg.tool_calls\\n\\n\\u200bPrompt chaining\\nPrompt chaining is when each LLM call processes the output of the previous call. It’s often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:\\n\\nTranslating documents into different languages\\nVerifying generated content for consistency\\n\\n\\nGraph APIFunctional APICopyAsk AIfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom IPython.display import Image, display\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    improved_joke: str\\n    final_joke: str\\n\\n\\n# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a short joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef check_punchline(state: State):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\\n        return \"Pass\"\\n    return \"Fail\"\\n\\n\\ndef improve_joke(state: State):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state[\\'joke\\']}\")\\n    return {\"improved_joke\": msg.content}\\n\\n\\ndef polish_joke(state: State):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state[\\'improved_joke\\']}\")\\n    return {\"final_joke\": msg.content}\\n\\n\\n# Build workflow\\nworkflow = StateGraph(State)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Build workflow\\nworkflow = StateGraph(State)\\n\\n# Add nodes\\nworkflow.add_node(\"generate_joke\", generate_joke)\\nworkflow.add_node(\"improve_joke\", improve_joke)\\nworkflow.add_node(\"polish_joke\", polish_joke)\\n\\n# Add edges to connect nodes\\nworkflow.add_edge(START, \"generate_joke\")\\nworkflow.add_conditional_edges(\\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\\n)\\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\\nworkflow.add_edge(\"polish_joke\", END)\\n\\n# Compile\\nchain = workflow.compile()\\n\\n# Show workflow\\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = chain.invoke({\"topic\": \"cats\"})\\nprint(\"Initial joke:\")\\nprint(state[\"joke\"])\\nprint(\"\\\\n--- --- ---\\\\n\")\\nif \"improved_joke\" in state:\\n    print(\"Improved joke:\")\\n    print(state[\"improved_joke\"])\\n    print(\"\\\\n--- --- ---\\\\n\")\\n\\n    print(\"Final joke:\")\\n    print(state[\"final_joke\"])\\nelse:\\n    print(\"Joke failed quality gate - no punchline detected!\")'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='\\u200bParallelization\\nWith parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:\\n\\nSplit up subtasks and run them in parallel, which increases speed\\nRun tasks multiple times to check for different outputs, which increases confidence\\n\\nSome examples include:\\n\\nRunning one subtask that processes a document for keywords, and a second subtask to check for formatting errors\\nRunning a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources\\n\\n\\nGraph APIFunctional APICopyAsk AI# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    story: str\\n    poem: str\\n    combined_output: str\\n\\n\\n# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef call_llm_2(state: State):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n\\n    msg = llm.invoke(f\"Write a story about {state[\\'topic\\']}\")\\n    return {\"story\": msg.content}\\n\\n\\ndef call_llm_3(state: State):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n\\n    msg = llm.invoke(f\"Write a poem about {state[\\'topic\\']}\")\\n    return {\"poem\": msg.content}\\n\\n\\ndef aggregator(state: State):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {state[\\'topic\\']}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{state[\\'story\\']}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{state[\\'joke\\']}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{state[\\'poem\\']}\"\\n    return {\"combined_output\": combined}\\n\\n\\n# Build workflow\\nparallel_builder = StateGraph(State)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Build workflow\\nparallel_builder = StateGraph(State)\\n\\n# Add nodes\\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\\nparallel_builder.add_node(\"aggregator\", aggregator)\\n\\n# Add edges to connect nodes\\nparallel_builder.add_edge(START, \"call_llm_1\")\\nparallel_builder.add_edge(START, \"call_llm_2\")\\nparallel_builder.add_edge(START, \"call_llm_3\")\\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\\nparallel_builder.add_edge(\"aggregator\", END)\\nparallel_workflow = parallel_builder.compile()\\n\\n# Show workflow\\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])\\n\\n\\u200bRouting\\nRouting workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.\\n\\nGraph APIFunctional APICopyAsk AIfrom typing_extensions import Literal\\nfrom langchain.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str\\n\\n\\n# Nodes\\ndef llm_call_1(state: State):\\n    \"\"\"Write a story\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_2(state: State):\\n    \"\"\"Write a joke\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_3(state: State):\\n    \"\"\"Write a poem\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_router(state: State):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=state[\"input\"]),\\n        ]\\n    )\\n\\n    return {\"decision\": decision.step}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='return {\"decision\": decision.step}\\n\\n\\n# Conditional edge function to route to the appropriate node\\ndef route_decision(state: State):\\n    # Return the node name you want to visit next\\n    if state[\"decision\"] == \"story\":\\n        return \"llm_call_1\"\\n    elif state[\"decision\"] == \"joke\":\\n        return \"llm_call_2\"\\n    elif state[\"decision\"] == \"poem\":\\n        return \"llm_call_3\"\\n\\n\\n# Build workflow\\nrouter_builder = StateGraph(State)\\n\\n# Add nodes\\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Add edges to connect nodes\\nrouter_builder.add_edge(START, \"llm_call_router\")\\nrouter_builder.add_conditional_edges(\\n    \"llm_call_router\",\\n    route_decision,\\n    {  # Name returned by route_decision : Name of next node to visit\\n        \"llm_call_1\": \"llm_call_1\",\\n        \"llm_call_2\": \"llm_call_2\",\\n        \"llm_call_3\": \"llm_call_3\",\\n    },\\n)\\nrouter_builder.add_edge(\"llm_call_1\", END)\\nrouter_builder.add_edge(\"llm_call_2\", END)\\nrouter_builder.add_edge(\"llm_call_3\", END)\\n\\n# Compile workflow\\nrouter_workflow = router_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\\nprint(state[\"output\"])\\n\\n\\u200bOrchestrator-worker\\nIn an orchestrator-worker configuration, the orchestrator:\\n\\nBreaks down tasks into subtasks\\nDelegates subtasks to workers\\nSynthesizes worker outputs into a final result'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Orchestrator-worker workflows provide more flexibility and are often used when subtasks cannot be predefined the way they can with parallelization. This is common with workflows that write code or need to update content across multiple files. For example, a workflow that needs to update installation instructions for multiple Python libraries across an unknown number of documents might use this pattern.\\nGraph APIFunctional APICopyAsk AIfrom typing import Annotated, List\\nimport operator\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\n\\u200bCreating workers in LangGraph\\nOrchestrator-worker workflows are common and LangGraph has built-in support for them. The Send API lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the Send API to send a section to each worker.\\nCopyAsk AIfrom langgraph.types import Send\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str  # Report topic\\n    sections: list[Section]  # List of report sections\\n    completed_sections: Annotated[\\n        list, operator.add\\n    ]  # All workers write to this key in parallel\\n    final_report: str  # Final report'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Worker state\\nclass WorkerState(TypedDict):\\n    section: Section\\n    completed_sections: Annotated[list, operator.add]\\n\\n\\n# Nodes\\ndef orchestrator(state: State):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {state[\\'topic\\']}\"),\\n        ]\\n    )\\n\\n    return {\"sections\": report_sections.sections}\\n\\n\\ndef llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='def llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    section = llm.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\\n            ),\\n            HumanMessage(\\n                content=f\"Here is the section name: {state[\\'section\\'].name} and description: {state[\\'section\\'].description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return {\"completed_sections\": [section.content]}\\n\\n\\ndef synthesizer(state: State):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n\\n    # List of completed sections\\n    completed_sections = state[\"completed_sections\"]\\n\\n    # Format completed section to str to use as context for final sections\\n    completed_report_sections = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='return {\"final_report\": completed_report_sections}\\n\\n\\n# Conditional edge function to create llm_call workers that each write a section of the report\\ndef assign_workers(state: State):\\n    \"\"\"Assign a worker to each section in the plan\"\"\"\\n\\n    # Kick off section writing in parallel via Send() API\\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\\n\\n\\n# Build workflow\\norchestrator_worker_builder = StateGraph(State)\\n\\n# Add the nodes\\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\\n\\n# Add edges to connect nodes\\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\\norchestrator_worker_builder.add_conditional_edges(\\n    \"orchestrator\", assign_workers, [\"llm_call\"]\\n)\\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\\norchestrator_worker_builder.add_edge(\"synthesizer\", END)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Compile the workflow\\norchestrator_worker = orchestrator_worker_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\\n\\nfrom IPython.display import Markdown\\nMarkdown(state[\"final_report\"])'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='from IPython.display import Markdown\\nMarkdown(state[\"final_report\"])\\n\\n\\u200bEvaluator-optimizer\\nIn evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a human-in-the-loop determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.\\nEvaluator-optimizer workflows are commonly used when there’s particular success criteria for a task, but iteration is required to meet that criteria. For example, there’s not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.\\n\\nGraph APIFunctional APICopyAsk AI# Graph state\\nclass State(TypedDict):\\n    joke: str\\n    topic: str\\n    feedback: str\\n    funny_or_not: str'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\ndef llm_call_generator(state: State):\\n    \"\"\"LLM generates a joke\"\"\"\\n\\n    if state.get(\"feedback\"):\\n        msg = llm.invoke(\\n            f\"Write a joke about {state[\\'topic\\']} but take into account the feedback: {state[\\'feedback\\']}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef llm_call_evaluator(state: State):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n\\n    grade = evaluator.invoke(f\"Grade the joke {state[\\'joke\\']}\")\\n    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\\ndef route_joke(state: State):\\n    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\\n\\n    if state[\"funny_or_not\"] == \"funny\":\\n        return \"Accepted\"\\n    elif state[\"funny_or_not\"] == \"not funny\":\\n        return \"Rejected + Feedback\"\\n\\n\\n# Build workflow\\noptimizer_builder = StateGraph(State)\\n\\n# Add the nodes\\noptimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\\noptimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\\n\\n# Add edges to connect nodes\\noptimizer_builder.add_edge(START, \"llm_call_generator\")\\noptimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\\noptimizer_builder.add_conditional_edges(\\n    \"llm_call_evaluator\",\\n    route_joke,\\n    {  # Name returned by route_joke : Name of next node to visit\\n        \"Accepted\": END,\\n        \"Rejected + Feedback\": \"llm_call_generator\",\\n    },\\n)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Compile the workflow\\noptimizer_workflow = optimizer_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = optimizer_workflow.invoke({\"topic\": \"Cats\"})\\nprint(state[\"joke\"])\\n\\n\\u200bAgents\\nAgents are typically implemented as an LLM performing actions using tools. They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.\\n\\nTo get started with agents, see the quickstart or read more about how they work in LangChain.\\nUsing toolsCopyAsk AIfrom langchain.tools import tool\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a * b'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nllm_with_tools = llm.bind_tools(tools)\\n\\nGraph APIFunctional APICopyAsk AIfrom langgraph.graph import MessagesState\\nfrom langchain.messages import SystemMessage, HumanMessage, ToolMessage\\n\\n\\n# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            llm_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ]\\n    }\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\\ndef should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"tool_node\"\\n\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n\\n# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"tool_node\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    [\"tool_node\", END]\\n)\\nagent_builder.add_edge(\"tool_node\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoThinking in LangGraphPreviousPersistenceNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content=\"Graph API overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationGraph APIGraph API overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIGraph APIUse the graph APIFunctional APIRuntimeEnglishcloseOn this pageGraphsStateGraphCompiling your graphStateSchemaMultiple schemasReducersDefault ReducerOverwriteWorking with Messages in Graph StateWhy use messages?Using Messages in your GraphSerializationMessagesStateNodesSTART NodeEND NodeNode CachingEdgesNormal EdgesConditional\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='your GraphSerializationMessagesStateNodesSTART NodeEND NodeNode CachingEdgesNormal EdgesConditional EdgesEntry PointConditional Entry PointSendCommandWhen should I use Command instead of conditional edges?Navigating to a node in a parent graphUsing inside toolsHuman-in-the-loopGraph MigrationsRuntime ContextRecursion LimitAccessing and handling the recursion counterHow it worksAccessing the current step counterProactive recursion handlingProactive vs reactive approachesOther available metadataVisualizationLangGraph APIsGraph APIGraph API overviewCopy pageCopy page\\u200bGraphs'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='State: A shared data structure that represents the current snapshot of your application. It can be any data type, but is typically defined using a shared state schema.\\n\\n\\nNodes: Functions that encode the logic of your agents. They receive the current state as input, perform some computation or side-effect, and return an updated state.\\n\\n\\nEdges: Functions that determine which Node to execute next based on the current state. They can be conditional branches or fixed transitions.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='By composing Nodes and Edges, you can create complex, looping workflows that evolve the state over time. The real power, though, comes from how LangGraph manages that state. To emphasize: Nodes and Edges are nothing more than functions - they can contain an LLM or just good ol’ code.\\nIn short: nodes do the work, edges tell what to do next.\\nLangGraph’s underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google’s Pregel system, the program proceeds in discrete “super-steps.”'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or “channels”). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.\\n\\u200bStateGraph\\nThe StateGraph class is the main graph class to use. This is parameterized by a user defined State object.\\n\\u200bCompiling your graph\\nTo build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed?'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method:\\nCopyAsk AIgraph = graph_builder.compile(...)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='You MUST compile your graph before you can use it.\\n\\u200bState\\nThe first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.\\n\\u200bSchema\\nThe main documented way to specify the schema of a graph is by using a TypedDict. If you want to provide default values in your state, use a dataclass. We also support using a Pydantic BaseModel as your graph state if you want recursive data validation (though note that pydantic is less performant than a TypedDict or dataclass).'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the guide here for how to use.\\n\\u200bMultiple schemas\\nTypically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Internal nodes can pass information that is not required in the graph’s input / output.\\nWe may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key.\\n\\nIt is possible to have nodes write to private state channels inside the graph for internal node communication. We can simply define a private schema, PrivateState.\\nIt is also possible to define explicit input and output schemas for a graph. In these cases, we define an “internal” schema that contains all keys relevant to graph operations. But, we also define input and output schemas that are sub-sets of the “internal” schema to constrain the input and output of the graph. See this guide for more detail.\\nLet’s look at an example:\\nCopyAsk AIclass InputState(TypedDict):\\n    user_input: str\\n\\nclass OutputState(TypedDict):\\n    graph_output: str\\n\\nclass OverallState(TypedDict):\\n    foo: str\\n    user_input: str\\n    graph_output: str'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='class OverallState(TypedDict):\\n    foo: str\\n    user_input: str\\n    graph_output: str\\n\\nclass PrivateState(TypedDict):\\n    bar: str\\n\\ndef node_1(state: InputState) -> OverallState:\\n    # Write to OverallState\\n    return {\"foo\": state[\"user_input\"] + \" name\"}\\n\\ndef node_2(state: OverallState) -> PrivateState:\\n    # Read from OverallState, write to PrivateState\\n    return {\"bar\": state[\"foo\"] + \" is\"}\\n\\ndef node_3(state: PrivateState) -> OutputState:\\n    # Read from PrivateState, write to OutputState\\n    return {\"graph_output\": state[\"bar\"] + \" Lance\"}\\n\\nbuilder = StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)\\nbuilder.add_node(\"node_1\", node_1)\\nbuilder.add_node(\"node_2\", node_2)\\nbuilder.add_node(\"node_3\", node_3)\\nbuilder.add_edge(START, \"node_1\")\\nbuilder.add_edge(\"node_1\", \"node_2\")\\nbuilder.add_edge(\"node_2\", \"node_3\")\\nbuilder.add_edge(\"node_3\", END)\\n\\ngraph = builder.compile()\\ngraph.invoke({\"user_input\":\"My\"})\\n# {\\'graph_output\\': \\'My name is Lance\\'}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='graph = builder.compile()\\ngraph.invoke({\"user_input\":\"My\"})\\n# {\\'graph_output\\': \\'My name is Lance\\'}\\n\\nThere are two subtle and important points to note here:\\n\\n\\nWe pass state: InputState as the input schema to node_1. But, we write out to foo, a channel in OverallState. How can we write out to a state channel that is not included in the input schema? This is because a node can write to any state channel in the graph state. The graph state is the union of the state channels defined at initialization, which includes OverallState and the filters InputState and OutputState.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='We initialize the graph with StateGraph(OverallState,input_schema=InputState,output_schema=OutputState). So, how can we write to PrivateState in node_2? How does the graph gain access to this schema if it was not passed in the StateGraph initialization? We can do this because nodes can also declare additional state channels as long as the state schema definition exists. In this case, the PrivateState schema is defined, so we can add bar as a new state channel in the graph and write to it.\\n\\n\\n\\u200bReducers\\nReducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer:\\n\\u200bDefault Reducer\\nThese two examples show how to use the default reducer:\\nExample A:\\nCopyAsk AIfrom typing_extensions import TypedDict'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='class State(TypedDict):\\n    foo: int\\n    bar: list[str]\\n\\nIn this example, no reducer functions are specified for any key. Let’s assume the input to the graph is:\\n{\"foo\": 1, \"bar\": [\"hi\"]}. Let’s then assume the first Node returns {\"foo\": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {\"foo\": 2, \"bar\": [\"hi\"]}. If the second node returns {\"bar\": [\"bye\"]} then the State would then be {\"foo\": 2, \"bar\": [\"bye\"]}\\nExample B:\\nCopyAsk AIfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\nfrom operator import add\\n\\nclass State(TypedDict):\\n    foo: int\\n    bar: Annotated[list[str], add]'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='In this example, we’ve used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let’s assume the input to the graph is {\"foo\": 1, \"bar\": [\"hi\"]}. Let’s then assume the first Node returns {\"foo\": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {\"foo\": 2, \"bar\": [\"hi\"]}. If the second node returns {\"bar\": [\"bye\"]} then the State would then be {\"foo\": 2, \"bar\": [\"hi\", \"bye\"]}. Notice here that the bar key is updated by adding the two lists together.\\n\\u200bOverwrite\\nIn some cases, you may want to bypass a reducer and directly overwrite a state value. LangGraph provides the Overwrite type for this purpose. Learn how to use Overwrite here.\\n\\u200bWorking with Messages in Graph State\\n\\u200bWhy use messages?'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bWorking with Messages in Graph State\\n\\u200bWhy use messages?\\nMost modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain’s ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide.\\n\\u200bUsing Messages in your Graph'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bUsing Messages in your Graph\\nIn many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don’t specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.\\n\\u200bSerialization\\nIn addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format:\\nCopyAsk AI# this is supported'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='CopyAsk AI# this is supported\\n{\"messages\": [HumanMessage(content=\"message\")]}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='# and this is also supported\\n{\"messages\": [{\"type\": \"human\", \"content\": \"message\"}]}\\n\\nSince the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[\"messages\"][-1].content. Below is an example of a graph that uses add_messages as its reducer function.\\nCopyAsk AIfrom langchain.messages import AnyMessage\\nfrom langgraph.graph.message import add_messages\\nfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\n\\nclass GraphState(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='class GraphState(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]\\n\\n\\u200bMessagesState\\nSince having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:\\nCopyAsk AIfrom langgraph.graph import MessagesState\\n\\nclass State(MessagesState):\\n    documents: list[str]\\n\\n\\u200bNodes\\nIn LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments:\\n\\nstate: The state of the graph\\nconfig: A RunnableConfig object that contains configuration information like thread_id and tracing information like tags\\nruntime: A Runtime object that contains runtime context and other information like store and stream_writer'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Similar to NetworkX, you add these nodes to a graph using the add_node method:\\nCopyAsk AIfrom dataclasses import dataclass\\nfrom typing_extensions import TypedDict\\n\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph\\nfrom langgraph.runtime import Runtime\\n\\nclass State(TypedDict):\\n    input: str\\n    results: str\\n\\n@dataclass\\nclass Context:\\n    user_id: str\\n\\nbuilder = StateGraph(State)\\n\\ndef plain_node(state: State):\\n    return state\\n\\ndef node_with_runtime(state: State, runtime: Runtime[Context]):\\n    print(\"In node: \", runtime.context.user_id)\\n    return {\"results\": f\"Hello, {state[\\'input\\']}!\"}\\n\\ndef node_with_config(state: State, config: RunnableConfig):\\n    print(\"In node with thread_id: \", config[\"configurable\"][\"thread_id\"])\\n    return {\"results\": f\"Hello, {state[\\'input\\']}!\"}\\n\\n\\nbuilder.add_node(\"plain_node\", plain_node)\\nbuilder.add_node(\"node_with_runtime\", node_with_runtime)\\nbuilder.add_node(\"node_with_config\", node_with_config)\\n...'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Behind the scenes, functions are converted to RunnableLambdas, which add batch and async support to your function, along with native tracing and debugging.\\nIf you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name.\\nCopyAsk AIbuilder.add_node(my_node)\\n# You can then create edges to/from this node by referencing it as `\"my_node\"`\\n\\n\\u200bSTART Node\\nThe START Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.\\nCopyAsk AIfrom langgraph.graph import START\\n\\ngraph.add_edge(START, \"node_a\")\\n\\n\\u200bEND Node\\nThe END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.\\nCopyAsk AIfrom langgraph.graph import END\\n\\ngraph.add_edge(\"node_a\", END)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='graph.add_edge(\"node_a\", END)\\n\\n\\u200bNode Caching\\nLangGraph supports caching of tasks/nodes based on the input to the node. To use caching:\\n\\nSpecify a cache when compiling a graph (or specifying an entrypoint)\\nSpecify a cache policy for nodes. Each cache policy supports:\\n\\nkey_func used to generate a cache key based on the input to a node, which defaults to a hash of the input with pickle.\\nttl, the time to live for the cache in seconds. If not specified, the cache will never expire.\\n\\n\\n\\nFor example:\\nCopyAsk AIimport time\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph\\nfrom langgraph.cache.memory import InMemoryCache\\nfrom langgraph.types import CachePolicy\\n\\n\\nclass State(TypedDict):\\n    x: int\\n    result: int\\n\\n\\nbuilder = StateGraph(State)\\n\\n\\ndef expensive_node(state: State) -> dict[str, int]:\\n    # expensive computation\\n    time.sleep(2)\\n    return {\"result\": state[\"x\"] * 2}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='builder.add_node(\"expensive_node\", expensive_node, cache_policy=CachePolicy(ttl=3))\\nbuilder.set_entry_point(\"expensive_node\")\\nbuilder.set_finish_point(\"expensive_node\")\\n\\ngraph = builder.compile(cache=InMemoryCache())\\n\\nprint(graph.invoke({\"x\": 5}, stream_mode=\\'updates\\'))    \\n# [{\\'expensive_node\\': {\\'result\\': 10}}]\\nprint(graph.invoke({\"x\": 5}, stream_mode=\\'updates\\'))    \\n# [{\\'expensive_node\\': {\\'result\\': 10}, \\'__metadata__\\': {\\'cached\\': True}}]\\n\\n\\nFirst run takes two seconds to run (due to mocked expensive computation).\\nSecond run utilizes cache and returns quickly.\\n\\n\\u200bEdges\\nEdges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Normal Edges: Go directly from one node to the next.\\nConditional Edges: Call a function to determine which node(s) to go to next.\\nEntry Point: Which node to call first when user input arrives.\\nConditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.\\n\\nA node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.\\n\\u200bNormal Edges\\nIf you always want to go from node A to node B, you can use the add_edge method directly.\\nCopyAsk AIgraph.add_edge(\"node_a\", \"node_b\")\\n\\n\\u200bConditional Edges\\nIf you want to optionally route to one or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a “routing function” to call after that node is executed:\\nCopyAsk AIgraph.add_conditional_edges(\"node_a\", routing_function)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Similar to nodes, the routing_function accepts the current state of the graph and returns a value.\\nBy default, the return value routing_function is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.\\nYou can optionally provide a dictionary that maps the routing_function’s output to the name of the next node.\\nCopyAsk AIgraph.add_conditional_edges(\"node_a\", routing_function, {True: \"node_b\", False: \"node_c\"})\\n\\nUse Command instead of conditional edges if you want to combine state updates and routing in a single function.\\n\\u200bEntry Point\\nThe entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph.\\nCopyAsk AIfrom langgraph.graph import START\\n\\ngraph.add_edge(START, \"node_a\")'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='graph.add_edge(START, \"node_a\")\\n\\n\\u200bConditional Entry Point\\nA conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this.\\nCopyAsk AIfrom langgraph.graph import START\\n\\ngraph.add_conditional_edges(START, routing_function)\\n\\nYou can optionally provide a dictionary that maps the routing_function’s output to the name of the next node.\\nCopyAsk AIgraph.add_conditional_edges(START, routing_function, {True: \"node_b\", False: \"node_c\"})'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bSend\\nBy default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object).\\nTo support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node.\\nCopyAsk AIdef continue_to_jokes(state: OverallState):\\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\\'subjects\\']]'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='graph.add_conditional_edges(\"node_a\", continue_to_jokes)\\n\\n\\u200bCommand\\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:\\nCopyAsk AIdef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        # state update\\n        update={\"foo\": \"bar\"},\\n        # control flow\\n        goto=\"my_other_node\"\\n    )\\n\\nWith Command you can also achieve dynamic control flow behavior (identical to conditional edges):\\nCopyAsk AIdef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    if state[\"foo\"] == \"bar\":\\n        return Command(update={\"foo\": \"baz\"}, goto=\"my_other_node\")'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='When returning Command in your node functions, you must add return type annotations with the list of node names the node is routing to, e.g. Command[Literal[\"my_other_node\"]]. This is necessary for the graph rendering and tells LangGraph that my_node can navigate to my_other_node.\\nCheck out this how-to guide for an end-to-end example of how to use Command.\\n\\u200bWhen should I use Command instead of conditional edges?\\n\\nUse Command when you need to both update the graph state and route to a different node. For example, when implementing multi-agent handoffs where it’s important to route to a different agent and pass some information to that agent.\\nUse conditional edges to route between nodes conditionally without updating the state.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bNavigating to a node in a parent graph\\nIf you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify graph=Command.PARENT in Command:\\nCopyAsk AIdef my_node(state: State) -> Command[Literal[\"other_subgraph\"]]:\\n    return Command(\\n        update={\"foo\": \"bar\"},\\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\\n        graph=Command.PARENT\\n    )'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Setting graph to Command.PARENT will navigate to the closest parent graph.When you send updates from a subgraph node to a parent graph node for a key that’s shared by both parent and subgraph state schemas, you must define a reducer for the key you’re updating in the parent graph state. See this example.\\nThis is particularly useful when implementing multi-agent handoffs.\\nCheck out this guide for detail.\\n\\u200bUsing inside tools\\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation.\\nRefer to this guide for detail.\\n\\u200bHuman-in-the-loop\\nCommand is an important part of human-in-the-loop workflows: when using interrupt() to collect user input, Command is then used to supply the input and resume execution via Command(resume=\"User input\"). Check out this conceptual guide for more information.\\n\\u200bGraph Migrations'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bGraph Migrations\\nLangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)\\nFor threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) — if this is a blocker please reach out and we can prioritize a solution.\\nFor modifying state, we have full backwards and forwards compatibility for adding and removing keys\\nState keys that are renamed lose their saved state in existing threads\\nState keys whose types change in incompatible ways could currently cause issues in threads with state from before the change — if this is a blocker please reach out and we can prioritize a solution.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bRuntime Context\\nWhen creating a graph, you can specify a context_schema for runtime context passed to nodes. This is useful for passing\\ninformation to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection.\\nCopyAsk AI@dataclass\\nclass ContextSchema:\\n    llm_provider: str = \"openai\"\\n\\ngraph = StateGraph(State, context_schema=ContextSchema)\\n\\nYou can then pass this context into the graph using the context parameter of the invoke method.\\nCopyAsk AIgraph.invoke(inputs, context={\"llm_provider\": \"anthropic\"})\\n\\nYou can then access and use this context inside a node or conditional edge:\\nCopyAsk AIfrom langgraph.runtime import Runtime\\n\\ndef node_a(state: State, runtime: Runtime[ContextSchema]):\\n    llm = get_llm(runtime.context.llm_provider)\\n    # ...'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='See this guide for a full breakdown on configuration.\\n\\u200bRecursion Limit\\nThe recursion limit sets the maximum number of super-steps the graph can execute during a single execution. Once the limit is reached, LangGraph will raise GraphRecursionError. By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to invoke/stream via the config dictionary. Importantly, recursion_limit is a standalone config key and should not be passed inside the configurable key as all other user-defined configuration. See the example below:\\nCopyAsk AIgraph.invoke(inputs, config={\"recursion_limit\": 5}, context={\"llm\": \"anthropic\"})'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Read this how-to to learn more about how the recursion limit works.\\n\\u200bAccessing and handling the recursion counter\\nThe current step counter is accessible in config[\"metadata\"][\"langgraph_step\"] within any node, allowing for proactive recursion handling before hitting the recursion limit. This enables you to implement graceful degradation strategies within your graph logic.\\n\\u200bHow it works\\nThe step counter is stored in config[\"metadata\"][\"langgraph_step\"]. The recursion limit check follows the logic: step > stop where stop = step + recursion_limit + 1. When the limit is exceeded, LangGraph raises a GraphRecursionError.\\n\\u200bAccessing the current step counter\\nYou can access the current step counter within any node to monitor execution progress.\\nCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='def my_node(state: dict, config: RunnableConfig) -> dict:\\n    current_step = config[\"metadata\"][\"langgraph_step\"]\\n    print(f\"Currently on step: {current_step}\")\\n    return state\\n\\n\\u200bProactive recursion handling\\nYou can check the step counter and proactively route to a different node before hitting the limit. This allows for graceful degradation within your graph.\\nCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph, END\\n\\ndef reasoning_node(state: dict, config: RunnableConfig) -> dict:\\n    current_step = config[\"metadata\"][\"langgraph_step\"]\\n    recursion_limit = config[\"recursion_limit\"]  # always present, defaults to 25\\n\\n    # Check if we\\'re approaching the limit (e.g., 80% threshold)\\n    if current_step >= recursion_limit * 0.8:\\n        return {\\n            **state,\\n            \"route_to\": \"fallback\",\\n            \"reason\": \"Approaching recursion limit\"\\n        }'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='# Normal processing\\n    return {\"messages\": state[\"messages\"] + [\"thinking...\"]}\\n\\ndef fallback_node(state: dict, config: RunnableConfig) -> dict:\\n    \"\"\"Handle cases where recursion limit is approaching\"\"\"\\n    return {\\n        **state,\\n        \"messages\": state[\"messages\"] + [\"Reached complexity limit, providing best effort answer\"]\\n    }\\n\\ndef route_based_on_state(state: dict) -> str:\\n    if state.get(\"route_to\") == \"fallback\":\\n        return \"fallback\"\\n    elif state.get(\"done\"):\\n        return END\\n    return \"reasoning\"\\n\\n# Build graph\\ngraph = StateGraph(dict)\\ngraph.add_node(\"reasoning\", reasoning_node)\\ngraph.add_node(\"fallback\", fallback_node)\\ngraph.add_conditional_edges(\"reasoning\", route_based_on_state)\\ngraph.add_edge(\"fallback\", END)\\ngraph.set_entry_point(\"reasoning\")\\n\\napp = graph.compile()'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='app = graph.compile()\\n\\n\\u200bProactive vs reactive approaches\\nThere are two main approaches to handling recursion limits: proactive (monitoring within the graph) and reactive (catching errors externally).\\nCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph, END\\nfrom langgraph.errors import GraphRecursionError\\n\\n# Proactive Approach (recommended)\\ndef agent_with_monitoring(state: dict, config: RunnableConfig) -> dict:\\n    \"\"\"Proactively monitor and handle recursion within the graph\"\"\"\\n    current_step = config[\"metadata\"][\"langgraph_step\"]\\n    recursion_limit = config[\"recursion_limit\"]\\n\\n    # Early detection - route to internal handling\\n    if current_step >= recursion_limit - 2:  # 2 steps before limit\\n        return {\\n            **state,\\n            \"status\": \"recursion_limit_approaching\",\\n            \"final_answer\": \"Reached iteration limit, returning partial result\"\\n        }'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='# Normal processing\\n    return {\"messages\": state[\"messages\"] + [f\"Step {current_step}\"]}\\n\\n# Reactive Approach (fallback)\\ntry:\\n    result = graph.invoke(initial_state, {\"recursion_limit\": 10})\\nexcept GraphRecursionError as e:\\n    # Handle externally after graph execution fails\\n    result = fallback_handler(initial_state)\\n\\nThe key differences between these approaches are:\\n\\nApproachDetectionHandlingControl Flow\\nProactive (using langgraph_step)Before limit reachedInside graph via conditional routingGraph continues to completion nodeReactive (catching GraphRecursionError)After limit exceededOutside graph in try/catchGraph execution terminated\\n\\nProactive advantages:\\n\\nGraceful degradation within the graph\\nCan save intermediate state in checkpoints\\nBetter user experience with partial results\\nGraph completes normally (no exception)\\n\\nReactive advantages:\\n\\nSimpler implementation\\nNo need to modify graph logic\\nCentralized error handling'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Simpler implementation\\nNo need to modify graph logic\\nCentralized error handling\\n\\n\\u200bOther available metadata\\nAlong with langgraph_step, the following metadata is also available in config[\"metadata\"]:\\nCopyAsk AIdef inspect_metadata(state: dict, config: RunnableConfig) -> dict:\\n    metadata = config[\"metadata\"]\\n\\n    print(f\"Step: {metadata[\\'langgraph_step\\']}\")\\n    print(f\"Node: {metadata[\\'langgraph_node\\']}\")\\n    print(f\"Triggers: {metadata[\\'langgraph_triggers\\']}\")\\n    print(f\"Path: {metadata[\\'langgraph_path\\']}\")\\n    print(f\"Checkpoint NS: {metadata[\\'langgraph_checkpoint_ns\\']}\")\\n\\n    return state\\n\\n\\u200bVisualization\\nIt’s often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Edit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoObservabilityPreviousUse the graph APINext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content=\"Functional API overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationFunctional APIFunctional API overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIFunctional APIUse the Functional APIRuntimeEnglishcloseOn this pageFunctional API vs. Graph APIExampleEntrypointDefinitionInjectable parametersExecutingResumingShort-term memoryentrypoint.finalTaskDefinitionExecutionWhen to use a taskSerializationDeterminismIdempotencyCommon PitfallsHandling side\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='to use a taskSerializationDeterminismIdempotencyCommon PitfallsHandling side effectsNon-deterministic control flowLangGraph APIsFunctional APIFunctional API overviewCopy pageCopy pageThe Functional API allows you to add LangGraph’s key features — persistence, memory, human-in-the-loop, and streaming — to your applications with minimal changes to your existing code.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='It is designed to integrate these features into existing code that may use standard language primitives for branching and control flow, such as if statements, for loops, and function calls. Unlike many data orchestration frameworks that require restructuring code into an explicit pipeline or DAG, the Functional API allows you to incorporate these capabilities without enforcing a rigid execution model.\\nThe Functional API uses two key building blocks:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='@entrypoint – Marks a function as the starting point of a workflow, encapsulating logic and managing execution flow, including handling long-running tasks and interrupts.\\n@task – Represents a discrete unit of work, such as an API call or data processing step, that can be executed asynchronously within an entrypoint. Tasks return a future-like object that can be awaited or resolved synchronously.\\n\\nThis provides a minimal abstraction for building workflows with state management and streaming.\\nFor information on how to use the functional API, see Use Functional API.\\n\\u200bFunctional API vs. Graph API\\nFor users who prefer a more declarative approach, LangGraph’s Graph API allows you to define workflows using a Graph paradigm. Both APIs share the same underlying runtime, so you can use them together in the same application.\\nHere are some key differences:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Control flow: The Functional API does not require thinking about graph structure. You can use standard Python constructs to define workflows. This will usually trim the amount of code you need to write.\\nShort-term memory: The GraphAPI requires declaring a State and may require defining reducers to manage updates to the graph state. @entrypoint and @tasks do not require explicit state management as their state is scoped to the function and is not shared across functions.\\nCheckpointing: Both APIs generate and use checkpoints. In the Graph API a new checkpoint is generated after every superstep. In the Functional API, when tasks are executed, their results are saved to an existing checkpoint associated with the given entrypoint instead of creating a new checkpoint.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Visualization: The Graph API makes it easy to visualize the workflow as a graph which can be useful for debugging, understanding the workflow, and sharing with others. The Functional API does not support visualization as the graph is dynamically generated during runtime.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bExample\\nBelow we demonstrate a simple application that writes an essay and interrupts to request human review.\\nCopyAsk AIfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langgraph.func import entrypoint, task\\nfrom langgraph.types import interrupt\\n\\n@task\\ndef write_essay(topic: str) -> str:\\n    \"\"\"Write an essay about the given topic.\"\"\"\\n    time.sleep(1) # A placeholder for a long-running task.\\n    return f\"An essay about topic: {topic}\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='@entrypoint(checkpointer=InMemorySaver())\\ndef workflow(topic: str) -> dict:\\n    \"\"\"A simple workflow that writes an essay and asks for a review.\"\"\"\\n    essay = write_essay(\"cat\").result()\\n    is_approved = interrupt({\\n        # Any json-serializable payload provided to interrupt as argument.\\n        # It will be surfaced on the client side as an Interrupt when streaming data\\n        # from the workflow.\\n        \"essay\": essay, # The essay we want reviewed.\\n        # We can add any additional information that we need.\\n        # For example, introduce a key called \"action\" with some instructions.\\n        \"action\": \"Please approve/reject the essay\",\\n    })\\n\\n    return {\\n        \"essay\": essay, # The essay that was generated\\n        \"is_approved\": is_approved, # Response from HIL\\n    }'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Detailed ExplanationThis workflow will write an essay about the topic “cat” and then pause to get a review from a human. The workflow can be interrupted for an indefinite amount of time until a review is provided.When the workflow is resumed, it executes from the very start, but because the result of the writeEssay task was already saved, the task result will be loaded from the checkpoint instead of being recomputed.CopyAsk AIimport time\\nimport uuid\\nfrom langgraph.func import entrypoint, task\\nfrom langgraph.types import interrupt\\nfrom langgraph.checkpoint.memory import InMemorySaver\\n\\n\\n@task\\ndef write_essay(topic: str) -> str:\\n    \"\"\"Write an essay about the given topic.\"\"\"\\n    time.sleep(1)  # This is a placeholder for a long-running task.\\n    return f\"An essay about topic: {topic}\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='@entrypoint(checkpointer=InMemorySaver())\\ndef workflow(topic: str) -> dict:\\n    \"\"\"A simple workflow that writes an essay and asks for a review.\"\"\"\\n    essay = write_essay(\"cat\").result()\\n    is_approved = interrupt(\\n        {\\n            # Any json-serializable payload provided to interrupt as argument.\\n            # It will be surfaced on the client side as an Interrupt when streaming data\\n            # from the workflow.\\n            \"essay\": essay,  # The essay we want reviewed.\\n            # We can add any additional information that we need.\\n            # For example, introduce a key called \"action\" with some instructions.\\n            \"action\": \"Please approve/reject the essay\",\\n        }\\n    )\\n    return {\\n        \"essay\": essay,  # The essay that was generated\\n        \"is_approved\": is_approved,  # Response from HIL\\n    }'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='thread_id = str(uuid.uuid4())\\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\\nfor item in workflow.stream(\"cat\", config):\\n    print(item)\\n# > {\\'write_essay\\': \\'An essay about topic: cat\\'}\\n# > {\\n# >     \\'__interrupt__\\': (\\n# >        Interrupt(\\n# >            value={\\n# >                \\'essay\\': \\'An essay about topic: cat\\',\\n# >                \\'action\\': \\'Please approve/reject the essay\\'\\n# >            },\\n# >            id=\\'b9b2b9d788f482663ced6dc755c9e981\\'\\n# >        ),\\n# >    )\\n# > }\\nAn essay has been written and is ready for review. Once the review is provided, we can resume the workflow:CopyAsk AIfrom langgraph.types import Command\\n\\n# Get review from a user (e.g., via a UI)\\n# In this case, we\\'re using a bool, but this can be any json-serializable value.\\nhuman_review = True'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content=\"for item in workflow.stream(Command(resume=human_review), config):\\n    print(item)\\nCopyAsk AI{'workflow': {'essay': 'An essay about topic: cat', 'is_approved': False}}\\nThe workflow has been completed and the review has been added to the essay.\\n\\u200bEntrypoint\\nThe @entrypoint decorator can be used to create a workflow from a function. It encapsulates workflow logic and manages execution flow, including handling long-running tasks and interrupts.\\n\\u200bDefinition\\nAn entrypoint is defined by decorating a function with the @entrypoint decorator.\\nThe function must accept a single positional argument, which serves as the workflow input. If you need to pass multiple pieces of data, use a dictionary as the input type for the first argument.\\nDecorating a function with an entrypoint produces a Pregel instance which helps to manage the execution of the workflow (e.g., handles streaming, resumption, and checkpointing).\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='You will usually want to pass a checkpointer to the @entrypoint decorator to enable persistence and use features like human-in-the-loop.\\n Sync AsyncCopyAsk AIfrom langgraph.func import entrypoint'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(some_input: dict) -> int:\\n    # some logic that may involve long-running tasks like API calls,\\n    # and may be interrupted for human-in-the-loop.\\n    ...\\n    return result'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Serialization\\nThe inputs and outputs of entrypoints must be JSON-serializable to support checkpointing. Please see the serialization section for more details.\\n\\u200bInjectable parameters\\nWhen declaring an entrypoint, you can request access to additional parameters that will be injected automatically at run time. These parameters include:\\nParameterDescriptionpreviousAccess the state associated with the previous checkpoint for the given thread. See short-term-memory.storeAn instance of [BaseStore][langgraph.store.base.BaseStore]. Useful for long-term memory.writerUse to access the StreamWriter when working with Async Python < 3.11. See streaming with functional API for details.configFor accessing run time configuration. See RunnableConfig for information.\\nDeclare the parameters with the appropriate name and type annotation.\\nRequesting Injectable ParametersCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.func import entrypoint'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='from langgraph.func import entrypoint\\nfrom langgraph.store.base import BaseStore\\nfrom langgraph.store.memory import InMemoryStore'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='in_memory_store = InMemoryStore(...)  # An instance of InMemoryStore for long-term memory\\n\\n@entrypoint(\\n    checkpointer=checkpointer,  # Specify the checkpointer\\n    store=in_memory_store  # Specify the store\\n)\\ndef my_workflow(\\n    some_input: dict,  # The input (e.g., passed via `invoke`)\\n    *,\\n    previous: Any = None, # For short-term memory\\n    store: BaseStore,  # For long-term memory\\n    writer: StreamWriter,  # For streaming custom data\\n    config: RunnableConfig  # For accessing the configuration passed to the entrypoint\\n) -> ...:\\n\\n\\u200bExecuting\\nUsing the @entrypoint yields a Pregel object that can be executed using the invoke, ainvoke, stream, and astream methods.\\n Invoke Async Invoke Stream Async StreamCopyAsk AIconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\nmy_workflow.invoke(some_input, config)  # Wait for the result synchronously'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bResuming\\nResuming an execution after an interrupt can be done by passing a resume value to the Command primitive.\\n Invoke Async Invoke Stream Async StreamCopyAsk AIfrom langgraph.types import Command\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\n\\nmy_workflow.invoke(Command(resume=some_resume_value), config)\\n\\nResuming after an error\\nTo resume after an error, run the entrypoint with a None and the same thread id (config).\\nThis assumes that the underlying error has been resolved and execution can proceed successfully.\\n Invoke Async Invoke Stream Async StreamCopyAsk AI\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\n\\nmy_workflow.invoke(None, config)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='my_workflow.invoke(None, config)\\n\\n\\u200bShort-term memory\\nWhen an entrypoint is defined with a checkpointer, it stores information between successive invocations on the same thread id in checkpoints.\\nThis allows accessing the state from the previous invocation using the previous parameter.\\nBy default, the previous parameter is the return value of the previous invocation.\\nCopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(number: int, *, previous: Any = None) -> int:\\n    previous = previous or 0\\n    return number + previous\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\n\\nmy_workflow.invoke(1, config)  # 1 (previous was None)\\nmy_workflow.invoke(2, config)  # 3 (previous was 1 from the previous invocation)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bentrypoint.final\\nentrypoint.final is a special primitive that can be returned from an entrypoint and allows decoupling the value that is saved in the checkpoint from the return value of the entrypoint.\\nThe first value is the return value of the entrypoint, and the second value is the value that will be saved in the checkpoint. The type annotation is entrypoint.final[return_type, save_type].\\nCopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(number: int, *, previous: Any = None) -> entrypoint.final[int, int]:\\n    previous = previous or 0\\n    # This will return the previous value to the caller, saving\\n    # 2 * number to the checkpoint, which will be used in the next invocation\\n    # for the `previous` parameter.\\n    return entrypoint.final(value=previous, save=2 * number)\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"1\"\\n    }\\n}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='config = {\\n    \"configurable\": {\\n        \"thread_id\": \"1\"\\n    }\\n}\\n\\nmy_workflow.invoke(3, config)  # 0 (previous was None)\\nmy_workflow.invoke(1, config)  # 6 (previous was 3 * 2 from the previous invocation)\\n\\n\\u200bTask\\nA task represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics:\\n\\nAsynchronous Execution: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking.\\nCheckpointing: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See persistence for more details).\\n\\n\\u200bDefinition\\nTasks are defined using the @task decorator, which wraps a regular Python function.\\nCopyAsk AIfrom langgraph.func import task\\n\\n@task()\\ndef slow_computation(input_value):\\n    # Simulate a long-running operation\\n    ...\\n    return result'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Serialization\\nThe outputs of tasks must be JSON-serializable to support checkpointing.\\n\\u200bExecution\\nTasks can only be called from within an entrypoint, another task, or a state graph node.\\nTasks cannot be called directly from the main application code.\\nWhen you call a task, it returns immediately with a future object. A future is a placeholder for a result that will be available later.\\nTo obtain the result of a task, you can either wait for it synchronously (using result()) or await it asynchronously (using await).\\n Synchronous Invocation Asynchronous InvocationCopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(some_input: int) -> int:\\n    future = slow_computation(some_input)\\n    return future.result()  # Wait for the result synchronously\\n\\n\\u200bWhen to use a task\\nTasks are useful in the following scenarios:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bWhen to use a task\\nTasks are useful in the following scenarios:\\n\\nCheckpointing: When you need to save the result of a long-running operation to a checkpoint, so you don’t need to recompute it when resuming the workflow.\\nHuman-in-the-loop: If you’re building a workflow that requires human intervention, you MUST use tasks to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the determinism section for more details.\\nParallel Execution: For I/O-bound tasks, tasks enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs).\\nObservability: Wrapping operations in tasks provides a way to track the progress of the workflow and monitor the execution of individual operations using LangSmith.\\nRetryable Work: When work needs to be retried to handle failures or inconsistencies, tasks provide a way to encapsulate and manage the retry logic.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bSerialization\\nThere are two key aspects to serialization in LangGraph:\\n\\nentrypoint inputs and outputs must be JSON-serializable.\\ntask outputs must be JSON-serializable.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='These requirements are necessary for enabling checkpointing and workflow resumption. Use python primitives like dictionaries, lists, strings, numbers, and booleans to ensure that your inputs and outputs are serializable.\\nSerialization ensures that workflow state, such as task results and intermediate values, can be reliably saved and restored. This is critical for enabling human-in-the-loop interactions, fault tolerance, and parallel execution.\\nProviding non-serializable inputs or outputs will result in a runtime error when a workflow is configured with a checkpointer.\\n\\u200bDeterminism\\nTo utilize features like human-in-the-loop, any randomness should be encapsulated inside of tasks. This guarantees that when execution is halted (e.g., for human in the loop) and then resumed, it will follow the same sequence of steps, even if task results are non-deterministic.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='LangGraph achieves this behavior by persisting task and subgraph results as they execute. A well-designed workflow ensures that resuming execution follows the same sequence of steps, allowing previously computed results to be retrieved correctly without having to re-execute them. This is particularly useful for long-running tasks or tasks with non-deterministic results, as it avoids repeating previously done work and allows resuming from essentially the same.\\nWhile different runs of a workflow can produce different results, resuming a specific run should always follow the same sequence of recorded steps. This allows LangGraph to efficiently look up task and subgraph results that were executed prior to the graph being interrupted and avoid recomputing them.\\n\\u200bIdempotency'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bIdempotency\\nIdempotency ensures that running the same operation multiple times produces the same result. This helps prevent duplicate API calls and redundant processing if a step is rerun due to a failure. Always place API calls inside tasks functions for checkpointing, and design them to be idempotent in case of re-execution. Re-execution can occur if a task starts, but does not complete successfully. Then, if the workflow is resumed, the task will run again. Use idempotency keys or verify existing results to avoid duplication.\\n\\u200bCommon Pitfalls\\n\\u200bHandling side effects\\nEncapsulate side effects (e.g., writing to a file, sending an email) in tasks to ensure they are not executed multiple times when resuming a workflow.\\n Incorrect CorrectIn this example, a side effect (writing to a file) is directly included in the workflow, so it will be executed a second time when resuming the workflow.CopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(inputs: dict) -> int:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='def my_workflow(inputs: dict) -> int:\\n    # This code will be executed a second time when resuming the workflow.\\n    # Which is likely not what you want.\\n    with open(\"output.txt\", \"w\") as f:  \\n        f.write(\"Side effect executed\")  \\n    value = interrupt(\"question\")\\n    return value'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bNon-deterministic control flow\\nOperations that might give different results each time (like getting current time or random numbers) should be encapsulated in tasks to ensure that on resume, the same result is returned.\\n\\nIn a task: Get random number (5) → interrupt → resume → (returns 5 again) → …\\nNot in a task: Get random number (5) → interrupt → resume → get new random number (7) → …'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='This is especially important when using human-in-the-loop workflows with multiple interrupts calls. LangGraph keeps a list of resume values for each task/entrypoint. When an interrupt is encountered, it’s matched with the corresponding resume value. This matching is strictly index-based, so the order of the resume values should match the order of the interrupts.\\nIf order of execution is not maintained when resuming, one interrupt call may be matched with the wrong resume value, leading to incorrect results.\\nPlease read the section on determinism for more details.\\n Incorrect CorrectIn this example, the workflow uses the current time to determine which task to execute. This is non-deterministic because the result of the workflow depends on the time at which it is executed.CopyAsk AIfrom langgraph.func import entrypoint\\n\\n@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(inputs: dict) -> int:\\n    t0 = inputs[\"t0\"]\\n    t1 = time.time()  \\n\\n    delta_t = t1 - t0'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='delta_t = t1 - t0\\n\\n    if delta_t > 1:\\n        result = slow_task(1).result()\\n        value = interrupt(\"question\")\\n    else:\\n        result = slow_task(2).result()\\n        value = interrupt(\"question\")\\n\\n    return {\\n        \"result\": result,\\n        \"value\": value\\n    }\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoUse the graph APIPreviousUse the functional APINext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content=\"LangGraph overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeEnglishcloseOn this page InstallCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageCopy pageLangGraph v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='Trusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b Install\\npipuvCopyAsk AIpip install -U langgraph'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='Then, create a simple hello world example:\\nCopyAsk AIfrom langgraph.graph import StateGraph, MessagesState, START, END\\n\\ndef mock_llm(state: MessagesState):\\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\\n\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(mock_llm)\\ngraph.add_edge(START, \"mock_llm\")\\ngraph.add_edge(\"mock_llm\", END)\\ngraph = graph.compile()\\n\\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\\n\\n\\u200bCore benefits\\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='Durable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\nHuman-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.\\nComprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\nDebugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\nProduction-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\\n\\n\\u200bLangGraph ecosystem\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='LangSmith — Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\\nLangSmith — Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in Studio.\\nLangChain - Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\\n\\n\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content=\"Edit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoWhat's new in v1Next⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content=\"Workflows and agents - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedWorkflows and agentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeEnglishcloseOn this pageSetupLLMs and augmentationsPrompt chainingParallelizationRoutingOrchestrator-workerCreating workers in LangGraphEvaluator-optimizerAgentsGet startedWorkflows and agentsCopy pageCopy pageThis guide reviews common workflow and agent patterns.\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Workflows have predetermined code paths and are designed to operate in a certain order.\\nAgents are dynamic and define their own processes and tool usage.\\n\\n\\nLangGraph offers several benefits when building agents and workflows, including persistence, streaming, and support for debugging as well as deployment.\\n\\u200bSetup\\nTo build a workflow or agent, you can use any chat model that supports structured outputs and tool calling. The following example uses Anthropic:\\n\\nInstall dependencies:\\n\\nCopyAsk AIpip install langchain_core langchain-anthropic langgraph\\n\\n\\nInitialize the LLM:\\n\\nCopyAsk AIimport os\\nimport getpass\\n\\nfrom langchain_anthropic import ChatAnthropic\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\\n\\n\\u200bLLMs and augmentations\\nWorkflows and agentic systems are based on LLMs and the various augmentations you add to them. Tool calling, structured outputs, and short term memory are a few options for tailoring LLMs to your needs.\\n\\nCopyAsk AI# Schema for structured output\\nfrom pydantic import BaseModel, Field\\n\\n\\nclass SearchQuery(BaseModel):\\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\\n    justification: str = Field(\\n        None, description=\"Why this query is relevant to the user\\'s request.\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nstructured_llm = llm.with_structured_output(SearchQuery)\\n\\n# Invoke the augmented LLM\\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\\n\\n# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b\\n\\n# Augment the LLM with tools\\nllm_with_tools = llm.bind_tools([multiply])\\n\\n# Invoke the LLM with input that triggers the tool call\\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\\n\\n# Get the tool call\\nmsg.tool_calls\\n\\n\\u200bPrompt chaining\\nPrompt chaining is when each LLM call processes the output of the previous call. It’s often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:\\n\\nTranslating documents into different languages\\nVerifying generated content for consistency\\n\\n\\nGraph APIFunctional APICopyAsk AIfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom IPython.display import Image, display\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    improved_joke: str\\n    final_joke: str\\n\\n\\n# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a short joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef check_punchline(state: State):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\\n        return \"Pass\"\\n    return \"Fail\"\\n\\n\\ndef improve_joke(state: State):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state[\\'joke\\']}\")\\n    return {\"improved_joke\": msg.content}\\n\\n\\ndef polish_joke(state: State):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state[\\'improved_joke\\']}\")\\n    return {\"final_joke\": msg.content}\\n\\n\\n# Build workflow\\nworkflow = StateGraph(State)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Build workflow\\nworkflow = StateGraph(State)\\n\\n# Add nodes\\nworkflow.add_node(\"generate_joke\", generate_joke)\\nworkflow.add_node(\"improve_joke\", improve_joke)\\nworkflow.add_node(\"polish_joke\", polish_joke)\\n\\n# Add edges to connect nodes\\nworkflow.add_edge(START, \"generate_joke\")\\nworkflow.add_conditional_edges(\\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\\n)\\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\\nworkflow.add_edge(\"polish_joke\", END)\\n\\n# Compile\\nchain = workflow.compile()\\n\\n# Show workflow\\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = chain.invoke({\"topic\": \"cats\"})\\nprint(\"Initial joke:\")\\nprint(state[\"joke\"])\\nprint(\"\\\\n--- --- ---\\\\n\")\\nif \"improved_joke\" in state:\\n    print(\"Improved joke:\")\\n    print(state[\"improved_joke\"])\\n    print(\"\\\\n--- --- ---\\\\n\")\\n\\n    print(\"Final joke:\")\\n    print(state[\"final_joke\"])\\nelse:\\n    print(\"Joke failed quality gate - no punchline detected!\")'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='\\u200bParallelization\\nWith parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:\\n\\nSplit up subtasks and run them in parallel, which increases speed\\nRun tasks multiple times to check for different outputs, which increases confidence\\n\\nSome examples include:\\n\\nRunning one subtask that processes a document for keywords, and a second subtask to check for formatting errors\\nRunning a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources\\n\\n\\nGraph APIFunctional APICopyAsk AI# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    story: str\\n    poem: str\\n    combined_output: str\\n\\n\\n# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef call_llm_2(state: State):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n\\n    msg = llm.invoke(f\"Write a story about {state[\\'topic\\']}\")\\n    return {\"story\": msg.content}\\n\\n\\ndef call_llm_3(state: State):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n\\n    msg = llm.invoke(f\"Write a poem about {state[\\'topic\\']}\")\\n    return {\"poem\": msg.content}\\n\\n\\ndef aggregator(state: State):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {state[\\'topic\\']}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{state[\\'story\\']}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{state[\\'joke\\']}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{state[\\'poem\\']}\"\\n    return {\"combined_output\": combined}\\n\\n\\n# Build workflow\\nparallel_builder = StateGraph(State)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Build workflow\\nparallel_builder = StateGraph(State)\\n\\n# Add nodes\\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\\nparallel_builder.add_node(\"aggregator\", aggregator)\\n\\n# Add edges to connect nodes\\nparallel_builder.add_edge(START, \"call_llm_1\")\\nparallel_builder.add_edge(START, \"call_llm_2\")\\nparallel_builder.add_edge(START, \"call_llm_3\")\\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\\nparallel_builder.add_edge(\"aggregator\", END)\\nparallel_workflow = parallel_builder.compile()\\n\\n# Show workflow\\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])\\n\\n\\u200bRouting\\nRouting workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.\\n\\nGraph APIFunctional APICopyAsk AIfrom typing_extensions import Literal\\nfrom langchain.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str\\n\\n\\n# Nodes\\ndef llm_call_1(state: State):\\n    \"\"\"Write a story\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_2(state: State):\\n    \"\"\"Write a joke\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_3(state: State):\\n    \"\"\"Write a poem\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_router(state: State):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=state[\"input\"]),\\n        ]\\n    )\\n\\n    return {\"decision\": decision.step}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='return {\"decision\": decision.step}\\n\\n\\n# Conditional edge function to route to the appropriate node\\ndef route_decision(state: State):\\n    # Return the node name you want to visit next\\n    if state[\"decision\"] == \"story\":\\n        return \"llm_call_1\"\\n    elif state[\"decision\"] == \"joke\":\\n        return \"llm_call_2\"\\n    elif state[\"decision\"] == \"poem\":\\n        return \"llm_call_3\"\\n\\n\\n# Build workflow\\nrouter_builder = StateGraph(State)\\n\\n# Add nodes\\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Add edges to connect nodes\\nrouter_builder.add_edge(START, \"llm_call_router\")\\nrouter_builder.add_conditional_edges(\\n    \"llm_call_router\",\\n    route_decision,\\n    {  # Name returned by route_decision : Name of next node to visit\\n        \"llm_call_1\": \"llm_call_1\",\\n        \"llm_call_2\": \"llm_call_2\",\\n        \"llm_call_3\": \"llm_call_3\",\\n    },\\n)\\nrouter_builder.add_edge(\"llm_call_1\", END)\\nrouter_builder.add_edge(\"llm_call_2\", END)\\nrouter_builder.add_edge(\"llm_call_3\", END)\\n\\n# Compile workflow\\nrouter_workflow = router_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\\nprint(state[\"output\"])\\n\\n\\u200bOrchestrator-worker\\nIn an orchestrator-worker configuration, the orchestrator:\\n\\nBreaks down tasks into subtasks\\nDelegates subtasks to workers\\nSynthesizes worker outputs into a final result'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Orchestrator-worker workflows provide more flexibility and are often used when subtasks cannot be predefined the way they can with parallelization. This is common with workflows that write code or need to update content across multiple files. For example, a workflow that needs to update installation instructions for multiple Python libraries across an unknown number of documents might use this pattern.\\nGraph APIFunctional APICopyAsk AIfrom typing import Annotated, List\\nimport operator\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\n\\u200bCreating workers in LangGraph\\nOrchestrator-worker workflows are common and LangGraph has built-in support for them. The Send API lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the Send API to send a section to each worker.\\nCopyAsk AIfrom langgraph.types import Send\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str  # Report topic\\n    sections: list[Section]  # List of report sections\\n    completed_sections: Annotated[\\n        list, operator.add\\n    ]  # All workers write to this key in parallel\\n    final_report: str  # Final report'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Worker state\\nclass WorkerState(TypedDict):\\n    section: Section\\n    completed_sections: Annotated[list, operator.add]\\n\\n\\n# Nodes\\ndef orchestrator(state: State):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {state[\\'topic\\']}\"),\\n        ]\\n    )\\n\\n    return {\"sections\": report_sections.sections}\\n\\n\\ndef llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='def llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    section = llm.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\\n            ),\\n            HumanMessage(\\n                content=f\"Here is the section name: {state[\\'section\\'].name} and description: {state[\\'section\\'].description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return {\"completed_sections\": [section.content]}\\n\\n\\ndef synthesizer(state: State):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n\\n    # List of completed sections\\n    completed_sections = state[\"completed_sections\"]\\n\\n    # Format completed section to str to use as context for final sections\\n    completed_report_sections = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='return {\"final_report\": completed_report_sections}\\n\\n\\n# Conditional edge function to create llm_call workers that each write a section of the report\\ndef assign_workers(state: State):\\n    \"\"\"Assign a worker to each section in the plan\"\"\"\\n\\n    # Kick off section writing in parallel via Send() API\\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\\n\\n\\n# Build workflow\\norchestrator_worker_builder = StateGraph(State)\\n\\n# Add the nodes\\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\\n\\n# Add edges to connect nodes\\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\\norchestrator_worker_builder.add_conditional_edges(\\n    \"orchestrator\", assign_workers, [\"llm_call\"]\\n)\\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\\norchestrator_worker_builder.add_edge(\"synthesizer\", END)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Compile the workflow\\norchestrator_worker = orchestrator_worker_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\\n\\nfrom IPython.display import Markdown\\nMarkdown(state[\"final_report\"])'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='from IPython.display import Markdown\\nMarkdown(state[\"final_report\"])\\n\\n\\u200bEvaluator-optimizer\\nIn evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a human-in-the-loop determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.\\nEvaluator-optimizer workflows are commonly used when there’s particular success criteria for a task, but iteration is required to meet that criteria. For example, there’s not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.\\n\\nGraph APIFunctional APICopyAsk AI# Graph state\\nclass State(TypedDict):\\n    joke: str\\n    topic: str\\n    feedback: str\\n    funny_or_not: str'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\ndef llm_call_generator(state: State):\\n    \"\"\"LLM generates a joke\"\"\"\\n\\n    if state.get(\"feedback\"):\\n        msg = llm.invoke(\\n            f\"Write a joke about {state[\\'topic\\']} but take into account the feedback: {state[\\'feedback\\']}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef llm_call_evaluator(state: State):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n\\n    grade = evaluator.invoke(f\"Grade the joke {state[\\'joke\\']}\")\\n    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\\ndef route_joke(state: State):\\n    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\\n\\n    if state[\"funny_or_not\"] == \"funny\":\\n        return \"Accepted\"\\n    elif state[\"funny_or_not\"] == \"not funny\":\\n        return \"Rejected + Feedback\"\\n\\n\\n# Build workflow\\noptimizer_builder = StateGraph(State)\\n\\n# Add the nodes\\noptimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\\noptimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\\n\\n# Add edges to connect nodes\\noptimizer_builder.add_edge(START, \"llm_call_generator\")\\noptimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\\noptimizer_builder.add_conditional_edges(\\n    \"llm_call_evaluator\",\\n    route_joke,\\n    {  # Name returned by route_joke : Name of next node to visit\\n        \"Accepted\": END,\\n        \"Rejected + Feedback\": \"llm_call_generator\",\\n    },\\n)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Compile the workflow\\noptimizer_workflow = optimizer_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = optimizer_workflow.invoke({\"topic\": \"Cats\"})\\nprint(state[\"joke\"])\\n\\n\\u200bAgents\\nAgents are typically implemented as an LLM performing actions using tools. They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.\\n\\nTo get started with agents, see the quickstart or read more about how they work in LangChain.\\nUsing toolsCopyAsk AIfrom langchain.tools import tool\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a * b'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nllm_with_tools = llm.bind_tools(tools)\\n\\nGraph APIFunctional APICopyAsk AIfrom langgraph.graph import MessagesState\\nfrom langchain.messages import SystemMessage, HumanMessage, ToolMessage\\n\\n\\n# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            llm_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ]\\n    }\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\\ndef should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"tool_node\"\\n\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n\\n# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"tool_node\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    [\"tool_node\", END]\\n)\\nagent_builder.add_edge(\"tool_node\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoThinking in LangGraphPreviousPersistenceNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content=\"Graph API overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationGraph APIGraph API overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIGraph APIUse the graph APIFunctional APIRuntimeEnglishcloseOn this pageGraphsStateGraphCompiling your graphStateSchemaMultiple schemasReducersDefault ReducerOverwriteWorking with Messages in Graph StateWhy use messages?Using Messages in your GraphSerializationMessagesStateNodesSTART NodeEND NodeNode CachingEdgesNormal EdgesConditional\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='your GraphSerializationMessagesStateNodesSTART NodeEND NodeNode CachingEdgesNormal EdgesConditional EdgesEntry PointConditional Entry PointSendCommandWhen should I use Command instead of conditional edges?Navigating to a node in a parent graphUsing inside toolsHuman-in-the-loopGraph MigrationsRuntime ContextRecursion LimitAccessing and handling the recursion counterHow it worksAccessing the current step counterProactive recursion handlingProactive vs reactive approachesOther available metadataVisualizationLangGraph APIsGraph APIGraph API overviewCopy pageCopy page\\u200bGraphs'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='State: A shared data structure that represents the current snapshot of your application. It can be any data type, but is typically defined using a shared state schema.\\n\\n\\nNodes: Functions that encode the logic of your agents. They receive the current state as input, perform some computation or side-effect, and return an updated state.\\n\\n\\nEdges: Functions that determine which Node to execute next based on the current state. They can be conditional branches or fixed transitions.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='By composing Nodes and Edges, you can create complex, looping workflows that evolve the state over time. The real power, though, comes from how LangGraph manages that state. To emphasize: Nodes and Edges are nothing more than functions - they can contain an LLM or just good ol’ code.\\nIn short: nodes do the work, edges tell what to do next.\\nLangGraph’s underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google’s Pregel system, the program proceeds in discrete “super-steps.”'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or “channels”). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.\\n\\u200bStateGraph\\nThe StateGraph class is the main graph class to use. This is parameterized by a user defined State object.\\n\\u200bCompiling your graph\\nTo build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed?'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method:\\nCopyAsk AIgraph = graph_builder.compile(...)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='You MUST compile your graph before you can use it.\\n\\u200bState\\nThe first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.\\n\\u200bSchema\\nThe main documented way to specify the schema of a graph is by using a TypedDict. If you want to provide default values in your state, use a dataclass. We also support using a Pydantic BaseModel as your graph state if you want recursive data validation (though note that pydantic is less performant than a TypedDict or dataclass).'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the guide here for how to use.\\n\\u200bMultiple schemas\\nTypically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Internal nodes can pass information that is not required in the graph’s input / output.\\nWe may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key.\\n\\nIt is possible to have nodes write to private state channels inside the graph for internal node communication. We can simply define a private schema, PrivateState.\\nIt is also possible to define explicit input and output schemas for a graph. In these cases, we define an “internal” schema that contains all keys relevant to graph operations. But, we also define input and output schemas that are sub-sets of the “internal” schema to constrain the input and output of the graph. See this guide for more detail.\\nLet’s look at an example:\\nCopyAsk AIclass InputState(TypedDict):\\n    user_input: str\\n\\nclass OutputState(TypedDict):\\n    graph_output: str\\n\\nclass OverallState(TypedDict):\\n    foo: str\\n    user_input: str\\n    graph_output: str'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='class OverallState(TypedDict):\\n    foo: str\\n    user_input: str\\n    graph_output: str\\n\\nclass PrivateState(TypedDict):\\n    bar: str\\n\\ndef node_1(state: InputState) -> OverallState:\\n    # Write to OverallState\\n    return {\"foo\": state[\"user_input\"] + \" name\"}\\n\\ndef node_2(state: OverallState) -> PrivateState:\\n    # Read from OverallState, write to PrivateState\\n    return {\"bar\": state[\"foo\"] + \" is\"}\\n\\ndef node_3(state: PrivateState) -> OutputState:\\n    # Read from PrivateState, write to OutputState\\n    return {\"graph_output\": state[\"bar\"] + \" Lance\"}\\n\\nbuilder = StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)\\nbuilder.add_node(\"node_1\", node_1)\\nbuilder.add_node(\"node_2\", node_2)\\nbuilder.add_node(\"node_3\", node_3)\\nbuilder.add_edge(START, \"node_1\")\\nbuilder.add_edge(\"node_1\", \"node_2\")\\nbuilder.add_edge(\"node_2\", \"node_3\")\\nbuilder.add_edge(\"node_3\", END)\\n\\ngraph = builder.compile()\\ngraph.invoke({\"user_input\":\"My\"})\\n# {\\'graph_output\\': \\'My name is Lance\\'}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='graph = builder.compile()\\ngraph.invoke({\"user_input\":\"My\"})\\n# {\\'graph_output\\': \\'My name is Lance\\'}\\n\\nThere are two subtle and important points to note here:\\n\\n\\nWe pass state: InputState as the input schema to node_1. But, we write out to foo, a channel in OverallState. How can we write out to a state channel that is not included in the input schema? This is because a node can write to any state channel in the graph state. The graph state is the union of the state channels defined at initialization, which includes OverallState and the filters InputState and OutputState.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='We initialize the graph with StateGraph(OverallState,input_schema=InputState,output_schema=OutputState). So, how can we write to PrivateState in node_2? How does the graph gain access to this schema if it was not passed in the StateGraph initialization? We can do this because nodes can also declare additional state channels as long as the state schema definition exists. In this case, the PrivateState schema is defined, so we can add bar as a new state channel in the graph and write to it.\\n\\n\\n\\u200bReducers\\nReducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer:\\n\\u200bDefault Reducer\\nThese two examples show how to use the default reducer:\\nExample A:\\nCopyAsk AIfrom typing_extensions import TypedDict'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='class State(TypedDict):\\n    foo: int\\n    bar: list[str]\\n\\nIn this example, no reducer functions are specified for any key. Let’s assume the input to the graph is:\\n{\"foo\": 1, \"bar\": [\"hi\"]}. Let’s then assume the first Node returns {\"foo\": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {\"foo\": 2, \"bar\": [\"hi\"]}. If the second node returns {\"bar\": [\"bye\"]} then the State would then be {\"foo\": 2, \"bar\": [\"bye\"]}\\nExample B:\\nCopyAsk AIfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\nfrom operator import add\\n\\nclass State(TypedDict):\\n    foo: int\\n    bar: Annotated[list[str], add]'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='In this example, we’ve used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let’s assume the input to the graph is {\"foo\": 1, \"bar\": [\"hi\"]}. Let’s then assume the first Node returns {\"foo\": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {\"foo\": 2, \"bar\": [\"hi\"]}. If the second node returns {\"bar\": [\"bye\"]} then the State would then be {\"foo\": 2, \"bar\": [\"hi\", \"bye\"]}. Notice here that the bar key is updated by adding the two lists together.\\n\\u200bOverwrite\\nIn some cases, you may want to bypass a reducer and directly overwrite a state value. LangGraph provides the Overwrite type for this purpose. Learn how to use Overwrite here.\\n\\u200bWorking with Messages in Graph State\\n\\u200bWhy use messages?'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bWorking with Messages in Graph State\\n\\u200bWhy use messages?\\nMost modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain’s ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide.\\n\\u200bUsing Messages in your Graph'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bUsing Messages in your Graph\\nIn many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don’t specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.\\n\\u200bSerialization\\nIn addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format:\\nCopyAsk AI# this is supported'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='CopyAsk AI# this is supported\\n{\"messages\": [HumanMessage(content=\"message\")]}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='# and this is also supported\\n{\"messages\": [{\"type\": \"human\", \"content\": \"message\"}]}\\n\\nSince the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[\"messages\"][-1].content. Below is an example of a graph that uses add_messages as its reducer function.\\nCopyAsk AIfrom langchain.messages import AnyMessage\\nfrom langgraph.graph.message import add_messages\\nfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\n\\nclass GraphState(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='class GraphState(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]\\n\\n\\u200bMessagesState\\nSince having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:\\nCopyAsk AIfrom langgraph.graph import MessagesState\\n\\nclass State(MessagesState):\\n    documents: list[str]\\n\\n\\u200bNodes\\nIn LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments:\\n\\nstate: The state of the graph\\nconfig: A RunnableConfig object that contains configuration information like thread_id and tracing information like tags\\nruntime: A Runtime object that contains runtime context and other information like store and stream_writer'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Similar to NetworkX, you add these nodes to a graph using the add_node method:\\nCopyAsk AIfrom dataclasses import dataclass\\nfrom typing_extensions import TypedDict\\n\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph\\nfrom langgraph.runtime import Runtime\\n\\nclass State(TypedDict):\\n    input: str\\n    results: str\\n\\n@dataclass\\nclass Context:\\n    user_id: str\\n\\nbuilder = StateGraph(State)\\n\\ndef plain_node(state: State):\\n    return state\\n\\ndef node_with_runtime(state: State, runtime: Runtime[Context]):\\n    print(\"In node: \", runtime.context.user_id)\\n    return {\"results\": f\"Hello, {state[\\'input\\']}!\"}\\n\\ndef node_with_config(state: State, config: RunnableConfig):\\n    print(\"In node with thread_id: \", config[\"configurable\"][\"thread_id\"])\\n    return {\"results\": f\"Hello, {state[\\'input\\']}!\"}\\n\\n\\nbuilder.add_node(\"plain_node\", plain_node)\\nbuilder.add_node(\"node_with_runtime\", node_with_runtime)\\nbuilder.add_node(\"node_with_config\", node_with_config)\\n...'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Behind the scenes, functions are converted to RunnableLambdas, which add batch and async support to your function, along with native tracing and debugging.\\nIf you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name.\\nCopyAsk AIbuilder.add_node(my_node)\\n# You can then create edges to/from this node by referencing it as `\"my_node\"`\\n\\n\\u200bSTART Node\\nThe START Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.\\nCopyAsk AIfrom langgraph.graph import START\\n\\ngraph.add_edge(START, \"node_a\")\\n\\n\\u200bEND Node\\nThe END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.\\nCopyAsk AIfrom langgraph.graph import END\\n\\ngraph.add_edge(\"node_a\", END)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='graph.add_edge(\"node_a\", END)\\n\\n\\u200bNode Caching\\nLangGraph supports caching of tasks/nodes based on the input to the node. To use caching:\\n\\nSpecify a cache when compiling a graph (or specifying an entrypoint)\\nSpecify a cache policy for nodes. Each cache policy supports:\\n\\nkey_func used to generate a cache key based on the input to a node, which defaults to a hash of the input with pickle.\\nttl, the time to live for the cache in seconds. If not specified, the cache will never expire.\\n\\n\\n\\nFor example:\\nCopyAsk AIimport time\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph\\nfrom langgraph.cache.memory import InMemoryCache\\nfrom langgraph.types import CachePolicy\\n\\n\\nclass State(TypedDict):\\n    x: int\\n    result: int\\n\\n\\nbuilder = StateGraph(State)\\n\\n\\ndef expensive_node(state: State) -> dict[str, int]:\\n    # expensive computation\\n    time.sleep(2)\\n    return {\"result\": state[\"x\"] * 2}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='builder.add_node(\"expensive_node\", expensive_node, cache_policy=CachePolicy(ttl=3))\\nbuilder.set_entry_point(\"expensive_node\")\\nbuilder.set_finish_point(\"expensive_node\")\\n\\ngraph = builder.compile(cache=InMemoryCache())\\n\\nprint(graph.invoke({\"x\": 5}, stream_mode=\\'updates\\'))    \\n# [{\\'expensive_node\\': {\\'result\\': 10}}]\\nprint(graph.invoke({\"x\": 5}, stream_mode=\\'updates\\'))    \\n# [{\\'expensive_node\\': {\\'result\\': 10}, \\'__metadata__\\': {\\'cached\\': True}}]\\n\\n\\nFirst run takes two seconds to run (due to mocked expensive computation).\\nSecond run utilizes cache and returns quickly.\\n\\n\\u200bEdges\\nEdges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Normal Edges: Go directly from one node to the next.\\nConditional Edges: Call a function to determine which node(s) to go to next.\\nEntry Point: Which node to call first when user input arrives.\\nConditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.\\n\\nA node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.\\n\\u200bNormal Edges\\nIf you always want to go from node A to node B, you can use the add_edge method directly.\\nCopyAsk AIgraph.add_edge(\"node_a\", \"node_b\")\\n\\n\\u200bConditional Edges\\nIf you want to optionally route to one or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a “routing function” to call after that node is executed:\\nCopyAsk AIgraph.add_conditional_edges(\"node_a\", routing_function)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Similar to nodes, the routing_function accepts the current state of the graph and returns a value.\\nBy default, the return value routing_function is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.\\nYou can optionally provide a dictionary that maps the routing_function’s output to the name of the next node.\\nCopyAsk AIgraph.add_conditional_edges(\"node_a\", routing_function, {True: \"node_b\", False: \"node_c\"})\\n\\nUse Command instead of conditional edges if you want to combine state updates and routing in a single function.\\n\\u200bEntry Point\\nThe entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph.\\nCopyAsk AIfrom langgraph.graph import START\\n\\ngraph.add_edge(START, \"node_a\")'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='graph.add_edge(START, \"node_a\")\\n\\n\\u200bConditional Entry Point\\nA conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this.\\nCopyAsk AIfrom langgraph.graph import START\\n\\ngraph.add_conditional_edges(START, routing_function)\\n\\nYou can optionally provide a dictionary that maps the routing_function’s output to the name of the next node.\\nCopyAsk AIgraph.add_conditional_edges(START, routing_function, {True: \"node_b\", False: \"node_c\"})'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bSend\\nBy default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object).\\nTo support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node.\\nCopyAsk AIdef continue_to_jokes(state: OverallState):\\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\\'subjects\\']]'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='graph.add_conditional_edges(\"node_a\", continue_to_jokes)\\n\\n\\u200bCommand\\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:\\nCopyAsk AIdef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        # state update\\n        update={\"foo\": \"bar\"},\\n        # control flow\\n        goto=\"my_other_node\"\\n    )\\n\\nWith Command you can also achieve dynamic control flow behavior (identical to conditional edges):\\nCopyAsk AIdef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    if state[\"foo\"] == \"bar\":\\n        return Command(update={\"foo\": \"baz\"}, goto=\"my_other_node\")'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='When returning Command in your node functions, you must add return type annotations with the list of node names the node is routing to, e.g. Command[Literal[\"my_other_node\"]]. This is necessary for the graph rendering and tells LangGraph that my_node can navigate to my_other_node.\\nCheck out this how-to guide for an end-to-end example of how to use Command.\\n\\u200bWhen should I use Command instead of conditional edges?\\n\\nUse Command when you need to both update the graph state and route to a different node. For example, when implementing multi-agent handoffs where it’s important to route to a different agent and pass some information to that agent.\\nUse conditional edges to route between nodes conditionally without updating the state.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bNavigating to a node in a parent graph\\nIf you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify graph=Command.PARENT in Command:\\nCopyAsk AIdef my_node(state: State) -> Command[Literal[\"other_subgraph\"]]:\\n    return Command(\\n        update={\"foo\": \"bar\"},\\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\\n        graph=Command.PARENT\\n    )'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Setting graph to Command.PARENT will navigate to the closest parent graph.When you send updates from a subgraph node to a parent graph node for a key that’s shared by both parent and subgraph state schemas, you must define a reducer for the key you’re updating in the parent graph state. See this example.\\nThis is particularly useful when implementing multi-agent handoffs.\\nCheck out this guide for detail.\\n\\u200bUsing inside tools\\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation.\\nRefer to this guide for detail.\\n\\u200bHuman-in-the-loop\\nCommand is an important part of human-in-the-loop workflows: when using interrupt() to collect user input, Command is then used to supply the input and resume execution via Command(resume=\"User input\"). Check out this conceptual guide for more information.\\n\\u200bGraph Migrations'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bGraph Migrations\\nLangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)\\nFor threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) — if this is a blocker please reach out and we can prioritize a solution.\\nFor modifying state, we have full backwards and forwards compatibility for adding and removing keys\\nState keys that are renamed lose their saved state in existing threads\\nState keys whose types change in incompatible ways could currently cause issues in threads with state from before the change — if this is a blocker please reach out and we can prioritize a solution.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bRuntime Context\\nWhen creating a graph, you can specify a context_schema for runtime context passed to nodes. This is useful for passing\\ninformation to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection.\\nCopyAsk AI@dataclass\\nclass ContextSchema:\\n    llm_provider: str = \"openai\"\\n\\ngraph = StateGraph(State, context_schema=ContextSchema)\\n\\nYou can then pass this context into the graph using the context parameter of the invoke method.\\nCopyAsk AIgraph.invoke(inputs, context={\"llm_provider\": \"anthropic\"})\\n\\nYou can then access and use this context inside a node or conditional edge:\\nCopyAsk AIfrom langgraph.runtime import Runtime\\n\\ndef node_a(state: State, runtime: Runtime[ContextSchema]):\\n    llm = get_llm(runtime.context.llm_provider)\\n    # ...'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='See this guide for a full breakdown on configuration.\\n\\u200bRecursion Limit\\nThe recursion limit sets the maximum number of super-steps the graph can execute during a single execution. Once the limit is reached, LangGraph will raise GraphRecursionError. By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to invoke/stream via the config dictionary. Importantly, recursion_limit is a standalone config key and should not be passed inside the configurable key as all other user-defined configuration. See the example below:\\nCopyAsk AIgraph.invoke(inputs, config={\"recursion_limit\": 5}, context={\"llm\": \"anthropic\"})'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Read this how-to to learn more about how the recursion limit works.\\n\\u200bAccessing and handling the recursion counter\\nThe current step counter is accessible in config[\"metadata\"][\"langgraph_step\"] within any node, allowing for proactive recursion handling before hitting the recursion limit. This enables you to implement graceful degradation strategies within your graph logic.\\n\\u200bHow it works\\nThe step counter is stored in config[\"metadata\"][\"langgraph_step\"]. The recursion limit check follows the logic: step > stop where stop = step + recursion_limit + 1. When the limit is exceeded, LangGraph raises a GraphRecursionError.\\n\\u200bAccessing the current step counter\\nYou can access the current step counter within any node to monitor execution progress.\\nCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='def my_node(state: dict, config: RunnableConfig) -> dict:\\n    current_step = config[\"metadata\"][\"langgraph_step\"]\\n    print(f\"Currently on step: {current_step}\")\\n    return state\\n\\n\\u200bProactive recursion handling\\nYou can check the step counter and proactively route to a different node before hitting the limit. This allows for graceful degradation within your graph.\\nCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph, END\\n\\ndef reasoning_node(state: dict, config: RunnableConfig) -> dict:\\n    current_step = config[\"metadata\"][\"langgraph_step\"]\\n    recursion_limit = config[\"recursion_limit\"]  # always present, defaults to 25\\n\\n    # Check if we\\'re approaching the limit (e.g., 80% threshold)\\n    if current_step >= recursion_limit * 0.8:\\n        return {\\n            **state,\\n            \"route_to\": \"fallback\",\\n            \"reason\": \"Approaching recursion limit\"\\n        }'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='# Normal processing\\n    return {\"messages\": state[\"messages\"] + [\"thinking...\"]}\\n\\ndef fallback_node(state: dict, config: RunnableConfig) -> dict:\\n    \"\"\"Handle cases where recursion limit is approaching\"\"\"\\n    return {\\n        **state,\\n        \"messages\": state[\"messages\"] + [\"Reached complexity limit, providing best effort answer\"]\\n    }\\n\\ndef route_based_on_state(state: dict) -> str:\\n    if state.get(\"route_to\") == \"fallback\":\\n        return \"fallback\"\\n    elif state.get(\"done\"):\\n        return END\\n    return \"reasoning\"\\n\\n# Build graph\\ngraph = StateGraph(dict)\\ngraph.add_node(\"reasoning\", reasoning_node)\\ngraph.add_node(\"fallback\", fallback_node)\\ngraph.add_conditional_edges(\"reasoning\", route_based_on_state)\\ngraph.add_edge(\"fallback\", END)\\ngraph.set_entry_point(\"reasoning\")\\n\\napp = graph.compile()'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='app = graph.compile()\\n\\n\\u200bProactive vs reactive approaches\\nThere are two main approaches to handling recursion limits: proactive (monitoring within the graph) and reactive (catching errors externally).\\nCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph, END\\nfrom langgraph.errors import GraphRecursionError\\n\\n# Proactive Approach (recommended)\\ndef agent_with_monitoring(state: dict, config: RunnableConfig) -> dict:\\n    \"\"\"Proactively monitor and handle recursion within the graph\"\"\"\\n    current_step = config[\"metadata\"][\"langgraph_step\"]\\n    recursion_limit = config[\"recursion_limit\"]\\n\\n    # Early detection - route to internal handling\\n    if current_step >= recursion_limit - 2:  # 2 steps before limit\\n        return {\\n            **state,\\n            \"status\": \"recursion_limit_approaching\",\\n            \"final_answer\": \"Reached iteration limit, returning partial result\"\\n        }'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='# Normal processing\\n    return {\"messages\": state[\"messages\"] + [f\"Step {current_step}\"]}\\n\\n# Reactive Approach (fallback)\\ntry:\\n    result = graph.invoke(initial_state, {\"recursion_limit\": 10})\\nexcept GraphRecursionError as e:\\n    # Handle externally after graph execution fails\\n    result = fallback_handler(initial_state)\\n\\nThe key differences between these approaches are:\\n\\nApproachDetectionHandlingControl Flow\\nProactive (using langgraph_step)Before limit reachedInside graph via conditional routingGraph continues to completion nodeReactive (catching GraphRecursionError)After limit exceededOutside graph in try/catchGraph execution terminated\\n\\nProactive advantages:\\n\\nGraceful degradation within the graph\\nCan save intermediate state in checkpoints\\nBetter user experience with partial results\\nGraph completes normally (no exception)\\n\\nReactive advantages:\\n\\nSimpler implementation\\nNo need to modify graph logic\\nCentralized error handling'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Simpler implementation\\nNo need to modify graph logic\\nCentralized error handling\\n\\n\\u200bOther available metadata\\nAlong with langgraph_step, the following metadata is also available in config[\"metadata\"]:\\nCopyAsk AIdef inspect_metadata(state: dict, config: RunnableConfig) -> dict:\\n    metadata = config[\"metadata\"]\\n\\n    print(f\"Step: {metadata[\\'langgraph_step\\']}\")\\n    print(f\"Node: {metadata[\\'langgraph_node\\']}\")\\n    print(f\"Triggers: {metadata[\\'langgraph_triggers\\']}\")\\n    print(f\"Path: {metadata[\\'langgraph_path\\']}\")\\n    print(f\"Checkpoint NS: {metadata[\\'langgraph_checkpoint_ns\\']}\")\\n\\n    return state\\n\\n\\u200bVisualization\\nIt’s often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Edit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoObservabilityPreviousUse the graph APINext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content=\"Functional API overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationFunctional APIFunctional API overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIFunctional APIUse the Functional APIRuntimeEnglishcloseOn this pageFunctional API vs. Graph APIExampleEntrypointDefinitionInjectable parametersExecutingResumingShort-term memoryentrypoint.finalTaskDefinitionExecutionWhen to use a taskSerializationDeterminismIdempotencyCommon PitfallsHandling side\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='to use a taskSerializationDeterminismIdempotencyCommon PitfallsHandling side effectsNon-deterministic control flowLangGraph APIsFunctional APIFunctional API overviewCopy pageCopy pageThe Functional API allows you to add LangGraph’s key features — persistence, memory, human-in-the-loop, and streaming — to your applications with minimal changes to your existing code.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='It is designed to integrate these features into existing code that may use standard language primitives for branching and control flow, such as if statements, for loops, and function calls. Unlike many data orchestration frameworks that require restructuring code into an explicit pipeline or DAG, the Functional API allows you to incorporate these capabilities without enforcing a rigid execution model.\\nThe Functional API uses two key building blocks:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='@entrypoint – Marks a function as the starting point of a workflow, encapsulating logic and managing execution flow, including handling long-running tasks and interrupts.\\n@task – Represents a discrete unit of work, such as an API call or data processing step, that can be executed asynchronously within an entrypoint. Tasks return a future-like object that can be awaited or resolved synchronously.\\n\\nThis provides a minimal abstraction for building workflows with state management and streaming.\\nFor information on how to use the functional API, see Use Functional API.\\n\\u200bFunctional API vs. Graph API\\nFor users who prefer a more declarative approach, LangGraph’s Graph API allows you to define workflows using a Graph paradigm. Both APIs share the same underlying runtime, so you can use them together in the same application.\\nHere are some key differences:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Control flow: The Functional API does not require thinking about graph structure. You can use standard Python constructs to define workflows. This will usually trim the amount of code you need to write.\\nShort-term memory: The GraphAPI requires declaring a State and may require defining reducers to manage updates to the graph state. @entrypoint and @tasks do not require explicit state management as their state is scoped to the function and is not shared across functions.\\nCheckpointing: Both APIs generate and use checkpoints. In the Graph API a new checkpoint is generated after every superstep. In the Functional API, when tasks are executed, their results are saved to an existing checkpoint associated with the given entrypoint instead of creating a new checkpoint.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Visualization: The Graph API makes it easy to visualize the workflow as a graph which can be useful for debugging, understanding the workflow, and sharing with others. The Functional API does not support visualization as the graph is dynamically generated during runtime.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bExample\\nBelow we demonstrate a simple application that writes an essay and interrupts to request human review.\\nCopyAsk AIfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langgraph.func import entrypoint, task\\nfrom langgraph.types import interrupt\\n\\n@task\\ndef write_essay(topic: str) -> str:\\n    \"\"\"Write an essay about the given topic.\"\"\"\\n    time.sleep(1) # A placeholder for a long-running task.\\n    return f\"An essay about topic: {topic}\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='@entrypoint(checkpointer=InMemorySaver())\\ndef workflow(topic: str) -> dict:\\n    \"\"\"A simple workflow that writes an essay and asks for a review.\"\"\"\\n    essay = write_essay(\"cat\").result()\\n    is_approved = interrupt({\\n        # Any json-serializable payload provided to interrupt as argument.\\n        # It will be surfaced on the client side as an Interrupt when streaming data\\n        # from the workflow.\\n        \"essay\": essay, # The essay we want reviewed.\\n        # We can add any additional information that we need.\\n        # For example, introduce a key called \"action\" with some instructions.\\n        \"action\": \"Please approve/reject the essay\",\\n    })\\n\\n    return {\\n        \"essay\": essay, # The essay that was generated\\n        \"is_approved\": is_approved, # Response from HIL\\n    }'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Detailed ExplanationThis workflow will write an essay about the topic “cat” and then pause to get a review from a human. The workflow can be interrupted for an indefinite amount of time until a review is provided.When the workflow is resumed, it executes from the very start, but because the result of the writeEssay task was already saved, the task result will be loaded from the checkpoint instead of being recomputed.CopyAsk AIimport time\\nimport uuid\\nfrom langgraph.func import entrypoint, task\\nfrom langgraph.types import interrupt\\nfrom langgraph.checkpoint.memory import InMemorySaver\\n\\n\\n@task\\ndef write_essay(topic: str) -> str:\\n    \"\"\"Write an essay about the given topic.\"\"\"\\n    time.sleep(1)  # This is a placeholder for a long-running task.\\n    return f\"An essay about topic: {topic}\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='@entrypoint(checkpointer=InMemorySaver())\\ndef workflow(topic: str) -> dict:\\n    \"\"\"A simple workflow that writes an essay and asks for a review.\"\"\"\\n    essay = write_essay(\"cat\").result()\\n    is_approved = interrupt(\\n        {\\n            # Any json-serializable payload provided to interrupt as argument.\\n            # It will be surfaced on the client side as an Interrupt when streaming data\\n            # from the workflow.\\n            \"essay\": essay,  # The essay we want reviewed.\\n            # We can add any additional information that we need.\\n            # For example, introduce a key called \"action\" with some instructions.\\n            \"action\": \"Please approve/reject the essay\",\\n        }\\n    )\\n    return {\\n        \"essay\": essay,  # The essay that was generated\\n        \"is_approved\": is_approved,  # Response from HIL\\n    }'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='thread_id = str(uuid.uuid4())\\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\\nfor item in workflow.stream(\"cat\", config):\\n    print(item)\\n# > {\\'write_essay\\': \\'An essay about topic: cat\\'}\\n# > {\\n# >     \\'__interrupt__\\': (\\n# >        Interrupt(\\n# >            value={\\n# >                \\'essay\\': \\'An essay about topic: cat\\',\\n# >                \\'action\\': \\'Please approve/reject the essay\\'\\n# >            },\\n# >            id=\\'b9b2b9d788f482663ced6dc755c9e981\\'\\n# >        ),\\n# >    )\\n# > }\\nAn essay has been written and is ready for review. Once the review is provided, we can resume the workflow:CopyAsk AIfrom langgraph.types import Command\\n\\n# Get review from a user (e.g., via a UI)\\n# In this case, we\\'re using a bool, but this can be any json-serializable value.\\nhuman_review = True'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content=\"for item in workflow.stream(Command(resume=human_review), config):\\n    print(item)\\nCopyAsk AI{'workflow': {'essay': 'An essay about topic: cat', 'is_approved': False}}\\nThe workflow has been completed and the review has been added to the essay.\\n\\u200bEntrypoint\\nThe @entrypoint decorator can be used to create a workflow from a function. It encapsulates workflow logic and manages execution flow, including handling long-running tasks and interrupts.\\n\\u200bDefinition\\nAn entrypoint is defined by decorating a function with the @entrypoint decorator.\\nThe function must accept a single positional argument, which serves as the workflow input. If you need to pass multiple pieces of data, use a dictionary as the input type for the first argument.\\nDecorating a function with an entrypoint produces a Pregel instance which helps to manage the execution of the workflow (e.g., handles streaming, resumption, and checkpointing).\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='You will usually want to pass a checkpointer to the @entrypoint decorator to enable persistence and use features like human-in-the-loop.\\n Sync AsyncCopyAsk AIfrom langgraph.func import entrypoint'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(some_input: dict) -> int:\\n    # some logic that may involve long-running tasks like API calls,\\n    # and may be interrupted for human-in-the-loop.\\n    ...\\n    return result'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Serialization\\nThe inputs and outputs of entrypoints must be JSON-serializable to support checkpointing. Please see the serialization section for more details.\\n\\u200bInjectable parameters\\nWhen declaring an entrypoint, you can request access to additional parameters that will be injected automatically at run time. These parameters include:\\nParameterDescriptionpreviousAccess the state associated with the previous checkpoint for the given thread. See short-term-memory.storeAn instance of [BaseStore][langgraph.store.base.BaseStore]. Useful for long-term memory.writerUse to access the StreamWriter when working with Async Python < 3.11. See streaming with functional API for details.configFor accessing run time configuration. See RunnableConfig for information.\\nDeclare the parameters with the appropriate name and type annotation.\\nRequesting Injectable ParametersCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.func import entrypoint'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='from langgraph.func import entrypoint\\nfrom langgraph.store.base import BaseStore\\nfrom langgraph.store.memory import InMemoryStore'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='in_memory_store = InMemoryStore(...)  # An instance of InMemoryStore for long-term memory\\n\\n@entrypoint(\\n    checkpointer=checkpointer,  # Specify the checkpointer\\n    store=in_memory_store  # Specify the store\\n)\\ndef my_workflow(\\n    some_input: dict,  # The input (e.g., passed via `invoke`)\\n    *,\\n    previous: Any = None, # For short-term memory\\n    store: BaseStore,  # For long-term memory\\n    writer: StreamWriter,  # For streaming custom data\\n    config: RunnableConfig  # For accessing the configuration passed to the entrypoint\\n) -> ...:\\n\\n\\u200bExecuting\\nUsing the @entrypoint yields a Pregel object that can be executed using the invoke, ainvoke, stream, and astream methods.\\n Invoke Async Invoke Stream Async StreamCopyAsk AIconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\nmy_workflow.invoke(some_input, config)  # Wait for the result synchronously'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bResuming\\nResuming an execution after an interrupt can be done by passing a resume value to the Command primitive.\\n Invoke Async Invoke Stream Async StreamCopyAsk AIfrom langgraph.types import Command\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\n\\nmy_workflow.invoke(Command(resume=some_resume_value), config)\\n\\nResuming after an error\\nTo resume after an error, run the entrypoint with a None and the same thread id (config).\\nThis assumes that the underlying error has been resolved and execution can proceed successfully.\\n Invoke Async Invoke Stream Async StreamCopyAsk AI\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\n\\nmy_workflow.invoke(None, config)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='my_workflow.invoke(None, config)\\n\\n\\u200bShort-term memory\\nWhen an entrypoint is defined with a checkpointer, it stores information between successive invocations on the same thread id in checkpoints.\\nThis allows accessing the state from the previous invocation using the previous parameter.\\nBy default, the previous parameter is the return value of the previous invocation.\\nCopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(number: int, *, previous: Any = None) -> int:\\n    previous = previous or 0\\n    return number + previous\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\n\\nmy_workflow.invoke(1, config)  # 1 (previous was None)\\nmy_workflow.invoke(2, config)  # 3 (previous was 1 from the previous invocation)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bentrypoint.final\\nentrypoint.final is a special primitive that can be returned from an entrypoint and allows decoupling the value that is saved in the checkpoint from the return value of the entrypoint.\\nThe first value is the return value of the entrypoint, and the second value is the value that will be saved in the checkpoint. The type annotation is entrypoint.final[return_type, save_type].\\nCopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(number: int, *, previous: Any = None) -> entrypoint.final[int, int]:\\n    previous = previous or 0\\n    # This will return the previous value to the caller, saving\\n    # 2 * number to the checkpoint, which will be used in the next invocation\\n    # for the `previous` parameter.\\n    return entrypoint.final(value=previous, save=2 * number)\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"1\"\\n    }\\n}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='config = {\\n    \"configurable\": {\\n        \"thread_id\": \"1\"\\n    }\\n}\\n\\nmy_workflow.invoke(3, config)  # 0 (previous was None)\\nmy_workflow.invoke(1, config)  # 6 (previous was 3 * 2 from the previous invocation)\\n\\n\\u200bTask\\nA task represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics:\\n\\nAsynchronous Execution: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking.\\nCheckpointing: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See persistence for more details).\\n\\n\\u200bDefinition\\nTasks are defined using the @task decorator, which wraps a regular Python function.\\nCopyAsk AIfrom langgraph.func import task\\n\\n@task()\\ndef slow_computation(input_value):\\n    # Simulate a long-running operation\\n    ...\\n    return result'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Serialization\\nThe outputs of tasks must be JSON-serializable to support checkpointing.\\n\\u200bExecution\\nTasks can only be called from within an entrypoint, another task, or a state graph node.\\nTasks cannot be called directly from the main application code.\\nWhen you call a task, it returns immediately with a future object. A future is a placeholder for a result that will be available later.\\nTo obtain the result of a task, you can either wait for it synchronously (using result()) or await it asynchronously (using await).\\n Synchronous Invocation Asynchronous InvocationCopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(some_input: int) -> int:\\n    future = slow_computation(some_input)\\n    return future.result()  # Wait for the result synchronously\\n\\n\\u200bWhen to use a task\\nTasks are useful in the following scenarios:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bWhen to use a task\\nTasks are useful in the following scenarios:\\n\\nCheckpointing: When you need to save the result of a long-running operation to a checkpoint, so you don’t need to recompute it when resuming the workflow.\\nHuman-in-the-loop: If you’re building a workflow that requires human intervention, you MUST use tasks to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the determinism section for more details.\\nParallel Execution: For I/O-bound tasks, tasks enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs).\\nObservability: Wrapping operations in tasks provides a way to track the progress of the workflow and monitor the execution of individual operations using LangSmith.\\nRetryable Work: When work needs to be retried to handle failures or inconsistencies, tasks provide a way to encapsulate and manage the retry logic.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bSerialization\\nThere are two key aspects to serialization in LangGraph:\\n\\nentrypoint inputs and outputs must be JSON-serializable.\\ntask outputs must be JSON-serializable.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='These requirements are necessary for enabling checkpointing and workflow resumption. Use python primitives like dictionaries, lists, strings, numbers, and booleans to ensure that your inputs and outputs are serializable.\\nSerialization ensures that workflow state, such as task results and intermediate values, can be reliably saved and restored. This is critical for enabling human-in-the-loop interactions, fault tolerance, and parallel execution.\\nProviding non-serializable inputs or outputs will result in a runtime error when a workflow is configured with a checkpointer.\\n\\u200bDeterminism\\nTo utilize features like human-in-the-loop, any randomness should be encapsulated inside of tasks. This guarantees that when execution is halted (e.g., for human in the loop) and then resumed, it will follow the same sequence of steps, even if task results are non-deterministic.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='LangGraph achieves this behavior by persisting task and subgraph results as they execute. A well-designed workflow ensures that resuming execution follows the same sequence of steps, allowing previously computed results to be retrieved correctly without having to re-execute them. This is particularly useful for long-running tasks or tasks with non-deterministic results, as it avoids repeating previously done work and allows resuming from essentially the same.\\nWhile different runs of a workflow can produce different results, resuming a specific run should always follow the same sequence of recorded steps. This allows LangGraph to efficiently look up task and subgraph results that were executed prior to the graph being interrupted and avoid recomputing them.\\n\\u200bIdempotency'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bIdempotency\\nIdempotency ensures that running the same operation multiple times produces the same result. This helps prevent duplicate API calls and redundant processing if a step is rerun due to a failure. Always place API calls inside tasks functions for checkpointing, and design them to be idempotent in case of re-execution. Re-execution can occur if a task starts, but does not complete successfully. Then, if the workflow is resumed, the task will run again. Use idempotency keys or verify existing results to avoid duplication.\\n\\u200bCommon Pitfalls\\n\\u200bHandling side effects\\nEncapsulate side effects (e.g., writing to a file, sending an email) in tasks to ensure they are not executed multiple times when resuming a workflow.\\n Incorrect CorrectIn this example, a side effect (writing to a file) is directly included in the workflow, so it will be executed a second time when resuming the workflow.CopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(inputs: dict) -> int:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='def my_workflow(inputs: dict) -> int:\\n    # This code will be executed a second time when resuming the workflow.\\n    # Which is likely not what you want.\\n    with open(\"output.txt\", \"w\") as f:  \\n        f.write(\"Side effect executed\")  \\n    value = interrupt(\"question\")\\n    return value'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bNon-deterministic control flow\\nOperations that might give different results each time (like getting current time or random numbers) should be encapsulated in tasks to ensure that on resume, the same result is returned.\\n\\nIn a task: Get random number (5) → interrupt → resume → (returns 5 again) → …\\nNot in a task: Get random number (5) → interrupt → resume → get new random number (7) → …'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='This is especially important when using human-in-the-loop workflows with multiple interrupts calls. LangGraph keeps a list of resume values for each task/entrypoint. When an interrupt is encountered, it’s matched with the corresponding resume value. This matching is strictly index-based, so the order of the resume values should match the order of the interrupts.\\nIf order of execution is not maintained when resuming, one interrupt call may be matched with the wrong resume value, leading to incorrect results.\\nPlease read the section on determinism for more details.\\n Incorrect CorrectIn this example, the workflow uses the current time to determine which task to execute. This is non-deterministic because the result of the workflow depends on the time at which it is executed.CopyAsk AIfrom langgraph.func import entrypoint\\n\\n@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(inputs: dict) -> int:\\n    t0 = inputs[\"t0\"]\\n    t1 = time.time()  \\n\\n    delta_t = t1 - t0'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='delta_t = t1 - t0\\n\\n    if delta_t > 1:\\n        result = slow_task(1).result()\\n        value = interrupt(\"question\")\\n    else:\\n        result = slow_task(2).result()\\n        value = interrupt(\"question\")\\n\\n    return {\\n        \"result\": result,\\n        \"value\": value\\n    }\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoUse the graph APIPreviousUse the functional APINext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content=\"LangGraph overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeEnglishcloseOn this page InstallCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageCopy pageLangGraph v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='Trusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b Install\\npipuvCopyAsk AIpip install -U langgraph'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='Then, create a simple hello world example:\\nCopyAsk AIfrom langgraph.graph import StateGraph, MessagesState, START, END\\n\\ndef mock_llm(state: MessagesState):\\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\\n\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(mock_llm)\\ngraph.add_edge(START, \"mock_llm\")\\ngraph.add_edge(\"mock_llm\", END)\\ngraph = graph.compile()\\n\\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\\n\\n\\u200bCore benefits\\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='Durable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\nHuman-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.\\nComprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\nDebugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\nProduction-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\\n\\n\\u200bLangGraph ecosystem\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='LangSmith — Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\\nLangSmith — Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in Studio.\\nLangChain - Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\\n\\n\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content=\"Edit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoWhat's new in v1Next⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content=\"Workflows and agents - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedWorkflows and agentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeEnglishcloseOn this pageSetupLLMs and augmentationsPrompt chainingParallelizationRoutingOrchestrator-workerCreating workers in LangGraphEvaluator-optimizerAgentsGet startedWorkflows and agentsCopy pageCopy pageThis guide reviews common workflow and agent patterns.\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Workflows have predetermined code paths and are designed to operate in a certain order.\\nAgents are dynamic and define their own processes and tool usage.\\n\\n\\nLangGraph offers several benefits when building agents and workflows, including persistence, streaming, and support for debugging as well as deployment.\\n\\u200bSetup\\nTo build a workflow or agent, you can use any chat model that supports structured outputs and tool calling. The following example uses Anthropic:\\n\\nInstall dependencies:\\n\\nCopyAsk AIpip install langchain_core langchain-anthropic langgraph\\n\\n\\nInitialize the LLM:\\n\\nCopyAsk AIimport os\\nimport getpass\\n\\nfrom langchain_anthropic import ChatAnthropic\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\\n\\n\\u200bLLMs and augmentations\\nWorkflows and agentic systems are based on LLMs and the various augmentations you add to them. Tool calling, structured outputs, and short term memory are a few options for tailoring LLMs to your needs.\\n\\nCopyAsk AI# Schema for structured output\\nfrom pydantic import BaseModel, Field\\n\\n\\nclass SearchQuery(BaseModel):\\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\\n    justification: str = Field(\\n        None, description=\"Why this query is relevant to the user\\'s request.\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nstructured_llm = llm.with_structured_output(SearchQuery)\\n\\n# Invoke the augmented LLM\\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\\n\\n# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b\\n\\n# Augment the LLM with tools\\nllm_with_tools = llm.bind_tools([multiply])\\n\\n# Invoke the LLM with input that triggers the tool call\\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\\n\\n# Get the tool call\\nmsg.tool_calls\\n\\n\\u200bPrompt chaining\\nPrompt chaining is when each LLM call processes the output of the previous call. It’s often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:\\n\\nTranslating documents into different languages\\nVerifying generated content for consistency\\n\\n\\nGraph APIFunctional APICopyAsk AIfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom IPython.display import Image, display\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    improved_joke: str\\n    final_joke: str\\n\\n\\n# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a short joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef check_punchline(state: State):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\\n        return \"Pass\"\\n    return \"Fail\"\\n\\n\\ndef improve_joke(state: State):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state[\\'joke\\']}\")\\n    return {\"improved_joke\": msg.content}\\n\\n\\ndef polish_joke(state: State):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state[\\'improved_joke\\']}\")\\n    return {\"final_joke\": msg.content}\\n\\n\\n# Build workflow\\nworkflow = StateGraph(State)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Build workflow\\nworkflow = StateGraph(State)\\n\\n# Add nodes\\nworkflow.add_node(\"generate_joke\", generate_joke)\\nworkflow.add_node(\"improve_joke\", improve_joke)\\nworkflow.add_node(\"polish_joke\", polish_joke)\\n\\n# Add edges to connect nodes\\nworkflow.add_edge(START, \"generate_joke\")\\nworkflow.add_conditional_edges(\\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\\n)\\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\\nworkflow.add_edge(\"polish_joke\", END)\\n\\n# Compile\\nchain = workflow.compile()\\n\\n# Show workflow\\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = chain.invoke({\"topic\": \"cats\"})\\nprint(\"Initial joke:\")\\nprint(state[\"joke\"])\\nprint(\"\\\\n--- --- ---\\\\n\")\\nif \"improved_joke\" in state:\\n    print(\"Improved joke:\")\\n    print(state[\"improved_joke\"])\\n    print(\"\\\\n--- --- ---\\\\n\")\\n\\n    print(\"Final joke:\")\\n    print(state[\"final_joke\"])\\nelse:\\n    print(\"Joke failed quality gate - no punchline detected!\")'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='\\u200bParallelization\\nWith parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:\\n\\nSplit up subtasks and run them in parallel, which increases speed\\nRun tasks multiple times to check for different outputs, which increases confidence\\n\\nSome examples include:\\n\\nRunning one subtask that processes a document for keywords, and a second subtask to check for formatting errors\\nRunning a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources\\n\\n\\nGraph APIFunctional APICopyAsk AI# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    story: str\\n    poem: str\\n    combined_output: str\\n\\n\\n# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef call_llm_2(state: State):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n\\n    msg = llm.invoke(f\"Write a story about {state[\\'topic\\']}\")\\n    return {\"story\": msg.content}\\n\\n\\ndef call_llm_3(state: State):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n\\n    msg = llm.invoke(f\"Write a poem about {state[\\'topic\\']}\")\\n    return {\"poem\": msg.content}\\n\\n\\ndef aggregator(state: State):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {state[\\'topic\\']}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{state[\\'story\\']}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{state[\\'joke\\']}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{state[\\'poem\\']}\"\\n    return {\"combined_output\": combined}\\n\\n\\n# Build workflow\\nparallel_builder = StateGraph(State)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Build workflow\\nparallel_builder = StateGraph(State)\\n\\n# Add nodes\\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\\nparallel_builder.add_node(\"aggregator\", aggregator)\\n\\n# Add edges to connect nodes\\nparallel_builder.add_edge(START, \"call_llm_1\")\\nparallel_builder.add_edge(START, \"call_llm_2\")\\nparallel_builder.add_edge(START, \"call_llm_3\")\\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\\nparallel_builder.add_edge(\"aggregator\", END)\\nparallel_workflow = parallel_builder.compile()\\n\\n# Show workflow\\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])\\n\\n\\u200bRouting\\nRouting workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.\\n\\nGraph APIFunctional APICopyAsk AIfrom typing_extensions import Literal\\nfrom langchain.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str\\n\\n\\n# Nodes\\ndef llm_call_1(state: State):\\n    \"\"\"Write a story\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_2(state: State):\\n    \"\"\"Write a joke\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_3(state: State):\\n    \"\"\"Write a poem\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_router(state: State):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=state[\"input\"]),\\n        ]\\n    )\\n\\n    return {\"decision\": decision.step}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='return {\"decision\": decision.step}\\n\\n\\n# Conditional edge function to route to the appropriate node\\ndef route_decision(state: State):\\n    # Return the node name you want to visit next\\n    if state[\"decision\"] == \"story\":\\n        return \"llm_call_1\"\\n    elif state[\"decision\"] == \"joke\":\\n        return \"llm_call_2\"\\n    elif state[\"decision\"] == \"poem\":\\n        return \"llm_call_3\"\\n\\n\\n# Build workflow\\nrouter_builder = StateGraph(State)\\n\\n# Add nodes\\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Add edges to connect nodes\\nrouter_builder.add_edge(START, \"llm_call_router\")\\nrouter_builder.add_conditional_edges(\\n    \"llm_call_router\",\\n    route_decision,\\n    {  # Name returned by route_decision : Name of next node to visit\\n        \"llm_call_1\": \"llm_call_1\",\\n        \"llm_call_2\": \"llm_call_2\",\\n        \"llm_call_3\": \"llm_call_3\",\\n    },\\n)\\nrouter_builder.add_edge(\"llm_call_1\", END)\\nrouter_builder.add_edge(\"llm_call_2\", END)\\nrouter_builder.add_edge(\"llm_call_3\", END)\\n\\n# Compile workflow\\nrouter_workflow = router_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\\nprint(state[\"output\"])\\n\\n\\u200bOrchestrator-worker\\nIn an orchestrator-worker configuration, the orchestrator:\\n\\nBreaks down tasks into subtasks\\nDelegates subtasks to workers\\nSynthesizes worker outputs into a final result'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Orchestrator-worker workflows provide more flexibility and are often used when subtasks cannot be predefined the way they can with parallelization. This is common with workflows that write code or need to update content across multiple files. For example, a workflow that needs to update installation instructions for multiple Python libraries across an unknown number of documents might use this pattern.\\nGraph APIFunctional APICopyAsk AIfrom typing import Annotated, List\\nimport operator\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\n\\u200bCreating workers in LangGraph\\nOrchestrator-worker workflows are common and LangGraph has built-in support for them. The Send API lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the Send API to send a section to each worker.\\nCopyAsk AIfrom langgraph.types import Send\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str  # Report topic\\n    sections: list[Section]  # List of report sections\\n    completed_sections: Annotated[\\n        list, operator.add\\n    ]  # All workers write to this key in parallel\\n    final_report: str  # Final report'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Worker state\\nclass WorkerState(TypedDict):\\n    section: Section\\n    completed_sections: Annotated[list, operator.add]\\n\\n\\n# Nodes\\ndef orchestrator(state: State):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {state[\\'topic\\']}\"),\\n        ]\\n    )\\n\\n    return {\"sections\": report_sections.sections}\\n\\n\\ndef llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='def llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    section = llm.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\\n            ),\\n            HumanMessage(\\n                content=f\"Here is the section name: {state[\\'section\\'].name} and description: {state[\\'section\\'].description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return {\"completed_sections\": [section.content]}\\n\\n\\ndef synthesizer(state: State):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n\\n    # List of completed sections\\n    completed_sections = state[\"completed_sections\"]\\n\\n    # Format completed section to str to use as context for final sections\\n    completed_report_sections = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='return {\"final_report\": completed_report_sections}\\n\\n\\n# Conditional edge function to create llm_call workers that each write a section of the report\\ndef assign_workers(state: State):\\n    \"\"\"Assign a worker to each section in the plan\"\"\"\\n\\n    # Kick off section writing in parallel via Send() API\\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\\n\\n\\n# Build workflow\\norchestrator_worker_builder = StateGraph(State)\\n\\n# Add the nodes\\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\\n\\n# Add edges to connect nodes\\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\\norchestrator_worker_builder.add_conditional_edges(\\n    \"orchestrator\", assign_workers, [\"llm_call\"]\\n)\\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\\norchestrator_worker_builder.add_edge(\"synthesizer\", END)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Compile the workflow\\norchestrator_worker = orchestrator_worker_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\\n\\nfrom IPython.display import Markdown\\nMarkdown(state[\"final_report\"])'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='from IPython.display import Markdown\\nMarkdown(state[\"final_report\"])\\n\\n\\u200bEvaluator-optimizer\\nIn evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a human-in-the-loop determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.\\nEvaluator-optimizer workflows are commonly used when there’s particular success criteria for a task, but iteration is required to meet that criteria. For example, there’s not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.\\n\\nGraph APIFunctional APICopyAsk AI# Graph state\\nclass State(TypedDict):\\n    joke: str\\n    topic: str\\n    feedback: str\\n    funny_or_not: str'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\ndef llm_call_generator(state: State):\\n    \"\"\"LLM generates a joke\"\"\"\\n\\n    if state.get(\"feedback\"):\\n        msg = llm.invoke(\\n            f\"Write a joke about {state[\\'topic\\']} but take into account the feedback: {state[\\'feedback\\']}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef llm_call_evaluator(state: State):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n\\n    grade = evaluator.invoke(f\"Grade the joke {state[\\'joke\\']}\")\\n    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\\ndef route_joke(state: State):\\n    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\\n\\n    if state[\"funny_or_not\"] == \"funny\":\\n        return \"Accepted\"\\n    elif state[\"funny_or_not\"] == \"not funny\":\\n        return \"Rejected + Feedback\"\\n\\n\\n# Build workflow\\noptimizer_builder = StateGraph(State)\\n\\n# Add the nodes\\noptimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\\noptimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\\n\\n# Add edges to connect nodes\\noptimizer_builder.add_edge(START, \"llm_call_generator\")\\noptimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\\noptimizer_builder.add_conditional_edges(\\n    \"llm_call_evaluator\",\\n    route_joke,\\n    {  # Name returned by route_joke : Name of next node to visit\\n        \"Accepted\": END,\\n        \"Rejected + Feedback\": \"llm_call_generator\",\\n    },\\n)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Compile the workflow\\noptimizer_workflow = optimizer_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = optimizer_workflow.invoke({\"topic\": \"Cats\"})\\nprint(state[\"joke\"])\\n\\n\\u200bAgents\\nAgents are typically implemented as an LLM performing actions using tools. They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.\\n\\nTo get started with agents, see the quickstart or read more about how they work in LangChain.\\nUsing toolsCopyAsk AIfrom langchain.tools import tool\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a * b'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nllm_with_tools = llm.bind_tools(tools)\\n\\nGraph APIFunctional APICopyAsk AIfrom langgraph.graph import MessagesState\\nfrom langchain.messages import SystemMessage, HumanMessage, ToolMessage\\n\\n\\n# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            llm_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ]\\n    }\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\\ndef should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"tool_node\"\\n\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n\\n# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"tool_node\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    [\"tool_node\", END]\\n)\\nagent_builder.add_edge(\"tool_node\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoThinking in LangGraphPreviousPersistenceNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content=\"Graph API overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationGraph APIGraph API overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIGraph APIUse the graph APIFunctional APIRuntimeEnglishcloseOn this pageGraphsStateGraphCompiling your graphStateSchemaMultiple schemasReducersDefault ReducerOverwriteWorking with Messages in Graph StateWhy use messages?Using Messages in your GraphSerializationMessagesStateNodesSTART NodeEND NodeNode CachingEdgesNormal EdgesConditional\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='your GraphSerializationMessagesStateNodesSTART NodeEND NodeNode CachingEdgesNormal EdgesConditional EdgesEntry PointConditional Entry PointSendCommandWhen should I use Command instead of conditional edges?Navigating to a node in a parent graphUsing inside toolsHuman-in-the-loopGraph MigrationsRuntime ContextRecursion LimitAccessing and handling the recursion counterHow it worksAccessing the current step counterProactive recursion handlingProactive vs reactive approachesOther available metadataVisualizationLangGraph APIsGraph APIGraph API overviewCopy pageCopy page\\u200bGraphs'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='State: A shared data structure that represents the current snapshot of your application. It can be any data type, but is typically defined using a shared state schema.\\n\\n\\nNodes: Functions that encode the logic of your agents. They receive the current state as input, perform some computation or side-effect, and return an updated state.\\n\\n\\nEdges: Functions that determine which Node to execute next based on the current state. They can be conditional branches or fixed transitions.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='By composing Nodes and Edges, you can create complex, looping workflows that evolve the state over time. The real power, though, comes from how LangGraph manages that state. To emphasize: Nodes and Edges are nothing more than functions - they can contain an LLM or just good ol’ code.\\nIn short: nodes do the work, edges tell what to do next.\\nLangGraph’s underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google’s Pregel system, the program proceeds in discrete “super-steps.”'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or “channels”). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.\\n\\u200bStateGraph\\nThe StateGraph class is the main graph class to use. This is parameterized by a user defined State object.\\n\\u200bCompiling your graph\\nTo build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed?'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method:\\nCopyAsk AIgraph = graph_builder.compile(...)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='You MUST compile your graph before you can use it.\\n\\u200bState\\nThe first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.\\n\\u200bSchema\\nThe main documented way to specify the schema of a graph is by using a TypedDict. If you want to provide default values in your state, use a dataclass. We also support using a Pydantic BaseModel as your graph state if you want recursive data validation (though note that pydantic is less performant than a TypedDict or dataclass).'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the guide here for how to use.\\n\\u200bMultiple schemas\\nTypically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Internal nodes can pass information that is not required in the graph’s input / output.\\nWe may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key.\\n\\nIt is possible to have nodes write to private state channels inside the graph for internal node communication. We can simply define a private schema, PrivateState.\\nIt is also possible to define explicit input and output schemas for a graph. In these cases, we define an “internal” schema that contains all keys relevant to graph operations. But, we also define input and output schemas that are sub-sets of the “internal” schema to constrain the input and output of the graph. See this guide for more detail.\\nLet’s look at an example:\\nCopyAsk AIclass InputState(TypedDict):\\n    user_input: str\\n\\nclass OutputState(TypedDict):\\n    graph_output: str\\n\\nclass OverallState(TypedDict):\\n    foo: str\\n    user_input: str\\n    graph_output: str'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='class OverallState(TypedDict):\\n    foo: str\\n    user_input: str\\n    graph_output: str\\n\\nclass PrivateState(TypedDict):\\n    bar: str\\n\\ndef node_1(state: InputState) -> OverallState:\\n    # Write to OverallState\\n    return {\"foo\": state[\"user_input\"] + \" name\"}\\n\\ndef node_2(state: OverallState) -> PrivateState:\\n    # Read from OverallState, write to PrivateState\\n    return {\"bar\": state[\"foo\"] + \" is\"}\\n\\ndef node_3(state: PrivateState) -> OutputState:\\n    # Read from PrivateState, write to OutputState\\n    return {\"graph_output\": state[\"bar\"] + \" Lance\"}\\n\\nbuilder = StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)\\nbuilder.add_node(\"node_1\", node_1)\\nbuilder.add_node(\"node_2\", node_2)\\nbuilder.add_node(\"node_3\", node_3)\\nbuilder.add_edge(START, \"node_1\")\\nbuilder.add_edge(\"node_1\", \"node_2\")\\nbuilder.add_edge(\"node_2\", \"node_3\")\\nbuilder.add_edge(\"node_3\", END)\\n\\ngraph = builder.compile()\\ngraph.invoke({\"user_input\":\"My\"})\\n# {\\'graph_output\\': \\'My name is Lance\\'}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='graph = builder.compile()\\ngraph.invoke({\"user_input\":\"My\"})\\n# {\\'graph_output\\': \\'My name is Lance\\'}\\n\\nThere are two subtle and important points to note here:\\n\\n\\nWe pass state: InputState as the input schema to node_1. But, we write out to foo, a channel in OverallState. How can we write out to a state channel that is not included in the input schema? This is because a node can write to any state channel in the graph state. The graph state is the union of the state channels defined at initialization, which includes OverallState and the filters InputState and OutputState.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='We initialize the graph with StateGraph(OverallState,input_schema=InputState,output_schema=OutputState). So, how can we write to PrivateState in node_2? How does the graph gain access to this schema if it was not passed in the StateGraph initialization? We can do this because nodes can also declare additional state channels as long as the state schema definition exists. In this case, the PrivateState schema is defined, so we can add bar as a new state channel in the graph and write to it.\\n\\n\\n\\u200bReducers\\nReducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer:\\n\\u200bDefault Reducer\\nThese two examples show how to use the default reducer:\\nExample A:\\nCopyAsk AIfrom typing_extensions import TypedDict'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='class State(TypedDict):\\n    foo: int\\n    bar: list[str]\\n\\nIn this example, no reducer functions are specified for any key. Let’s assume the input to the graph is:\\n{\"foo\": 1, \"bar\": [\"hi\"]}. Let’s then assume the first Node returns {\"foo\": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {\"foo\": 2, \"bar\": [\"hi\"]}. If the second node returns {\"bar\": [\"bye\"]} then the State would then be {\"foo\": 2, \"bar\": [\"bye\"]}\\nExample B:\\nCopyAsk AIfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\nfrom operator import add\\n\\nclass State(TypedDict):\\n    foo: int\\n    bar: Annotated[list[str], add]'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='In this example, we’ve used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let’s assume the input to the graph is {\"foo\": 1, \"bar\": [\"hi\"]}. Let’s then assume the first Node returns {\"foo\": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {\"foo\": 2, \"bar\": [\"hi\"]}. If the second node returns {\"bar\": [\"bye\"]} then the State would then be {\"foo\": 2, \"bar\": [\"hi\", \"bye\"]}. Notice here that the bar key is updated by adding the two lists together.\\n\\u200bOverwrite\\nIn some cases, you may want to bypass a reducer and directly overwrite a state value. LangGraph provides the Overwrite type for this purpose. Learn how to use Overwrite here.\\n\\u200bWorking with Messages in Graph State\\n\\u200bWhy use messages?'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bWorking with Messages in Graph State\\n\\u200bWhy use messages?\\nMost modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain’s ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide.\\n\\u200bUsing Messages in your Graph'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bUsing Messages in your Graph\\nIn many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don’t specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.\\n\\u200bSerialization\\nIn addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format:\\nCopyAsk AI# this is supported'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='CopyAsk AI# this is supported\\n{\"messages\": [HumanMessage(content=\"message\")]}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='# and this is also supported\\n{\"messages\": [{\"type\": \"human\", \"content\": \"message\"}]}\\n\\nSince the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[\"messages\"][-1].content. Below is an example of a graph that uses add_messages as its reducer function.\\nCopyAsk AIfrom langchain.messages import AnyMessage\\nfrom langgraph.graph.message import add_messages\\nfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\n\\nclass GraphState(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='class GraphState(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]\\n\\n\\u200bMessagesState\\nSince having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:\\nCopyAsk AIfrom langgraph.graph import MessagesState\\n\\nclass State(MessagesState):\\n    documents: list[str]\\n\\n\\u200bNodes\\nIn LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments:\\n\\nstate: The state of the graph\\nconfig: A RunnableConfig object that contains configuration information like thread_id and tracing information like tags\\nruntime: A Runtime object that contains runtime context and other information like store and stream_writer'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Similar to NetworkX, you add these nodes to a graph using the add_node method:\\nCopyAsk AIfrom dataclasses import dataclass\\nfrom typing_extensions import TypedDict\\n\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph\\nfrom langgraph.runtime import Runtime\\n\\nclass State(TypedDict):\\n    input: str\\n    results: str\\n\\n@dataclass\\nclass Context:\\n    user_id: str\\n\\nbuilder = StateGraph(State)\\n\\ndef plain_node(state: State):\\n    return state\\n\\ndef node_with_runtime(state: State, runtime: Runtime[Context]):\\n    print(\"In node: \", runtime.context.user_id)\\n    return {\"results\": f\"Hello, {state[\\'input\\']}!\"}\\n\\ndef node_with_config(state: State, config: RunnableConfig):\\n    print(\"In node with thread_id: \", config[\"configurable\"][\"thread_id\"])\\n    return {\"results\": f\"Hello, {state[\\'input\\']}!\"}\\n\\n\\nbuilder.add_node(\"plain_node\", plain_node)\\nbuilder.add_node(\"node_with_runtime\", node_with_runtime)\\nbuilder.add_node(\"node_with_config\", node_with_config)\\n...'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Behind the scenes, functions are converted to RunnableLambdas, which add batch and async support to your function, along with native tracing and debugging.\\nIf you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name.\\nCopyAsk AIbuilder.add_node(my_node)\\n# You can then create edges to/from this node by referencing it as `\"my_node\"`\\n\\n\\u200bSTART Node\\nThe START Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.\\nCopyAsk AIfrom langgraph.graph import START\\n\\ngraph.add_edge(START, \"node_a\")\\n\\n\\u200bEND Node\\nThe END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.\\nCopyAsk AIfrom langgraph.graph import END\\n\\ngraph.add_edge(\"node_a\", END)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='graph.add_edge(\"node_a\", END)\\n\\n\\u200bNode Caching\\nLangGraph supports caching of tasks/nodes based on the input to the node. To use caching:\\n\\nSpecify a cache when compiling a graph (or specifying an entrypoint)\\nSpecify a cache policy for nodes. Each cache policy supports:\\n\\nkey_func used to generate a cache key based on the input to a node, which defaults to a hash of the input with pickle.\\nttl, the time to live for the cache in seconds. If not specified, the cache will never expire.\\n\\n\\n\\nFor example:\\nCopyAsk AIimport time\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph\\nfrom langgraph.cache.memory import InMemoryCache\\nfrom langgraph.types import CachePolicy\\n\\n\\nclass State(TypedDict):\\n    x: int\\n    result: int\\n\\n\\nbuilder = StateGraph(State)\\n\\n\\ndef expensive_node(state: State) -> dict[str, int]:\\n    # expensive computation\\n    time.sleep(2)\\n    return {\"result\": state[\"x\"] * 2}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='builder.add_node(\"expensive_node\", expensive_node, cache_policy=CachePolicy(ttl=3))\\nbuilder.set_entry_point(\"expensive_node\")\\nbuilder.set_finish_point(\"expensive_node\")\\n\\ngraph = builder.compile(cache=InMemoryCache())\\n\\nprint(graph.invoke({\"x\": 5}, stream_mode=\\'updates\\'))    \\n# [{\\'expensive_node\\': {\\'result\\': 10}}]\\nprint(graph.invoke({\"x\": 5}, stream_mode=\\'updates\\'))    \\n# [{\\'expensive_node\\': {\\'result\\': 10}, \\'__metadata__\\': {\\'cached\\': True}}]\\n\\n\\nFirst run takes two seconds to run (due to mocked expensive computation).\\nSecond run utilizes cache and returns quickly.\\n\\n\\u200bEdges\\nEdges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Normal Edges: Go directly from one node to the next.\\nConditional Edges: Call a function to determine which node(s) to go to next.\\nEntry Point: Which node to call first when user input arrives.\\nConditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.\\n\\nA node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.\\n\\u200bNormal Edges\\nIf you always want to go from node A to node B, you can use the add_edge method directly.\\nCopyAsk AIgraph.add_edge(\"node_a\", \"node_b\")\\n\\n\\u200bConditional Edges\\nIf you want to optionally route to one or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a “routing function” to call after that node is executed:\\nCopyAsk AIgraph.add_conditional_edges(\"node_a\", routing_function)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Similar to nodes, the routing_function accepts the current state of the graph and returns a value.\\nBy default, the return value routing_function is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.\\nYou can optionally provide a dictionary that maps the routing_function’s output to the name of the next node.\\nCopyAsk AIgraph.add_conditional_edges(\"node_a\", routing_function, {True: \"node_b\", False: \"node_c\"})\\n\\nUse Command instead of conditional edges if you want to combine state updates and routing in a single function.\\n\\u200bEntry Point\\nThe entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph.\\nCopyAsk AIfrom langgraph.graph import START\\n\\ngraph.add_edge(START, \"node_a\")'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='graph.add_edge(START, \"node_a\")\\n\\n\\u200bConditional Entry Point\\nA conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this.\\nCopyAsk AIfrom langgraph.graph import START\\n\\ngraph.add_conditional_edges(START, routing_function)\\n\\nYou can optionally provide a dictionary that maps the routing_function’s output to the name of the next node.\\nCopyAsk AIgraph.add_conditional_edges(START, routing_function, {True: \"node_b\", False: \"node_c\"})'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bSend\\nBy default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object).\\nTo support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node.\\nCopyAsk AIdef continue_to_jokes(state: OverallState):\\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\\'subjects\\']]'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='graph.add_conditional_edges(\"node_a\", continue_to_jokes)\\n\\n\\u200bCommand\\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:\\nCopyAsk AIdef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        # state update\\n        update={\"foo\": \"bar\"},\\n        # control flow\\n        goto=\"my_other_node\"\\n    )\\n\\nWith Command you can also achieve dynamic control flow behavior (identical to conditional edges):\\nCopyAsk AIdef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    if state[\"foo\"] == \"bar\":\\n        return Command(update={\"foo\": \"baz\"}, goto=\"my_other_node\")'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='When returning Command in your node functions, you must add return type annotations with the list of node names the node is routing to, e.g. Command[Literal[\"my_other_node\"]]. This is necessary for the graph rendering and tells LangGraph that my_node can navigate to my_other_node.\\nCheck out this how-to guide for an end-to-end example of how to use Command.\\n\\u200bWhen should I use Command instead of conditional edges?\\n\\nUse Command when you need to both update the graph state and route to a different node. For example, when implementing multi-agent handoffs where it’s important to route to a different agent and pass some information to that agent.\\nUse conditional edges to route between nodes conditionally without updating the state.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bNavigating to a node in a parent graph\\nIf you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify graph=Command.PARENT in Command:\\nCopyAsk AIdef my_node(state: State) -> Command[Literal[\"other_subgraph\"]]:\\n    return Command(\\n        update={\"foo\": \"bar\"},\\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\\n        graph=Command.PARENT\\n    )'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Setting graph to Command.PARENT will navigate to the closest parent graph.When you send updates from a subgraph node to a parent graph node for a key that’s shared by both parent and subgraph state schemas, you must define a reducer for the key you’re updating in the parent graph state. See this example.\\nThis is particularly useful when implementing multi-agent handoffs.\\nCheck out this guide for detail.\\n\\u200bUsing inside tools\\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation.\\nRefer to this guide for detail.\\n\\u200bHuman-in-the-loop\\nCommand is an important part of human-in-the-loop workflows: when using interrupt() to collect user input, Command is then used to supply the input and resume execution via Command(resume=\"User input\"). Check out this conceptual guide for more information.\\n\\u200bGraph Migrations'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bGraph Migrations\\nLangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)\\nFor threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) — if this is a blocker please reach out and we can prioritize a solution.\\nFor modifying state, we have full backwards and forwards compatibility for adding and removing keys\\nState keys that are renamed lose their saved state in existing threads\\nState keys whose types change in incompatible ways could currently cause issues in threads with state from before the change — if this is a blocker please reach out and we can prioritize a solution.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bRuntime Context\\nWhen creating a graph, you can specify a context_schema for runtime context passed to nodes. This is useful for passing\\ninformation to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection.\\nCopyAsk AI@dataclass\\nclass ContextSchema:\\n    llm_provider: str = \"openai\"\\n\\ngraph = StateGraph(State, context_schema=ContextSchema)\\n\\nYou can then pass this context into the graph using the context parameter of the invoke method.\\nCopyAsk AIgraph.invoke(inputs, context={\"llm_provider\": \"anthropic\"})\\n\\nYou can then access and use this context inside a node or conditional edge:\\nCopyAsk AIfrom langgraph.runtime import Runtime\\n\\ndef node_a(state: State, runtime: Runtime[ContextSchema]):\\n    llm = get_llm(runtime.context.llm_provider)\\n    # ...'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='See this guide for a full breakdown on configuration.\\n\\u200bRecursion Limit\\nThe recursion limit sets the maximum number of super-steps the graph can execute during a single execution. Once the limit is reached, LangGraph will raise GraphRecursionError. By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to invoke/stream via the config dictionary. Importantly, recursion_limit is a standalone config key and should not be passed inside the configurable key as all other user-defined configuration. See the example below:\\nCopyAsk AIgraph.invoke(inputs, config={\"recursion_limit\": 5}, context={\"llm\": \"anthropic\"})'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Read this how-to to learn more about how the recursion limit works.\\n\\u200bAccessing and handling the recursion counter\\nThe current step counter is accessible in config[\"metadata\"][\"langgraph_step\"] within any node, allowing for proactive recursion handling before hitting the recursion limit. This enables you to implement graceful degradation strategies within your graph logic.\\n\\u200bHow it works\\nThe step counter is stored in config[\"metadata\"][\"langgraph_step\"]. The recursion limit check follows the logic: step > stop where stop = step + recursion_limit + 1. When the limit is exceeded, LangGraph raises a GraphRecursionError.\\n\\u200bAccessing the current step counter\\nYou can access the current step counter within any node to monitor execution progress.\\nCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='def my_node(state: dict, config: RunnableConfig) -> dict:\\n    current_step = config[\"metadata\"][\"langgraph_step\"]\\n    print(f\"Currently on step: {current_step}\")\\n    return state\\n\\n\\u200bProactive recursion handling\\nYou can check the step counter and proactively route to a different node before hitting the limit. This allows for graceful degradation within your graph.\\nCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph, END\\n\\ndef reasoning_node(state: dict, config: RunnableConfig) -> dict:\\n    current_step = config[\"metadata\"][\"langgraph_step\"]\\n    recursion_limit = config[\"recursion_limit\"]  # always present, defaults to 25\\n\\n    # Check if we\\'re approaching the limit (e.g., 80% threshold)\\n    if current_step >= recursion_limit * 0.8:\\n        return {\\n            **state,\\n            \"route_to\": \"fallback\",\\n            \"reason\": \"Approaching recursion limit\"\\n        }'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='# Normal processing\\n    return {\"messages\": state[\"messages\"] + [\"thinking...\"]}\\n\\ndef fallback_node(state: dict, config: RunnableConfig) -> dict:\\n    \"\"\"Handle cases where recursion limit is approaching\"\"\"\\n    return {\\n        **state,\\n        \"messages\": state[\"messages\"] + [\"Reached complexity limit, providing best effort answer\"]\\n    }\\n\\ndef route_based_on_state(state: dict) -> str:\\n    if state.get(\"route_to\") == \"fallback\":\\n        return \"fallback\"\\n    elif state.get(\"done\"):\\n        return END\\n    return \"reasoning\"\\n\\n# Build graph\\ngraph = StateGraph(dict)\\ngraph.add_node(\"reasoning\", reasoning_node)\\ngraph.add_node(\"fallback\", fallback_node)\\ngraph.add_conditional_edges(\"reasoning\", route_based_on_state)\\ngraph.add_edge(\"fallback\", END)\\ngraph.set_entry_point(\"reasoning\")\\n\\napp = graph.compile()'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='app = graph.compile()\\n\\n\\u200bProactive vs reactive approaches\\nThere are two main approaches to handling recursion limits: proactive (monitoring within the graph) and reactive (catching errors externally).\\nCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph, END\\nfrom langgraph.errors import GraphRecursionError\\n\\n# Proactive Approach (recommended)\\ndef agent_with_monitoring(state: dict, config: RunnableConfig) -> dict:\\n    \"\"\"Proactively monitor and handle recursion within the graph\"\"\"\\n    current_step = config[\"metadata\"][\"langgraph_step\"]\\n    recursion_limit = config[\"recursion_limit\"]\\n\\n    # Early detection - route to internal handling\\n    if current_step >= recursion_limit - 2:  # 2 steps before limit\\n        return {\\n            **state,\\n            \"status\": \"recursion_limit_approaching\",\\n            \"final_answer\": \"Reached iteration limit, returning partial result\"\\n        }'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='# Normal processing\\n    return {\"messages\": state[\"messages\"] + [f\"Step {current_step}\"]}\\n\\n# Reactive Approach (fallback)\\ntry:\\n    result = graph.invoke(initial_state, {\"recursion_limit\": 10})\\nexcept GraphRecursionError as e:\\n    # Handle externally after graph execution fails\\n    result = fallback_handler(initial_state)\\n\\nThe key differences between these approaches are:\\n\\nApproachDetectionHandlingControl Flow\\nProactive (using langgraph_step)Before limit reachedInside graph via conditional routingGraph continues to completion nodeReactive (catching GraphRecursionError)After limit exceededOutside graph in try/catchGraph execution terminated\\n\\nProactive advantages:\\n\\nGraceful degradation within the graph\\nCan save intermediate state in checkpoints\\nBetter user experience with partial results\\nGraph completes normally (no exception)\\n\\nReactive advantages:\\n\\nSimpler implementation\\nNo need to modify graph logic\\nCentralized error handling'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Simpler implementation\\nNo need to modify graph logic\\nCentralized error handling\\n\\n\\u200bOther available metadata\\nAlong with langgraph_step, the following metadata is also available in config[\"metadata\"]:\\nCopyAsk AIdef inspect_metadata(state: dict, config: RunnableConfig) -> dict:\\n    metadata = config[\"metadata\"]\\n\\n    print(f\"Step: {metadata[\\'langgraph_step\\']}\")\\n    print(f\"Node: {metadata[\\'langgraph_node\\']}\")\\n    print(f\"Triggers: {metadata[\\'langgraph_triggers\\']}\")\\n    print(f\"Path: {metadata[\\'langgraph_path\\']}\")\\n    print(f\"Checkpoint NS: {metadata[\\'langgraph_checkpoint_ns\\']}\")\\n\\n    return state\\n\\n\\u200bVisualization\\nIt’s often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Edit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoObservabilityPreviousUse the graph APINext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content=\"Functional API overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationFunctional APIFunctional API overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIFunctional APIUse the Functional APIRuntimeEnglishcloseOn this pageFunctional API vs. Graph APIExampleEntrypointDefinitionInjectable parametersExecutingResumingShort-term memoryentrypoint.finalTaskDefinitionExecutionWhen to use a taskSerializationDeterminismIdempotencyCommon PitfallsHandling side\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='to use a taskSerializationDeterminismIdempotencyCommon PitfallsHandling side effectsNon-deterministic control flowLangGraph APIsFunctional APIFunctional API overviewCopy pageCopy pageThe Functional API allows you to add LangGraph’s key features — persistence, memory, human-in-the-loop, and streaming — to your applications with minimal changes to your existing code.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='It is designed to integrate these features into existing code that may use standard language primitives for branching and control flow, such as if statements, for loops, and function calls. Unlike many data orchestration frameworks that require restructuring code into an explicit pipeline or DAG, the Functional API allows you to incorporate these capabilities without enforcing a rigid execution model.\\nThe Functional API uses two key building blocks:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='@entrypoint – Marks a function as the starting point of a workflow, encapsulating logic and managing execution flow, including handling long-running tasks and interrupts.\\n@task – Represents a discrete unit of work, such as an API call or data processing step, that can be executed asynchronously within an entrypoint. Tasks return a future-like object that can be awaited or resolved synchronously.\\n\\nThis provides a minimal abstraction for building workflows with state management and streaming.\\nFor information on how to use the functional API, see Use Functional API.\\n\\u200bFunctional API vs. Graph API\\nFor users who prefer a more declarative approach, LangGraph’s Graph API allows you to define workflows using a Graph paradigm. Both APIs share the same underlying runtime, so you can use them together in the same application.\\nHere are some key differences:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Control flow: The Functional API does not require thinking about graph structure. You can use standard Python constructs to define workflows. This will usually trim the amount of code you need to write.\\nShort-term memory: The GraphAPI requires declaring a State and may require defining reducers to manage updates to the graph state. @entrypoint and @tasks do not require explicit state management as their state is scoped to the function and is not shared across functions.\\nCheckpointing: Both APIs generate and use checkpoints. In the Graph API a new checkpoint is generated after every superstep. In the Functional API, when tasks are executed, their results are saved to an existing checkpoint associated with the given entrypoint instead of creating a new checkpoint.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Visualization: The Graph API makes it easy to visualize the workflow as a graph which can be useful for debugging, understanding the workflow, and sharing with others. The Functional API does not support visualization as the graph is dynamically generated during runtime.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bExample\\nBelow we demonstrate a simple application that writes an essay and interrupts to request human review.\\nCopyAsk AIfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langgraph.func import entrypoint, task\\nfrom langgraph.types import interrupt\\n\\n@task\\ndef write_essay(topic: str) -> str:\\n    \"\"\"Write an essay about the given topic.\"\"\"\\n    time.sleep(1) # A placeholder for a long-running task.\\n    return f\"An essay about topic: {topic}\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='@entrypoint(checkpointer=InMemorySaver())\\ndef workflow(topic: str) -> dict:\\n    \"\"\"A simple workflow that writes an essay and asks for a review.\"\"\"\\n    essay = write_essay(\"cat\").result()\\n    is_approved = interrupt({\\n        # Any json-serializable payload provided to interrupt as argument.\\n        # It will be surfaced on the client side as an Interrupt when streaming data\\n        # from the workflow.\\n        \"essay\": essay, # The essay we want reviewed.\\n        # We can add any additional information that we need.\\n        # For example, introduce a key called \"action\" with some instructions.\\n        \"action\": \"Please approve/reject the essay\",\\n    })\\n\\n    return {\\n        \"essay\": essay, # The essay that was generated\\n        \"is_approved\": is_approved, # Response from HIL\\n    }'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Detailed ExplanationThis workflow will write an essay about the topic “cat” and then pause to get a review from a human. The workflow can be interrupted for an indefinite amount of time until a review is provided.When the workflow is resumed, it executes from the very start, but because the result of the writeEssay task was already saved, the task result will be loaded from the checkpoint instead of being recomputed.CopyAsk AIimport time\\nimport uuid\\nfrom langgraph.func import entrypoint, task\\nfrom langgraph.types import interrupt\\nfrom langgraph.checkpoint.memory import InMemorySaver\\n\\n\\n@task\\ndef write_essay(topic: str) -> str:\\n    \"\"\"Write an essay about the given topic.\"\"\"\\n    time.sleep(1)  # This is a placeholder for a long-running task.\\n    return f\"An essay about topic: {topic}\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='@entrypoint(checkpointer=InMemorySaver())\\ndef workflow(topic: str) -> dict:\\n    \"\"\"A simple workflow that writes an essay and asks for a review.\"\"\"\\n    essay = write_essay(\"cat\").result()\\n    is_approved = interrupt(\\n        {\\n            # Any json-serializable payload provided to interrupt as argument.\\n            # It will be surfaced on the client side as an Interrupt when streaming data\\n            # from the workflow.\\n            \"essay\": essay,  # The essay we want reviewed.\\n            # We can add any additional information that we need.\\n            # For example, introduce a key called \"action\" with some instructions.\\n            \"action\": \"Please approve/reject the essay\",\\n        }\\n    )\\n    return {\\n        \"essay\": essay,  # The essay that was generated\\n        \"is_approved\": is_approved,  # Response from HIL\\n    }'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='thread_id = str(uuid.uuid4())\\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\\nfor item in workflow.stream(\"cat\", config):\\n    print(item)\\n# > {\\'write_essay\\': \\'An essay about topic: cat\\'}\\n# > {\\n# >     \\'__interrupt__\\': (\\n# >        Interrupt(\\n# >            value={\\n# >                \\'essay\\': \\'An essay about topic: cat\\',\\n# >                \\'action\\': \\'Please approve/reject the essay\\'\\n# >            },\\n# >            id=\\'b9b2b9d788f482663ced6dc755c9e981\\'\\n# >        ),\\n# >    )\\n# > }\\nAn essay has been written and is ready for review. Once the review is provided, we can resume the workflow:CopyAsk AIfrom langgraph.types import Command\\n\\n# Get review from a user (e.g., via a UI)\\n# In this case, we\\'re using a bool, but this can be any json-serializable value.\\nhuman_review = True'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content=\"for item in workflow.stream(Command(resume=human_review), config):\\n    print(item)\\nCopyAsk AI{'workflow': {'essay': 'An essay about topic: cat', 'is_approved': False}}\\nThe workflow has been completed and the review has been added to the essay.\\n\\u200bEntrypoint\\nThe @entrypoint decorator can be used to create a workflow from a function. It encapsulates workflow logic and manages execution flow, including handling long-running tasks and interrupts.\\n\\u200bDefinition\\nAn entrypoint is defined by decorating a function with the @entrypoint decorator.\\nThe function must accept a single positional argument, which serves as the workflow input. If you need to pass multiple pieces of data, use a dictionary as the input type for the first argument.\\nDecorating a function with an entrypoint produces a Pregel instance which helps to manage the execution of the workflow (e.g., handles streaming, resumption, and checkpointing).\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='You will usually want to pass a checkpointer to the @entrypoint decorator to enable persistence and use features like human-in-the-loop.\\n Sync AsyncCopyAsk AIfrom langgraph.func import entrypoint'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(some_input: dict) -> int:\\n    # some logic that may involve long-running tasks like API calls,\\n    # and may be interrupted for human-in-the-loop.\\n    ...\\n    return result'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Serialization\\nThe inputs and outputs of entrypoints must be JSON-serializable to support checkpointing. Please see the serialization section for more details.\\n\\u200bInjectable parameters\\nWhen declaring an entrypoint, you can request access to additional parameters that will be injected automatically at run time. These parameters include:\\nParameterDescriptionpreviousAccess the state associated with the previous checkpoint for the given thread. See short-term-memory.storeAn instance of [BaseStore][langgraph.store.base.BaseStore]. Useful for long-term memory.writerUse to access the StreamWriter when working with Async Python < 3.11. See streaming with functional API for details.configFor accessing run time configuration. See RunnableConfig for information.\\nDeclare the parameters with the appropriate name and type annotation.\\nRequesting Injectable ParametersCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.func import entrypoint'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='from langgraph.func import entrypoint\\nfrom langgraph.store.base import BaseStore\\nfrom langgraph.store.memory import InMemoryStore'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='in_memory_store = InMemoryStore(...)  # An instance of InMemoryStore for long-term memory\\n\\n@entrypoint(\\n    checkpointer=checkpointer,  # Specify the checkpointer\\n    store=in_memory_store  # Specify the store\\n)\\ndef my_workflow(\\n    some_input: dict,  # The input (e.g., passed via `invoke`)\\n    *,\\n    previous: Any = None, # For short-term memory\\n    store: BaseStore,  # For long-term memory\\n    writer: StreamWriter,  # For streaming custom data\\n    config: RunnableConfig  # For accessing the configuration passed to the entrypoint\\n) -> ...:\\n\\n\\u200bExecuting\\nUsing the @entrypoint yields a Pregel object that can be executed using the invoke, ainvoke, stream, and astream methods.\\n Invoke Async Invoke Stream Async StreamCopyAsk AIconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\nmy_workflow.invoke(some_input, config)  # Wait for the result synchronously'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bResuming\\nResuming an execution after an interrupt can be done by passing a resume value to the Command primitive.\\n Invoke Async Invoke Stream Async StreamCopyAsk AIfrom langgraph.types import Command\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\n\\nmy_workflow.invoke(Command(resume=some_resume_value), config)\\n\\nResuming after an error\\nTo resume after an error, run the entrypoint with a None and the same thread id (config).\\nThis assumes that the underlying error has been resolved and execution can proceed successfully.\\n Invoke Async Invoke Stream Async StreamCopyAsk AI\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\n\\nmy_workflow.invoke(None, config)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='my_workflow.invoke(None, config)\\n\\n\\u200bShort-term memory\\nWhen an entrypoint is defined with a checkpointer, it stores information between successive invocations on the same thread id in checkpoints.\\nThis allows accessing the state from the previous invocation using the previous parameter.\\nBy default, the previous parameter is the return value of the previous invocation.\\nCopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(number: int, *, previous: Any = None) -> int:\\n    previous = previous or 0\\n    return number + previous\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\n\\nmy_workflow.invoke(1, config)  # 1 (previous was None)\\nmy_workflow.invoke(2, config)  # 3 (previous was 1 from the previous invocation)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bentrypoint.final\\nentrypoint.final is a special primitive that can be returned from an entrypoint and allows decoupling the value that is saved in the checkpoint from the return value of the entrypoint.\\nThe first value is the return value of the entrypoint, and the second value is the value that will be saved in the checkpoint. The type annotation is entrypoint.final[return_type, save_type].\\nCopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(number: int, *, previous: Any = None) -> entrypoint.final[int, int]:\\n    previous = previous or 0\\n    # This will return the previous value to the caller, saving\\n    # 2 * number to the checkpoint, which will be used in the next invocation\\n    # for the `previous` parameter.\\n    return entrypoint.final(value=previous, save=2 * number)\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"1\"\\n    }\\n}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='config = {\\n    \"configurable\": {\\n        \"thread_id\": \"1\"\\n    }\\n}\\n\\nmy_workflow.invoke(3, config)  # 0 (previous was None)\\nmy_workflow.invoke(1, config)  # 6 (previous was 3 * 2 from the previous invocation)\\n\\n\\u200bTask\\nA task represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics:\\n\\nAsynchronous Execution: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking.\\nCheckpointing: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See persistence for more details).\\n\\n\\u200bDefinition\\nTasks are defined using the @task decorator, which wraps a regular Python function.\\nCopyAsk AIfrom langgraph.func import task\\n\\n@task()\\ndef slow_computation(input_value):\\n    # Simulate a long-running operation\\n    ...\\n    return result'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Serialization\\nThe outputs of tasks must be JSON-serializable to support checkpointing.\\n\\u200bExecution\\nTasks can only be called from within an entrypoint, another task, or a state graph node.\\nTasks cannot be called directly from the main application code.\\nWhen you call a task, it returns immediately with a future object. A future is a placeholder for a result that will be available later.\\nTo obtain the result of a task, you can either wait for it synchronously (using result()) or await it asynchronously (using await).\\n Synchronous Invocation Asynchronous InvocationCopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(some_input: int) -> int:\\n    future = slow_computation(some_input)\\n    return future.result()  # Wait for the result synchronously\\n\\n\\u200bWhen to use a task\\nTasks are useful in the following scenarios:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bWhen to use a task\\nTasks are useful in the following scenarios:\\n\\nCheckpointing: When you need to save the result of a long-running operation to a checkpoint, so you don’t need to recompute it when resuming the workflow.\\nHuman-in-the-loop: If you’re building a workflow that requires human intervention, you MUST use tasks to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the determinism section for more details.\\nParallel Execution: For I/O-bound tasks, tasks enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs).\\nObservability: Wrapping operations in tasks provides a way to track the progress of the workflow and monitor the execution of individual operations using LangSmith.\\nRetryable Work: When work needs to be retried to handle failures or inconsistencies, tasks provide a way to encapsulate and manage the retry logic.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bSerialization\\nThere are two key aspects to serialization in LangGraph:\\n\\nentrypoint inputs and outputs must be JSON-serializable.\\ntask outputs must be JSON-serializable.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='These requirements are necessary for enabling checkpointing and workflow resumption. Use python primitives like dictionaries, lists, strings, numbers, and booleans to ensure that your inputs and outputs are serializable.\\nSerialization ensures that workflow state, such as task results and intermediate values, can be reliably saved and restored. This is critical for enabling human-in-the-loop interactions, fault tolerance, and parallel execution.\\nProviding non-serializable inputs or outputs will result in a runtime error when a workflow is configured with a checkpointer.\\n\\u200bDeterminism\\nTo utilize features like human-in-the-loop, any randomness should be encapsulated inside of tasks. This guarantees that when execution is halted (e.g., for human in the loop) and then resumed, it will follow the same sequence of steps, even if task results are non-deterministic.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='LangGraph achieves this behavior by persisting task and subgraph results as they execute. A well-designed workflow ensures that resuming execution follows the same sequence of steps, allowing previously computed results to be retrieved correctly without having to re-execute them. This is particularly useful for long-running tasks or tasks with non-deterministic results, as it avoids repeating previously done work and allows resuming from essentially the same.\\nWhile different runs of a workflow can produce different results, resuming a specific run should always follow the same sequence of recorded steps. This allows LangGraph to efficiently look up task and subgraph results that were executed prior to the graph being interrupted and avoid recomputing them.\\n\\u200bIdempotency'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bIdempotency\\nIdempotency ensures that running the same operation multiple times produces the same result. This helps prevent duplicate API calls and redundant processing if a step is rerun due to a failure. Always place API calls inside tasks functions for checkpointing, and design them to be idempotent in case of re-execution. Re-execution can occur if a task starts, but does not complete successfully. Then, if the workflow is resumed, the task will run again. Use idempotency keys or verify existing results to avoid duplication.\\n\\u200bCommon Pitfalls\\n\\u200bHandling side effects\\nEncapsulate side effects (e.g., writing to a file, sending an email) in tasks to ensure they are not executed multiple times when resuming a workflow.\\n Incorrect CorrectIn this example, a side effect (writing to a file) is directly included in the workflow, so it will be executed a second time when resuming the workflow.CopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(inputs: dict) -> int:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='def my_workflow(inputs: dict) -> int:\\n    # This code will be executed a second time when resuming the workflow.\\n    # Which is likely not what you want.\\n    with open(\"output.txt\", \"w\") as f:  \\n        f.write(\"Side effect executed\")  \\n    value = interrupt(\"question\")\\n    return value'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bNon-deterministic control flow\\nOperations that might give different results each time (like getting current time or random numbers) should be encapsulated in tasks to ensure that on resume, the same result is returned.\\n\\nIn a task: Get random number (5) → interrupt → resume → (returns 5 again) → …\\nNot in a task: Get random number (5) → interrupt → resume → get new random number (7) → …'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='This is especially important when using human-in-the-loop workflows with multiple interrupts calls. LangGraph keeps a list of resume values for each task/entrypoint. When an interrupt is encountered, it’s matched with the corresponding resume value. This matching is strictly index-based, so the order of the resume values should match the order of the interrupts.\\nIf order of execution is not maintained when resuming, one interrupt call may be matched with the wrong resume value, leading to incorrect results.\\nPlease read the section on determinism for more details.\\n Incorrect CorrectIn this example, the workflow uses the current time to determine which task to execute. This is non-deterministic because the result of the workflow depends on the time at which it is executed.CopyAsk AIfrom langgraph.func import entrypoint\\n\\n@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(inputs: dict) -> int:\\n    t0 = inputs[\"t0\"]\\n    t1 = time.time()  \\n\\n    delta_t = t1 - t0'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='delta_t = t1 - t0\\n\\n    if delta_t > 1:\\n        result = slow_task(1).result()\\n        value = interrupt(\"question\")\\n    else:\\n        result = slow_task(2).result()\\n        value = interrupt(\"question\")\\n\\n    return {\\n        \"result\": result,\\n        \"value\": value\\n    }\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoUse the graph APIPreviousUse the functional APINext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content=\"LangGraph overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeEnglishcloseOn this page InstallCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageCopy pageLangGraph v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='Trusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b Install\\npipuvCopyAsk AIpip install -U langgraph'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='Then, create a simple hello world example:\\nCopyAsk AIfrom langgraph.graph import StateGraph, MessagesState, START, END\\n\\ndef mock_llm(state: MessagesState):\\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\\n\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(mock_llm)\\ngraph.add_edge(START, \"mock_llm\")\\ngraph.add_edge(\"mock_llm\", END)\\ngraph = graph.compile()\\n\\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\\n\\n\\u200bCore benefits\\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='Durable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\nHuman-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.\\nComprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\nDebugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\nProduction-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\\n\\n\\u200bLangGraph ecosystem\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='LangSmith — Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\\nLangSmith — Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in Studio.\\nLangChain - Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\\n\\n\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content=\"Edit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoWhat's new in v1Next⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content=\"Workflows and agents - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedWorkflows and agentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeEnglishcloseOn this pageSetupLLMs and augmentationsPrompt chainingParallelizationRoutingOrchestrator-workerCreating workers in LangGraphEvaluator-optimizerAgentsGet startedWorkflows and agentsCopy pageCopy pageThis guide reviews common workflow and agent patterns.\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Workflows have predetermined code paths and are designed to operate in a certain order.\\nAgents are dynamic and define their own processes and tool usage.\\n\\n\\nLangGraph offers several benefits when building agents and workflows, including persistence, streaming, and support for debugging as well as deployment.\\n\\u200bSetup\\nTo build a workflow or agent, you can use any chat model that supports structured outputs and tool calling. The following example uses Anthropic:\\n\\nInstall dependencies:\\n\\nCopyAsk AIpip install langchain_core langchain-anthropic langgraph\\n\\n\\nInitialize the LLM:\\n\\nCopyAsk AIimport os\\nimport getpass\\n\\nfrom langchain_anthropic import ChatAnthropic\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\\n\\n\\u200bLLMs and augmentations\\nWorkflows and agentic systems are based on LLMs and the various augmentations you add to them. Tool calling, structured outputs, and short term memory are a few options for tailoring LLMs to your needs.\\n\\nCopyAsk AI# Schema for structured output\\nfrom pydantic import BaseModel, Field\\n\\n\\nclass SearchQuery(BaseModel):\\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\\n    justification: str = Field(\\n        None, description=\"Why this query is relevant to the user\\'s request.\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nstructured_llm = llm.with_structured_output(SearchQuery)\\n\\n# Invoke the augmented LLM\\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\\n\\n# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b\\n\\n# Augment the LLM with tools\\nllm_with_tools = llm.bind_tools([multiply])\\n\\n# Invoke the LLM with input that triggers the tool call\\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\\n\\n# Get the tool call\\nmsg.tool_calls\\n\\n\\u200bPrompt chaining\\nPrompt chaining is when each LLM call processes the output of the previous call. It’s often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:\\n\\nTranslating documents into different languages\\nVerifying generated content for consistency\\n\\n\\nGraph APIFunctional APICopyAsk AIfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom IPython.display import Image, display\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    improved_joke: str\\n    final_joke: str\\n\\n\\n# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a short joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef check_punchline(state: State):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\\n        return \"Pass\"\\n    return \"Fail\"\\n\\n\\ndef improve_joke(state: State):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state[\\'joke\\']}\")\\n    return {\"improved_joke\": msg.content}\\n\\n\\ndef polish_joke(state: State):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state[\\'improved_joke\\']}\")\\n    return {\"final_joke\": msg.content}\\n\\n\\n# Build workflow\\nworkflow = StateGraph(State)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Build workflow\\nworkflow = StateGraph(State)\\n\\n# Add nodes\\nworkflow.add_node(\"generate_joke\", generate_joke)\\nworkflow.add_node(\"improve_joke\", improve_joke)\\nworkflow.add_node(\"polish_joke\", polish_joke)\\n\\n# Add edges to connect nodes\\nworkflow.add_edge(START, \"generate_joke\")\\nworkflow.add_conditional_edges(\\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\\n)\\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\\nworkflow.add_edge(\"polish_joke\", END)\\n\\n# Compile\\nchain = workflow.compile()\\n\\n# Show workflow\\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = chain.invoke({\"topic\": \"cats\"})\\nprint(\"Initial joke:\")\\nprint(state[\"joke\"])\\nprint(\"\\\\n--- --- ---\\\\n\")\\nif \"improved_joke\" in state:\\n    print(\"Improved joke:\")\\n    print(state[\"improved_joke\"])\\n    print(\"\\\\n--- --- ---\\\\n\")\\n\\n    print(\"Final joke:\")\\n    print(state[\"final_joke\"])\\nelse:\\n    print(\"Joke failed quality gate - no punchline detected!\")'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='\\u200bParallelization\\nWith parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:\\n\\nSplit up subtasks and run them in parallel, which increases speed\\nRun tasks multiple times to check for different outputs, which increases confidence\\n\\nSome examples include:\\n\\nRunning one subtask that processes a document for keywords, and a second subtask to check for formatting errors\\nRunning a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources\\n\\n\\nGraph APIFunctional APICopyAsk AI# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    story: str\\n    poem: str\\n    combined_output: str\\n\\n\\n# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef call_llm_2(state: State):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n\\n    msg = llm.invoke(f\"Write a story about {state[\\'topic\\']}\")\\n    return {\"story\": msg.content}\\n\\n\\ndef call_llm_3(state: State):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n\\n    msg = llm.invoke(f\"Write a poem about {state[\\'topic\\']}\")\\n    return {\"poem\": msg.content}\\n\\n\\ndef aggregator(state: State):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {state[\\'topic\\']}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{state[\\'story\\']}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{state[\\'joke\\']}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{state[\\'poem\\']}\"\\n    return {\"combined_output\": combined}\\n\\n\\n# Build workflow\\nparallel_builder = StateGraph(State)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Build workflow\\nparallel_builder = StateGraph(State)\\n\\n# Add nodes\\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\\nparallel_builder.add_node(\"aggregator\", aggregator)\\n\\n# Add edges to connect nodes\\nparallel_builder.add_edge(START, \"call_llm_1\")\\nparallel_builder.add_edge(START, \"call_llm_2\")\\nparallel_builder.add_edge(START, \"call_llm_3\")\\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\\nparallel_builder.add_edge(\"aggregator\", END)\\nparallel_workflow = parallel_builder.compile()\\n\\n# Show workflow\\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])\\n\\n\\u200bRouting\\nRouting workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.\\n\\nGraph APIFunctional APICopyAsk AIfrom typing_extensions import Literal\\nfrom langchain.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str\\n\\n\\n# Nodes\\ndef llm_call_1(state: State):\\n    \"\"\"Write a story\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_2(state: State):\\n    \"\"\"Write a joke\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_3(state: State):\\n    \"\"\"Write a poem\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_router(state: State):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=state[\"input\"]),\\n        ]\\n    )\\n\\n    return {\"decision\": decision.step}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='return {\"decision\": decision.step}\\n\\n\\n# Conditional edge function to route to the appropriate node\\ndef route_decision(state: State):\\n    # Return the node name you want to visit next\\n    if state[\"decision\"] == \"story\":\\n        return \"llm_call_1\"\\n    elif state[\"decision\"] == \"joke\":\\n        return \"llm_call_2\"\\n    elif state[\"decision\"] == \"poem\":\\n        return \"llm_call_3\"\\n\\n\\n# Build workflow\\nrouter_builder = StateGraph(State)\\n\\n# Add nodes\\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Add edges to connect nodes\\nrouter_builder.add_edge(START, \"llm_call_router\")\\nrouter_builder.add_conditional_edges(\\n    \"llm_call_router\",\\n    route_decision,\\n    {  # Name returned by route_decision : Name of next node to visit\\n        \"llm_call_1\": \"llm_call_1\",\\n        \"llm_call_2\": \"llm_call_2\",\\n        \"llm_call_3\": \"llm_call_3\",\\n    },\\n)\\nrouter_builder.add_edge(\"llm_call_1\", END)\\nrouter_builder.add_edge(\"llm_call_2\", END)\\nrouter_builder.add_edge(\"llm_call_3\", END)\\n\\n# Compile workflow\\nrouter_workflow = router_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\\nprint(state[\"output\"])\\n\\n\\u200bOrchestrator-worker\\nIn an orchestrator-worker configuration, the orchestrator:\\n\\nBreaks down tasks into subtasks\\nDelegates subtasks to workers\\nSynthesizes worker outputs into a final result'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Orchestrator-worker workflows provide more flexibility and are often used when subtasks cannot be predefined the way they can with parallelization. This is common with workflows that write code or need to update content across multiple files. For example, a workflow that needs to update installation instructions for multiple Python libraries across an unknown number of documents might use this pattern.\\nGraph APIFunctional APICopyAsk AIfrom typing import Annotated, List\\nimport operator\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\n\\u200bCreating workers in LangGraph\\nOrchestrator-worker workflows are common and LangGraph has built-in support for them. The Send API lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the Send API to send a section to each worker.\\nCopyAsk AIfrom langgraph.types import Send\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str  # Report topic\\n    sections: list[Section]  # List of report sections\\n    completed_sections: Annotated[\\n        list, operator.add\\n    ]  # All workers write to this key in parallel\\n    final_report: str  # Final report'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Worker state\\nclass WorkerState(TypedDict):\\n    section: Section\\n    completed_sections: Annotated[list, operator.add]\\n\\n\\n# Nodes\\ndef orchestrator(state: State):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {state[\\'topic\\']}\"),\\n        ]\\n    )\\n\\n    return {\"sections\": report_sections.sections}\\n\\n\\ndef llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='def llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    section = llm.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\\n            ),\\n            HumanMessage(\\n                content=f\"Here is the section name: {state[\\'section\\'].name} and description: {state[\\'section\\'].description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return {\"completed_sections\": [section.content]}\\n\\n\\ndef synthesizer(state: State):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n\\n    # List of completed sections\\n    completed_sections = state[\"completed_sections\"]\\n\\n    # Format completed section to str to use as context for final sections\\n    completed_report_sections = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='return {\"final_report\": completed_report_sections}\\n\\n\\n# Conditional edge function to create llm_call workers that each write a section of the report\\ndef assign_workers(state: State):\\n    \"\"\"Assign a worker to each section in the plan\"\"\"\\n\\n    # Kick off section writing in parallel via Send() API\\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\\n\\n\\n# Build workflow\\norchestrator_worker_builder = StateGraph(State)\\n\\n# Add the nodes\\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\\n\\n# Add edges to connect nodes\\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\\norchestrator_worker_builder.add_conditional_edges(\\n    \"orchestrator\", assign_workers, [\"llm_call\"]\\n)\\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\\norchestrator_worker_builder.add_edge(\"synthesizer\", END)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Compile the workflow\\norchestrator_worker = orchestrator_worker_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\\n\\nfrom IPython.display import Markdown\\nMarkdown(state[\"final_report\"])'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='from IPython.display import Markdown\\nMarkdown(state[\"final_report\"])\\n\\n\\u200bEvaluator-optimizer\\nIn evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a human-in-the-loop determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.\\nEvaluator-optimizer workflows are commonly used when there’s particular success criteria for a task, but iteration is required to meet that criteria. For example, there’s not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.\\n\\nGraph APIFunctional APICopyAsk AI# Graph state\\nclass State(TypedDict):\\n    joke: str\\n    topic: str\\n    feedback: str\\n    funny_or_not: str'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\ndef llm_call_generator(state: State):\\n    \"\"\"LLM generates a joke\"\"\"\\n\\n    if state.get(\"feedback\"):\\n        msg = llm.invoke(\\n            f\"Write a joke about {state[\\'topic\\']} but take into account the feedback: {state[\\'feedback\\']}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef llm_call_evaluator(state: State):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n\\n    grade = evaluator.invoke(f\"Grade the joke {state[\\'joke\\']}\")\\n    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\\ndef route_joke(state: State):\\n    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\\n\\n    if state[\"funny_or_not\"] == \"funny\":\\n        return \"Accepted\"\\n    elif state[\"funny_or_not\"] == \"not funny\":\\n        return \"Rejected + Feedback\"\\n\\n\\n# Build workflow\\noptimizer_builder = StateGraph(State)\\n\\n# Add the nodes\\noptimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\\noptimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\\n\\n# Add edges to connect nodes\\noptimizer_builder.add_edge(START, \"llm_call_generator\")\\noptimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\\noptimizer_builder.add_conditional_edges(\\n    \"llm_call_evaluator\",\\n    route_joke,\\n    {  # Name returned by route_joke : Name of next node to visit\\n        \"Accepted\": END,\\n        \"Rejected + Feedback\": \"llm_call_generator\",\\n    },\\n)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Compile the workflow\\noptimizer_workflow = optimizer_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = optimizer_workflow.invoke({\"topic\": \"Cats\"})\\nprint(state[\"joke\"])\\n\\n\\u200bAgents\\nAgents are typically implemented as an LLM performing actions using tools. They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.\\n\\nTo get started with agents, see the quickstart or read more about how they work in LangChain.\\nUsing toolsCopyAsk AIfrom langchain.tools import tool\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a * b'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nllm_with_tools = llm.bind_tools(tools)\\n\\nGraph APIFunctional APICopyAsk AIfrom langgraph.graph import MessagesState\\nfrom langchain.messages import SystemMessage, HumanMessage, ToolMessage\\n\\n\\n# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            llm_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ]\\n    }\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\\ndef should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"tool_node\"\\n\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n\\n# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"tool_node\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    [\"tool_node\", END]\\n)\\nagent_builder.add_edge(\"tool_node\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents/', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoThinking in LangGraphPreviousPersistenceNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content=\"Graph API overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationGraph APIGraph API overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIGraph APIUse the graph APIFunctional APIRuntimeEnglishcloseOn this pageGraphsStateGraphCompiling your graphStateSchemaMultiple schemasReducersDefault ReducerOverwriteWorking with Messages in Graph StateWhy use messages?Using Messages in your GraphSerializationMessagesStateNodesSTART NodeEND NodeNode CachingEdgesNormal EdgesConditional\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='your GraphSerializationMessagesStateNodesSTART NodeEND NodeNode CachingEdgesNormal EdgesConditional EdgesEntry PointConditional Entry PointSendCommandWhen should I use Command instead of conditional edges?Navigating to a node in a parent graphUsing inside toolsHuman-in-the-loopGraph MigrationsRuntime ContextRecursion LimitAccessing and handling the recursion counterHow it worksAccessing the current step counterProactive recursion handlingProactive vs reactive approachesOther available metadataVisualizationLangGraph APIsGraph APIGraph API overviewCopy pageCopy page\\u200bGraphs'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='State: A shared data structure that represents the current snapshot of your application. It can be any data type, but is typically defined using a shared state schema.\\n\\n\\nNodes: Functions that encode the logic of your agents. They receive the current state as input, perform some computation or side-effect, and return an updated state.\\n\\n\\nEdges: Functions that determine which Node to execute next based on the current state. They can be conditional branches or fixed transitions.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='By composing Nodes and Edges, you can create complex, looping workflows that evolve the state over time. The real power, though, comes from how LangGraph manages that state. To emphasize: Nodes and Edges are nothing more than functions - they can contain an LLM or just good ol’ code.\\nIn short: nodes do the work, edges tell what to do next.\\nLangGraph’s underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google’s Pregel system, the program proceeds in discrete “super-steps.”'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or “channels”). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.\\n\\u200bStateGraph\\nThe StateGraph class is the main graph class to use. This is parameterized by a user defined State object.\\n\\u200bCompiling your graph\\nTo build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed?'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method:\\nCopyAsk AIgraph = graph_builder.compile(...)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='You MUST compile your graph before you can use it.\\n\\u200bState\\nThe first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.\\n\\u200bSchema\\nThe main documented way to specify the schema of a graph is by using a TypedDict. If you want to provide default values in your state, use a dataclass. We also support using a Pydantic BaseModel as your graph state if you want recursive data validation (though note that pydantic is less performant than a TypedDict or dataclass).'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the guide here for how to use.\\n\\u200bMultiple schemas\\nTypically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Internal nodes can pass information that is not required in the graph’s input / output.\\nWe may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key.\\n\\nIt is possible to have nodes write to private state channels inside the graph for internal node communication. We can simply define a private schema, PrivateState.\\nIt is also possible to define explicit input and output schemas for a graph. In these cases, we define an “internal” schema that contains all keys relevant to graph operations. But, we also define input and output schemas that are sub-sets of the “internal” schema to constrain the input and output of the graph. See this guide for more detail.\\nLet’s look at an example:\\nCopyAsk AIclass InputState(TypedDict):\\n    user_input: str\\n\\nclass OutputState(TypedDict):\\n    graph_output: str\\n\\nclass OverallState(TypedDict):\\n    foo: str\\n    user_input: str\\n    graph_output: str'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='class OverallState(TypedDict):\\n    foo: str\\n    user_input: str\\n    graph_output: str\\n\\nclass PrivateState(TypedDict):\\n    bar: str\\n\\ndef node_1(state: InputState) -> OverallState:\\n    # Write to OverallState\\n    return {\"foo\": state[\"user_input\"] + \" name\"}\\n\\ndef node_2(state: OverallState) -> PrivateState:\\n    # Read from OverallState, write to PrivateState\\n    return {\"bar\": state[\"foo\"] + \" is\"}\\n\\ndef node_3(state: PrivateState) -> OutputState:\\n    # Read from PrivateState, write to OutputState\\n    return {\"graph_output\": state[\"bar\"] + \" Lance\"}\\n\\nbuilder = StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)\\nbuilder.add_node(\"node_1\", node_1)\\nbuilder.add_node(\"node_2\", node_2)\\nbuilder.add_node(\"node_3\", node_3)\\nbuilder.add_edge(START, \"node_1\")\\nbuilder.add_edge(\"node_1\", \"node_2\")\\nbuilder.add_edge(\"node_2\", \"node_3\")\\nbuilder.add_edge(\"node_3\", END)\\n\\ngraph = builder.compile()\\ngraph.invoke({\"user_input\":\"My\"})\\n# {\\'graph_output\\': \\'My name is Lance\\'}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='graph = builder.compile()\\ngraph.invoke({\"user_input\":\"My\"})\\n# {\\'graph_output\\': \\'My name is Lance\\'}\\n\\nThere are two subtle and important points to note here:\\n\\n\\nWe pass state: InputState as the input schema to node_1. But, we write out to foo, a channel in OverallState. How can we write out to a state channel that is not included in the input schema? This is because a node can write to any state channel in the graph state. The graph state is the union of the state channels defined at initialization, which includes OverallState and the filters InputState and OutputState.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='We initialize the graph with StateGraph(OverallState,input_schema=InputState,output_schema=OutputState). So, how can we write to PrivateState in node_2? How does the graph gain access to this schema if it was not passed in the StateGraph initialization? We can do this because nodes can also declare additional state channels as long as the state schema definition exists. In this case, the PrivateState schema is defined, so we can add bar as a new state channel in the graph and write to it.\\n\\n\\n\\u200bReducers\\nReducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer:\\n\\u200bDefault Reducer\\nThese two examples show how to use the default reducer:\\nExample A:\\nCopyAsk AIfrom typing_extensions import TypedDict'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='class State(TypedDict):\\n    foo: int\\n    bar: list[str]\\n\\nIn this example, no reducer functions are specified for any key. Let’s assume the input to the graph is:\\n{\"foo\": 1, \"bar\": [\"hi\"]}. Let’s then assume the first Node returns {\"foo\": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {\"foo\": 2, \"bar\": [\"hi\"]}. If the second node returns {\"bar\": [\"bye\"]} then the State would then be {\"foo\": 2, \"bar\": [\"bye\"]}\\nExample B:\\nCopyAsk AIfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\nfrom operator import add\\n\\nclass State(TypedDict):\\n    foo: int\\n    bar: Annotated[list[str], add]'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='In this example, we’ve used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let’s assume the input to the graph is {\"foo\": 1, \"bar\": [\"hi\"]}. Let’s then assume the first Node returns {\"foo\": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {\"foo\": 2, \"bar\": [\"hi\"]}. If the second node returns {\"bar\": [\"bye\"]} then the State would then be {\"foo\": 2, \"bar\": [\"hi\", \"bye\"]}. Notice here that the bar key is updated by adding the two lists together.\\n\\u200bOverwrite\\nIn some cases, you may want to bypass a reducer and directly overwrite a state value. LangGraph provides the Overwrite type for this purpose. Learn how to use Overwrite here.\\n\\u200bWorking with Messages in Graph State\\n\\u200bWhy use messages?'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bWorking with Messages in Graph State\\n\\u200bWhy use messages?\\nMost modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain’s ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide.\\n\\u200bUsing Messages in your Graph'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bUsing Messages in your Graph\\nIn many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don’t specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.\\n\\u200bSerialization\\nIn addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format:\\nCopyAsk AI# this is supported'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='CopyAsk AI# this is supported\\n{\"messages\": [HumanMessage(content=\"message\")]}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='# and this is also supported\\n{\"messages\": [{\"type\": \"human\", \"content\": \"message\"}]}\\n\\nSince the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[\"messages\"][-1].content. Below is an example of a graph that uses add_messages as its reducer function.\\nCopyAsk AIfrom langchain.messages import AnyMessage\\nfrom langgraph.graph.message import add_messages\\nfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\n\\nclass GraphState(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='class GraphState(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]\\n\\n\\u200bMessagesState\\nSince having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:\\nCopyAsk AIfrom langgraph.graph import MessagesState\\n\\nclass State(MessagesState):\\n    documents: list[str]\\n\\n\\u200bNodes\\nIn LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments:\\n\\nstate: The state of the graph\\nconfig: A RunnableConfig object that contains configuration information like thread_id and tracing information like tags\\nruntime: A Runtime object that contains runtime context and other information like store and stream_writer'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Similar to NetworkX, you add these nodes to a graph using the add_node method:\\nCopyAsk AIfrom dataclasses import dataclass\\nfrom typing_extensions import TypedDict\\n\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph\\nfrom langgraph.runtime import Runtime\\n\\nclass State(TypedDict):\\n    input: str\\n    results: str\\n\\n@dataclass\\nclass Context:\\n    user_id: str\\n\\nbuilder = StateGraph(State)\\n\\ndef plain_node(state: State):\\n    return state\\n\\ndef node_with_runtime(state: State, runtime: Runtime[Context]):\\n    print(\"In node: \", runtime.context.user_id)\\n    return {\"results\": f\"Hello, {state[\\'input\\']}!\"}\\n\\ndef node_with_config(state: State, config: RunnableConfig):\\n    print(\"In node with thread_id: \", config[\"configurable\"][\"thread_id\"])\\n    return {\"results\": f\"Hello, {state[\\'input\\']}!\"}\\n\\n\\nbuilder.add_node(\"plain_node\", plain_node)\\nbuilder.add_node(\"node_with_runtime\", node_with_runtime)\\nbuilder.add_node(\"node_with_config\", node_with_config)\\n...'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Behind the scenes, functions are converted to RunnableLambdas, which add batch and async support to your function, along with native tracing and debugging.\\nIf you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name.\\nCopyAsk AIbuilder.add_node(my_node)\\n# You can then create edges to/from this node by referencing it as `\"my_node\"`\\n\\n\\u200bSTART Node\\nThe START Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.\\nCopyAsk AIfrom langgraph.graph import START\\n\\ngraph.add_edge(START, \"node_a\")\\n\\n\\u200bEND Node\\nThe END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.\\nCopyAsk AIfrom langgraph.graph import END\\n\\ngraph.add_edge(\"node_a\", END)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='graph.add_edge(\"node_a\", END)\\n\\n\\u200bNode Caching\\nLangGraph supports caching of tasks/nodes based on the input to the node. To use caching:\\n\\nSpecify a cache when compiling a graph (or specifying an entrypoint)\\nSpecify a cache policy for nodes. Each cache policy supports:\\n\\nkey_func used to generate a cache key based on the input to a node, which defaults to a hash of the input with pickle.\\nttl, the time to live for the cache in seconds. If not specified, the cache will never expire.\\n\\n\\n\\nFor example:\\nCopyAsk AIimport time\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph\\nfrom langgraph.cache.memory import InMemoryCache\\nfrom langgraph.types import CachePolicy\\n\\n\\nclass State(TypedDict):\\n    x: int\\n    result: int\\n\\n\\nbuilder = StateGraph(State)\\n\\n\\ndef expensive_node(state: State) -> dict[str, int]:\\n    # expensive computation\\n    time.sleep(2)\\n    return {\"result\": state[\"x\"] * 2}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='builder.add_node(\"expensive_node\", expensive_node, cache_policy=CachePolicy(ttl=3))\\nbuilder.set_entry_point(\"expensive_node\")\\nbuilder.set_finish_point(\"expensive_node\")\\n\\ngraph = builder.compile(cache=InMemoryCache())\\n\\nprint(graph.invoke({\"x\": 5}, stream_mode=\\'updates\\'))    \\n# [{\\'expensive_node\\': {\\'result\\': 10}}]\\nprint(graph.invoke({\"x\": 5}, stream_mode=\\'updates\\'))    \\n# [{\\'expensive_node\\': {\\'result\\': 10}, \\'__metadata__\\': {\\'cached\\': True}}]\\n\\n\\nFirst run takes two seconds to run (due to mocked expensive computation).\\nSecond run utilizes cache and returns quickly.\\n\\n\\u200bEdges\\nEdges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Normal Edges: Go directly from one node to the next.\\nConditional Edges: Call a function to determine which node(s) to go to next.\\nEntry Point: Which node to call first when user input arrives.\\nConditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.\\n\\nA node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.\\n\\u200bNormal Edges\\nIf you always want to go from node A to node B, you can use the add_edge method directly.\\nCopyAsk AIgraph.add_edge(\"node_a\", \"node_b\")\\n\\n\\u200bConditional Edges\\nIf you want to optionally route to one or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a “routing function” to call after that node is executed:\\nCopyAsk AIgraph.add_conditional_edges(\"node_a\", routing_function)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Similar to nodes, the routing_function accepts the current state of the graph and returns a value.\\nBy default, the return value routing_function is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.\\nYou can optionally provide a dictionary that maps the routing_function’s output to the name of the next node.\\nCopyAsk AIgraph.add_conditional_edges(\"node_a\", routing_function, {True: \"node_b\", False: \"node_c\"})\\n\\nUse Command instead of conditional edges if you want to combine state updates and routing in a single function.\\n\\u200bEntry Point\\nThe entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph.\\nCopyAsk AIfrom langgraph.graph import START\\n\\ngraph.add_edge(START, \"node_a\")'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='graph.add_edge(START, \"node_a\")\\n\\n\\u200bConditional Entry Point\\nA conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this.\\nCopyAsk AIfrom langgraph.graph import START\\n\\ngraph.add_conditional_edges(START, routing_function)\\n\\nYou can optionally provide a dictionary that maps the routing_function’s output to the name of the next node.\\nCopyAsk AIgraph.add_conditional_edges(START, routing_function, {True: \"node_b\", False: \"node_c\"})'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bSend\\nBy default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object).\\nTo support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node.\\nCopyAsk AIdef continue_to_jokes(state: OverallState):\\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\\'subjects\\']]'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='graph.add_conditional_edges(\"node_a\", continue_to_jokes)\\n\\n\\u200bCommand\\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:\\nCopyAsk AIdef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        # state update\\n        update={\"foo\": \"bar\"},\\n        # control flow\\n        goto=\"my_other_node\"\\n    )\\n\\nWith Command you can also achieve dynamic control flow behavior (identical to conditional edges):\\nCopyAsk AIdef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    if state[\"foo\"] == \"bar\":\\n        return Command(update={\"foo\": \"baz\"}, goto=\"my_other_node\")'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='When returning Command in your node functions, you must add return type annotations with the list of node names the node is routing to, e.g. Command[Literal[\"my_other_node\"]]. This is necessary for the graph rendering and tells LangGraph that my_node can navigate to my_other_node.\\nCheck out this how-to guide for an end-to-end example of how to use Command.\\n\\u200bWhen should I use Command instead of conditional edges?\\n\\nUse Command when you need to both update the graph state and route to a different node. For example, when implementing multi-agent handoffs where it’s important to route to a different agent and pass some information to that agent.\\nUse conditional edges to route between nodes conditionally without updating the state.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bNavigating to a node in a parent graph\\nIf you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify graph=Command.PARENT in Command:\\nCopyAsk AIdef my_node(state: State) -> Command[Literal[\"other_subgraph\"]]:\\n    return Command(\\n        update={\"foo\": \"bar\"},\\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\\n        graph=Command.PARENT\\n    )'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Setting graph to Command.PARENT will navigate to the closest parent graph.When you send updates from a subgraph node to a parent graph node for a key that’s shared by both parent and subgraph state schemas, you must define a reducer for the key you’re updating in the parent graph state. See this example.\\nThis is particularly useful when implementing multi-agent handoffs.\\nCheck out this guide for detail.\\n\\u200bUsing inside tools\\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation.\\nRefer to this guide for detail.\\n\\u200bHuman-in-the-loop\\nCommand is an important part of human-in-the-loop workflows: when using interrupt() to collect user input, Command is then used to supply the input and resume execution via Command(resume=\"User input\"). Check out this conceptual guide for more information.\\n\\u200bGraph Migrations'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bGraph Migrations\\nLangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)\\nFor threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) — if this is a blocker please reach out and we can prioritize a solution.\\nFor modifying state, we have full backwards and forwards compatibility for adding and removing keys\\nState keys that are renamed lose their saved state in existing threads\\nState keys whose types change in incompatible ways could currently cause issues in threads with state from before the change — if this is a blocker please reach out and we can prioritize a solution.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bRuntime Context\\nWhen creating a graph, you can specify a context_schema for runtime context passed to nodes. This is useful for passing\\ninformation to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection.\\nCopyAsk AI@dataclass\\nclass ContextSchema:\\n    llm_provider: str = \"openai\"\\n\\ngraph = StateGraph(State, context_schema=ContextSchema)\\n\\nYou can then pass this context into the graph using the context parameter of the invoke method.\\nCopyAsk AIgraph.invoke(inputs, context={\"llm_provider\": \"anthropic\"})\\n\\nYou can then access and use this context inside a node or conditional edge:\\nCopyAsk AIfrom langgraph.runtime import Runtime\\n\\ndef node_a(state: State, runtime: Runtime[ContextSchema]):\\n    llm = get_llm(runtime.context.llm_provider)\\n    # ...'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='See this guide for a full breakdown on configuration.\\n\\u200bRecursion Limit\\nThe recursion limit sets the maximum number of super-steps the graph can execute during a single execution. Once the limit is reached, LangGraph will raise GraphRecursionError. By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to invoke/stream via the config dictionary. Importantly, recursion_limit is a standalone config key and should not be passed inside the configurable key as all other user-defined configuration. See the example below:\\nCopyAsk AIgraph.invoke(inputs, config={\"recursion_limit\": 5}, context={\"llm\": \"anthropic\"})'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Read this how-to to learn more about how the recursion limit works.\\n\\u200bAccessing and handling the recursion counter\\nThe current step counter is accessible in config[\"metadata\"][\"langgraph_step\"] within any node, allowing for proactive recursion handling before hitting the recursion limit. This enables you to implement graceful degradation strategies within your graph logic.\\n\\u200bHow it works\\nThe step counter is stored in config[\"metadata\"][\"langgraph_step\"]. The recursion limit check follows the logic: step > stop where stop = step + recursion_limit + 1. When the limit is exceeded, LangGraph raises a GraphRecursionError.\\n\\u200bAccessing the current step counter\\nYou can access the current step counter within any node to monitor execution progress.\\nCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='def my_node(state: dict, config: RunnableConfig) -> dict:\\n    current_step = config[\"metadata\"][\"langgraph_step\"]\\n    print(f\"Currently on step: {current_step}\")\\n    return state\\n\\n\\u200bProactive recursion handling\\nYou can check the step counter and proactively route to a different node before hitting the limit. This allows for graceful degradation within your graph.\\nCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph, END\\n\\ndef reasoning_node(state: dict, config: RunnableConfig) -> dict:\\n    current_step = config[\"metadata\"][\"langgraph_step\"]\\n    recursion_limit = config[\"recursion_limit\"]  # always present, defaults to 25\\n\\n    # Check if we\\'re approaching the limit (e.g., 80% threshold)\\n    if current_step >= recursion_limit * 0.8:\\n        return {\\n            **state,\\n            \"route_to\": \"fallback\",\\n            \"reason\": \"Approaching recursion limit\"\\n        }'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='# Normal processing\\n    return {\"messages\": state[\"messages\"] + [\"thinking...\"]}\\n\\ndef fallback_node(state: dict, config: RunnableConfig) -> dict:\\n    \"\"\"Handle cases where recursion limit is approaching\"\"\"\\n    return {\\n        **state,\\n        \"messages\": state[\"messages\"] + [\"Reached complexity limit, providing best effort answer\"]\\n    }\\n\\ndef route_based_on_state(state: dict) -> str:\\n    if state.get(\"route_to\") == \"fallback\":\\n        return \"fallback\"\\n    elif state.get(\"done\"):\\n        return END\\n    return \"reasoning\"\\n\\n# Build graph\\ngraph = StateGraph(dict)\\ngraph.add_node(\"reasoning\", reasoning_node)\\ngraph.add_node(\"fallback\", fallback_node)\\ngraph.add_conditional_edges(\"reasoning\", route_based_on_state)\\ngraph.add_edge(\"fallback\", END)\\ngraph.set_entry_point(\"reasoning\")\\n\\napp = graph.compile()'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='app = graph.compile()\\n\\n\\u200bProactive vs reactive approaches\\nThere are two main approaches to handling recursion limits: proactive (monitoring within the graph) and reactive (catching errors externally).\\nCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph, END\\nfrom langgraph.errors import GraphRecursionError\\n\\n# Proactive Approach (recommended)\\ndef agent_with_monitoring(state: dict, config: RunnableConfig) -> dict:\\n    \"\"\"Proactively monitor and handle recursion within the graph\"\"\"\\n    current_step = config[\"metadata\"][\"langgraph_step\"]\\n    recursion_limit = config[\"recursion_limit\"]\\n\\n    # Early detection - route to internal handling\\n    if current_step >= recursion_limit - 2:  # 2 steps before limit\\n        return {\\n            **state,\\n            \"status\": \"recursion_limit_approaching\",\\n            \"final_answer\": \"Reached iteration limit, returning partial result\"\\n        }'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='# Normal processing\\n    return {\"messages\": state[\"messages\"] + [f\"Step {current_step}\"]}\\n\\n# Reactive Approach (fallback)\\ntry:\\n    result = graph.invoke(initial_state, {\"recursion_limit\": 10})\\nexcept GraphRecursionError as e:\\n    # Handle externally after graph execution fails\\n    result = fallback_handler(initial_state)\\n\\nThe key differences between these approaches are:\\n\\nApproachDetectionHandlingControl Flow\\nProactive (using langgraph_step)Before limit reachedInside graph via conditional routingGraph continues to completion nodeReactive (catching GraphRecursionError)After limit exceededOutside graph in try/catchGraph execution terminated\\n\\nProactive advantages:\\n\\nGraceful degradation within the graph\\nCan save intermediate state in checkpoints\\nBetter user experience with partial results\\nGraph completes normally (no exception)\\n\\nReactive advantages:\\n\\nSimpler implementation\\nNo need to modify graph logic\\nCentralized error handling'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Simpler implementation\\nNo need to modify graph logic\\nCentralized error handling\\n\\n\\u200bOther available metadata\\nAlong with langgraph_step, the following metadata is also available in config[\"metadata\"]:\\nCopyAsk AIdef inspect_metadata(state: dict, config: RunnableConfig) -> dict:\\n    metadata = config[\"metadata\"]\\n\\n    print(f\"Step: {metadata[\\'langgraph_step\\']}\")\\n    print(f\"Node: {metadata[\\'langgraph_node\\']}\")\\n    print(f\"Triggers: {metadata[\\'langgraph_triggers\\']}\")\\n    print(f\"Path: {metadata[\\'langgraph_path\\']}\")\\n    print(f\"Checkpoint NS: {metadata[\\'langgraph_checkpoint_ns\\']}\")\\n\\n    return state\\n\\n\\u200bVisualization\\nIt’s often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='Edit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoObservabilityPreviousUse the graph APINext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content=\"Functional API overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationFunctional APIFunctional API overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIFunctional APIUse the Functional APIRuntimeEnglishcloseOn this pageFunctional API vs. Graph APIExampleEntrypointDefinitionInjectable parametersExecutingResumingShort-term memoryentrypoint.finalTaskDefinitionExecutionWhen to use a taskSerializationDeterminismIdempotencyCommon PitfallsHandling side\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='to use a taskSerializationDeterminismIdempotencyCommon PitfallsHandling side effectsNon-deterministic control flowLangGraph APIsFunctional APIFunctional API overviewCopy pageCopy pageThe Functional API allows you to add LangGraph’s key features — persistence, memory, human-in-the-loop, and streaming — to your applications with minimal changes to your existing code.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='It is designed to integrate these features into existing code that may use standard language primitives for branching and control flow, such as if statements, for loops, and function calls. Unlike many data orchestration frameworks that require restructuring code into an explicit pipeline or DAG, the Functional API allows you to incorporate these capabilities without enforcing a rigid execution model.\\nThe Functional API uses two key building blocks:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='@entrypoint – Marks a function as the starting point of a workflow, encapsulating logic and managing execution flow, including handling long-running tasks and interrupts.\\n@task – Represents a discrete unit of work, such as an API call or data processing step, that can be executed asynchronously within an entrypoint. Tasks return a future-like object that can be awaited or resolved synchronously.\\n\\nThis provides a minimal abstraction for building workflows with state management and streaming.\\nFor information on how to use the functional API, see Use Functional API.\\n\\u200bFunctional API vs. Graph API\\nFor users who prefer a more declarative approach, LangGraph’s Graph API allows you to define workflows using a Graph paradigm. Both APIs share the same underlying runtime, so you can use them together in the same application.\\nHere are some key differences:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Control flow: The Functional API does not require thinking about graph structure. You can use standard Python constructs to define workflows. This will usually trim the amount of code you need to write.\\nShort-term memory: The GraphAPI requires declaring a State and may require defining reducers to manage updates to the graph state. @entrypoint and @tasks do not require explicit state management as their state is scoped to the function and is not shared across functions.\\nCheckpointing: Both APIs generate and use checkpoints. In the Graph API a new checkpoint is generated after every superstep. In the Functional API, when tasks are executed, their results are saved to an existing checkpoint associated with the given entrypoint instead of creating a new checkpoint.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Visualization: The Graph API makes it easy to visualize the workflow as a graph which can be useful for debugging, understanding the workflow, and sharing with others. The Functional API does not support visualization as the graph is dynamically generated during runtime.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bExample\\nBelow we demonstrate a simple application that writes an essay and interrupts to request human review.\\nCopyAsk AIfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langgraph.func import entrypoint, task\\nfrom langgraph.types import interrupt\\n\\n@task\\ndef write_essay(topic: str) -> str:\\n    \"\"\"Write an essay about the given topic.\"\"\"\\n    time.sleep(1) # A placeholder for a long-running task.\\n    return f\"An essay about topic: {topic}\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='@entrypoint(checkpointer=InMemorySaver())\\ndef workflow(topic: str) -> dict:\\n    \"\"\"A simple workflow that writes an essay and asks for a review.\"\"\"\\n    essay = write_essay(\"cat\").result()\\n    is_approved = interrupt({\\n        # Any json-serializable payload provided to interrupt as argument.\\n        # It will be surfaced on the client side as an Interrupt when streaming data\\n        # from the workflow.\\n        \"essay\": essay, # The essay we want reviewed.\\n        # We can add any additional information that we need.\\n        # For example, introduce a key called \"action\" with some instructions.\\n        \"action\": \"Please approve/reject the essay\",\\n    })\\n\\n    return {\\n        \"essay\": essay, # The essay that was generated\\n        \"is_approved\": is_approved, # Response from HIL\\n    }'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Detailed ExplanationThis workflow will write an essay about the topic “cat” and then pause to get a review from a human. The workflow can be interrupted for an indefinite amount of time until a review is provided.When the workflow is resumed, it executes from the very start, but because the result of the writeEssay task was already saved, the task result will be loaded from the checkpoint instead of being recomputed.CopyAsk AIimport time\\nimport uuid\\nfrom langgraph.func import entrypoint, task\\nfrom langgraph.types import interrupt\\nfrom langgraph.checkpoint.memory import InMemorySaver\\n\\n\\n@task\\ndef write_essay(topic: str) -> str:\\n    \"\"\"Write an essay about the given topic.\"\"\"\\n    time.sleep(1)  # This is a placeholder for a long-running task.\\n    return f\"An essay about topic: {topic}\"'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='@entrypoint(checkpointer=InMemorySaver())\\ndef workflow(topic: str) -> dict:\\n    \"\"\"A simple workflow that writes an essay and asks for a review.\"\"\"\\n    essay = write_essay(\"cat\").result()\\n    is_approved = interrupt(\\n        {\\n            # Any json-serializable payload provided to interrupt as argument.\\n            # It will be surfaced on the client side as an Interrupt when streaming data\\n            # from the workflow.\\n            \"essay\": essay,  # The essay we want reviewed.\\n            # We can add any additional information that we need.\\n            # For example, introduce a key called \"action\" with some instructions.\\n            \"action\": \"Please approve/reject the essay\",\\n        }\\n    )\\n    return {\\n        \"essay\": essay,  # The essay that was generated\\n        \"is_approved\": is_approved,  # Response from HIL\\n    }'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='thread_id = str(uuid.uuid4())\\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\\nfor item in workflow.stream(\"cat\", config):\\n    print(item)\\n# > {\\'write_essay\\': \\'An essay about topic: cat\\'}\\n# > {\\n# >     \\'__interrupt__\\': (\\n# >        Interrupt(\\n# >            value={\\n# >                \\'essay\\': \\'An essay about topic: cat\\',\\n# >                \\'action\\': \\'Please approve/reject the essay\\'\\n# >            },\\n# >            id=\\'b9b2b9d788f482663ced6dc755c9e981\\'\\n# >        ),\\n# >    )\\n# > }\\nAn essay has been written and is ready for review. Once the review is provided, we can resume the workflow:CopyAsk AIfrom langgraph.types import Command\\n\\n# Get review from a user (e.g., via a UI)\\n# In this case, we\\'re using a bool, but this can be any json-serializable value.\\nhuman_review = True'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content=\"for item in workflow.stream(Command(resume=human_review), config):\\n    print(item)\\nCopyAsk AI{'workflow': {'essay': 'An essay about topic: cat', 'is_approved': False}}\\nThe workflow has been completed and the review has been added to the essay.\\n\\u200bEntrypoint\\nThe @entrypoint decorator can be used to create a workflow from a function. It encapsulates workflow logic and manages execution flow, including handling long-running tasks and interrupts.\\n\\u200bDefinition\\nAn entrypoint is defined by decorating a function with the @entrypoint decorator.\\nThe function must accept a single positional argument, which serves as the workflow input. If you need to pass multiple pieces of data, use a dictionary as the input type for the first argument.\\nDecorating a function with an entrypoint produces a Pregel instance which helps to manage the execution of the workflow (e.g., handles streaming, resumption, and checkpointing).\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='You will usually want to pass a checkpointer to the @entrypoint decorator to enable persistence and use features like human-in-the-loop.\\n Sync AsyncCopyAsk AIfrom langgraph.func import entrypoint'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(some_input: dict) -> int:\\n    # some logic that may involve long-running tasks like API calls,\\n    # and may be interrupted for human-in-the-loop.\\n    ...\\n    return result'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Serialization\\nThe inputs and outputs of entrypoints must be JSON-serializable to support checkpointing. Please see the serialization section for more details.\\n\\u200bInjectable parameters\\nWhen declaring an entrypoint, you can request access to additional parameters that will be injected automatically at run time. These parameters include:\\nParameterDescriptionpreviousAccess the state associated with the previous checkpoint for the given thread. See short-term-memory.storeAn instance of [BaseStore][langgraph.store.base.BaseStore]. Useful for long-term memory.writerUse to access the StreamWriter when working with Async Python < 3.11. See streaming with functional API for details.configFor accessing run time configuration. See RunnableConfig for information.\\nDeclare the parameters with the appropriate name and type annotation.\\nRequesting Injectable ParametersCopyAsk AIfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.func import entrypoint'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='from langgraph.func import entrypoint\\nfrom langgraph.store.base import BaseStore\\nfrom langgraph.store.memory import InMemoryStore'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='in_memory_store = InMemoryStore(...)  # An instance of InMemoryStore for long-term memory\\n\\n@entrypoint(\\n    checkpointer=checkpointer,  # Specify the checkpointer\\n    store=in_memory_store  # Specify the store\\n)\\ndef my_workflow(\\n    some_input: dict,  # The input (e.g., passed via `invoke`)\\n    *,\\n    previous: Any = None, # For short-term memory\\n    store: BaseStore,  # For long-term memory\\n    writer: StreamWriter,  # For streaming custom data\\n    config: RunnableConfig  # For accessing the configuration passed to the entrypoint\\n) -> ...:\\n\\n\\u200bExecuting\\nUsing the @entrypoint yields a Pregel object that can be executed using the invoke, ainvoke, stream, and astream methods.\\n Invoke Async Invoke Stream Async StreamCopyAsk AIconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\nmy_workflow.invoke(some_input, config)  # Wait for the result synchronously'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bResuming\\nResuming an execution after an interrupt can be done by passing a resume value to the Command primitive.\\n Invoke Async Invoke Stream Async StreamCopyAsk AIfrom langgraph.types import Command\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\n\\nmy_workflow.invoke(Command(resume=some_resume_value), config)\\n\\nResuming after an error\\nTo resume after an error, run the entrypoint with a None and the same thread id (config).\\nThis assumes that the underlying error has been resolved and execution can proceed successfully.\\n Invoke Async Invoke Stream Async StreamCopyAsk AI\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\n\\nmy_workflow.invoke(None, config)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='my_workflow.invoke(None, config)\\n\\n\\u200bShort-term memory\\nWhen an entrypoint is defined with a checkpointer, it stores information between successive invocations on the same thread id in checkpoints.\\nThis allows accessing the state from the previous invocation using the previous parameter.\\nBy default, the previous parameter is the return value of the previous invocation.\\nCopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(number: int, *, previous: Any = None) -> int:\\n    previous = previous or 0\\n    return number + previous\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\n\\nmy_workflow.invoke(1, config)  # 1 (previous was None)\\nmy_workflow.invoke(2, config)  # 3 (previous was 1 from the previous invocation)'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bentrypoint.final\\nentrypoint.final is a special primitive that can be returned from an entrypoint and allows decoupling the value that is saved in the checkpoint from the return value of the entrypoint.\\nThe first value is the return value of the entrypoint, and the second value is the value that will be saved in the checkpoint. The type annotation is entrypoint.final[return_type, save_type].\\nCopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(number: int, *, previous: Any = None) -> entrypoint.final[int, int]:\\n    previous = previous or 0\\n    # This will return the previous value to the caller, saving\\n    # 2 * number to the checkpoint, which will be used in the next invocation\\n    # for the `previous` parameter.\\n    return entrypoint.final(value=previous, save=2 * number)\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"1\"\\n    }\\n}'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='config = {\\n    \"configurable\": {\\n        \"thread_id\": \"1\"\\n    }\\n}\\n\\nmy_workflow.invoke(3, config)  # 0 (previous was None)\\nmy_workflow.invoke(1, config)  # 6 (previous was 3 * 2 from the previous invocation)\\n\\n\\u200bTask\\nA task represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics:\\n\\nAsynchronous Execution: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking.\\nCheckpointing: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See persistence for more details).\\n\\n\\u200bDefinition\\nTasks are defined using the @task decorator, which wraps a regular Python function.\\nCopyAsk AIfrom langgraph.func import task\\n\\n@task()\\ndef slow_computation(input_value):\\n    # Simulate a long-running operation\\n    ...\\n    return result'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='Serialization\\nThe outputs of tasks must be JSON-serializable to support checkpointing.\\n\\u200bExecution\\nTasks can only be called from within an entrypoint, another task, or a state graph node.\\nTasks cannot be called directly from the main application code.\\nWhen you call a task, it returns immediately with a future object. A future is a placeholder for a result that will be available later.\\nTo obtain the result of a task, you can either wait for it synchronously (using result()) or await it asynchronously (using await).\\n Synchronous Invocation Asynchronous InvocationCopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(some_input: int) -> int:\\n    future = slow_computation(some_input)\\n    return future.result()  # Wait for the result synchronously\\n\\n\\u200bWhen to use a task\\nTasks are useful in the following scenarios:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bWhen to use a task\\nTasks are useful in the following scenarios:\\n\\nCheckpointing: When you need to save the result of a long-running operation to a checkpoint, so you don’t need to recompute it when resuming the workflow.\\nHuman-in-the-loop: If you’re building a workflow that requires human intervention, you MUST use tasks to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the determinism section for more details.\\nParallel Execution: For I/O-bound tasks, tasks enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs).\\nObservability: Wrapping operations in tasks provides a way to track the progress of the workflow and monitor the execution of individual operations using LangSmith.\\nRetryable Work: When work needs to be retried to handle failures or inconsistencies, tasks provide a way to encapsulate and manage the retry logic.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bSerialization\\nThere are two key aspects to serialization in LangGraph:\\n\\nentrypoint inputs and outputs must be JSON-serializable.\\ntask outputs must be JSON-serializable.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='These requirements are necessary for enabling checkpointing and workflow resumption. Use python primitives like dictionaries, lists, strings, numbers, and booleans to ensure that your inputs and outputs are serializable.\\nSerialization ensures that workflow state, such as task results and intermediate values, can be reliably saved and restored. This is critical for enabling human-in-the-loop interactions, fault tolerance, and parallel execution.\\nProviding non-serializable inputs or outputs will result in a runtime error when a workflow is configured with a checkpointer.\\n\\u200bDeterminism\\nTo utilize features like human-in-the-loop, any randomness should be encapsulated inside of tasks. This guarantees that when execution is halted (e.g., for human in the loop) and then resumed, it will follow the same sequence of steps, even if task results are non-deterministic.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='LangGraph achieves this behavior by persisting task and subgraph results as they execute. A well-designed workflow ensures that resuming execution follows the same sequence of steps, allowing previously computed results to be retrieved correctly without having to re-execute them. This is particularly useful for long-running tasks or tasks with non-deterministic results, as it avoids repeating previously done work and allows resuming from essentially the same.\\nWhile different runs of a workflow can produce different results, resuming a specific run should always follow the same sequence of recorded steps. This allows LangGraph to efficiently look up task and subgraph results that were executed prior to the graph being interrupted and avoid recomputing them.\\n\\u200bIdempotency'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bIdempotency\\nIdempotency ensures that running the same operation multiple times produces the same result. This helps prevent duplicate API calls and redundant processing if a step is rerun due to a failure. Always place API calls inside tasks functions for checkpointing, and design them to be idempotent in case of re-execution. Re-execution can occur if a task starts, but does not complete successfully. Then, if the workflow is resumed, the task will run again. Use idempotency keys or verify existing results to avoid duplication.\\n\\u200bCommon Pitfalls\\n\\u200bHandling side effects\\nEncapsulate side effects (e.g., writing to a file, sending an email) in tasks to ensure they are not executed multiple times when resuming a workflow.\\n Incorrect CorrectIn this example, a side effect (writing to a file) is directly included in the workflow, so it will be executed a second time when resuming the workflow.CopyAsk AI@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(inputs: dict) -> int:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='def my_workflow(inputs: dict) -> int:\\n    # This code will be executed a second time when resuming the workflow.\\n    # Which is likely not what you want.\\n    with open(\"output.txt\", \"w\") as f:  \\n        f.write(\"Side effect executed\")  \\n    value = interrupt(\"question\")\\n    return value'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bNon-deterministic control flow\\nOperations that might give different results each time (like getting current time or random numbers) should be encapsulated in tasks to ensure that on resume, the same result is returned.\\n\\nIn a task: Get random number (5) → interrupt → resume → (returns 5 again) → …\\nNot in a task: Get random number (5) → interrupt → resume → get new random number (7) → …'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='This is especially important when using human-in-the-loop workflows with multiple interrupts calls. LangGraph keeps a list of resume values for each task/entrypoint. When an interrupt is encountered, it’s matched with the corresponding resume value. This matching is strictly index-based, so the order of the resume values should match the order of the interrupts.\\nIf order of execution is not maintained when resuming, one interrupt call may be matched with the wrong resume value, leading to incorrect results.\\nPlease read the section on determinism for more details.\\n Incorrect CorrectIn this example, the workflow uses the current time to determine which task to execute. This is non-deterministic because the result of the workflow depends on the time at which it is executed.CopyAsk AIfrom langgraph.func import entrypoint\\n\\n@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(inputs: dict) -> int:\\n    t0 = inputs[\"t0\"]\\n    t1 = time.time()  \\n\\n    delta_t = t1 - t0'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/functional-api/', 'title': 'Functional API overview - Docs by LangChain', 'language': 'en'}, page_content='delta_t = t1 - t0\\n\\n    if delta_t > 1:\\n        result = slow_task(1).result()\\n        value = interrupt(\"question\")\\n    else:\\n        result = slow_task(2).result()\\n        value = interrupt(\"question\")\\n\\n    return {\\n        \"result\": result,\\n        \"value\": value\\n    }\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoUse the graph APIPreviousUse the functional APINext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]\n"
     ]
    }
   ],
   "source": [
    "doc_list = [item for sublist in docs for item in sublist]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
    "doc_splits = text_splitter.split_documents(doc_list)\n",
    "print(doc_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0a1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='LangGraph overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeEnglishcloseOn this page InstallCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageCopy pageLangGraph v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an' metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview/', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "print(doc_splits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b21d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding all the text to vectordb\n",
    "vectorstore = FAISS.from_documents(documents=doc_splits,embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d561d175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='d026fa7f-f18b-4e6b-8937-9a2a565edeca', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:'),\n",
       " Document(id='dd035483-6aa6-4929-97f3-4113ef2ad234', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:'),\n",
       " Document(id='af97d482-2163-4a61-81a5-aab1a81d34ac', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:'),\n",
       " Document(id='5818805b-5fa5-4e0d-9446-7a4665ec2f50', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/graph-api/', 'title': 'Graph API overview - Docs by LangChain', 'language': 'en'}, page_content='At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is langgraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2e9692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert retriever to retriever tools \n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retriever_vector_db_blog\",\n",
    "    \"Search and run information about langgraph\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d80f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='retriever_vector_db_blog', description='Search and run information about langgraph', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000001C480A20310>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001C48CDBC8B0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000001C480A203A0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001C48CDBC8B0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d5e3a9b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='python.langchain.com', port=443): Max retries exceeded with url: /docs/tutorials/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001C482E46740>: Failed to resolve 'python.langchain.com' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\urllib3\\connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\socket.py:967\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    966\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 967\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    968\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\urllib3\\connection.py:753\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 753\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    754\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\urllib3\\connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x000001C482E46740>: Failed to resolve 'python.langchain.com' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\requests\\adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='python.langchain.com', port=443): Max retries exceeded with url: /docs/tutorials/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001C482E46740>: Failed to resolve 'python.langchain.com' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m### langchain blogs retiever tools\u001b[39;00m\n\u001b[0;32m      2\u001b[0m langchain_urls\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://python.langchain.com/docs/tutorials/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://python.langchain.com/docs/tutorials/chatbot/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://python.langchain.com/docs/tutorials/qa_chat_history/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m ]\n\u001b[1;32m----> 8\u001b[0m docs\u001b[38;5;241m=\u001b[39m[WebBaseLoader(url)\u001b[38;5;241m.\u001b[39mload() \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m langchain_urls]\n\u001b[0;32m      9\u001b[0m docs\n",
      "Cell \u001b[1;32mIn[63], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m### langchain blogs retiever tools\u001b[39;00m\n\u001b[0;32m      2\u001b[0m langchain_urls\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://python.langchain.com/docs/tutorials/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://python.langchain.com/docs/tutorials/chatbot/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://python.langchain.com/docs/tutorials/qa_chat_history/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m ]\n\u001b[1;32m----> 8\u001b[0m docs\u001b[38;5;241m=\u001b[39m[\u001b[43mWebBaseLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m langchain_urls]\n\u001b[0;32m      9\u001b[0m docs\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\langchain_core\\document_loaders\\base.py:43\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m        the documents.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\langchain_community\\document_loaders\\web_base.py:375\u001b[0m, in \u001b[0;36mWebBaseLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Lazy load text from the url(s) in web_path.\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweb_paths:\n\u001b[1;32m--> 375\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbs_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    376\u001b[0m     text \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mget_text(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbs_get_text_kwargs)\n\u001b[0;32m    377\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m _build_metadata(soup, path)\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\langchain_community\\document_loaders\\web_base.py:357\u001b[0m, in \u001b[0;36mWebBaseLoader._scrape\u001b[1;34m(self, url, parser, bs_kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m         parser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_parser\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_parser(parser)\n\u001b[1;32m--> 357\u001b[0m html_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mget(url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequests_kwargs)\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraise_for_status:\n\u001b[0;32m    359\u001b[0m     html_doc\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32md:\\study\\udemy\\langchain\\venv\\lib\\site-packages\\requests\\adapters.py:677\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    674\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    675\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='python.langchain.com', port=443): Max retries exceeded with url: /docs/tutorials/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001C482E46740>: Failed to resolve 'python.langchain.com' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "### langchain blogs retiever tools\n",
    "langchain_urls=[\n",
    "    \"https://python.langchain.com/docs/tutorials/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/chatbot/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/qa_chat_history/\"\n",
    "]\n",
    "\n",
    "docs=[WebBaseLoader(url).load() for url in langchain_urls]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "## Add alll these text to vectordb\n",
    "\n",
    "vectorstorelangchain=FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "\n",
    "retrieverlangchain=vectorstorelangchain.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959e742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool_langchain=create_retriever_tool(\n",
    "    retrieverlangchain,\n",
    "    \"retriever_vector_langchain_blog\",\n",
    "    \"Search and run information about Langchain\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7103eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[retriever_tool,retriever_tool_langchain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213a9b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says \"append\". Reducers\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc32511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    model = llm.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625372e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecec9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Edges\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"rewrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4051e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated message\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e153862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    model = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac32c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"agent\", agent)  # agent\n",
    "retrieve = ToolNode([retriever_tool,retriever_tool_langchain])\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieval\n",
    "workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "workflow.add_node(\n",
    "    \"generate\", generate\n",
    ")  # Generating a response after we know the documents are relevant\n",
    "# Call agent node to decide to retrieve or not\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    # Assess agent decision\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f8fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"messages\":\"What is Langgraph?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328a04a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"messages\":\"What is Langchain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c4f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"messages\":\"What is Machine learning?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5e0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
